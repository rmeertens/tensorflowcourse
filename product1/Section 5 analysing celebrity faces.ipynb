{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5: analysing celebrity faces\n",
    "## Section 1: Explore possible solutions: downloading the data, visualising it, and trying our previous networks\n",
    "Download the data here: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the data\n",
    "\n",
    "## Using the load data pipeline of Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Reducing our trainable parameters by adding pooling layers\n",
    "As you can see in the thing above we have a lot of parameters. What we see is that our convolutional layers get activated, and that these activations tell a lot about what we can see in the image. \n",
    "For classification tasks it's a good idea to reduce the amount of parameters. \n",
    "\n",
    "To do this we can go over our feature map, take several activations, and group these together. This is called a \"pooling layer\", you take a pool of parameters and apply a simple function on it. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer\n",
    "\n",
    "Famous pooling layers are max-pooling and average pooling. \n",
    "`pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)`\n",
    "\n",
    "![max pool](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/314px-Max_pooling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "import tensorflow as tf\n",
    "tf.layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Countering overfitting techniques: dropout and L2 regularization\n",
    "With neural networks more data is always better. Large deep neural networks are almost always capable of learning input-output labels. This was recently demonstrated in this paper, in which they added random labels to images, the neural network was able to predict this nonsense label 100% of the time on the trainset. \n",
    "\n",
    "This \"overfitting\" is a big problem in neural networks. There are many ways to counter this, the two most famous ways are \"L2 normalisation\" and \"dropout\". \n",
    "\n",
    "### Dropout \n",
    "In neural networks every neuron tends to search for one specific pattern. This is all fine, except for that neurons tend to search for dataset specific patterns. To counter this we can randomly set the activations of neurons to zero during training. To score well on the trainset multiple neurons have to be able to detect the same patterns. This in turn ensures that your network generalises well and that you don't overfit. \n",
    "\n",
    "Units that are keps are scaled. This way the sum of the active neurons is unchanged. \n",
    "\n",
    "There is one parameter to dropout: what percentage of neurons to set to zero. A rate of 0.1 drops 10% of the input units. Hurray, another hyper-parameter you can control during learning!\n",
    "\n",
    "\n",
    "### L2 regularization\n",
    "When neural networks overfit some neurons have large weights to other neurons. If this pattern is not visible in the testset your network will still perform poorly. To counter this we can add an extra loss term: a penalty for having large weights! Although this is a great technique, it also required a lot of manual tuning. If you set the penalty too low your network does not care about the regularization. If you set the penalty too high your network won't be able to learn anything. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.layers.dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Tricks for faster training: batch normalisation\n",
    "With picture humans are able to spot patterns that are difficult for neural networks to spot. An image can be grayscale, overlighted, very dark, or noisy: humans can still spot it. \n",
    "\n",
    "https://www.quora.com/Why-does-batch-normalization-help\n",
    "\n",
    "It can help you learn faster, and also gives you a higher accuracy. \n",
    "\n",
    "There is a bit of a difference in the \"backward pass\" through the neural network. This writeup explains it in a very clear way. http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization\n",
    "\n",
    "Example in Tensorflow: http://ruishu.io/2016/12/27/batchnorm/\n",
    "\n",
    "\n",
    "Remember when we applied dropout? We used a placeholder to set the percentage it had to drop. During inference (our testing and deployment) we don't want to have any dropout or batch normalization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Understand what your network learned: visualising layers and activations\n",
    "\n",
    "A cool thing about neural networks is that you can see what your neuron reacts to. For normal neurons this can be difficult to see, especially in deeper layers. For convolutional layers however this can be a vital step to inspecting what your network actually learned. \n",
    "\n",
    "### Why is this important\n",
    "Although we humans are good at knowing exactly what to look for when classifying an image, a neural network has to learn this from scratch. Some famous stories tell about the US building neural networks to classify tanks. As training data for the American tanks they took some pictures of the tanks outside of their base. For the Russian tanks they used spy cameras. Although the network performed great on the train and testset it did not work at all in the field... It turned out that the neural network learned the difference between a blurry image (taken with a spy camera) and a clear image (taken at the base itself). \n",
    "\n",
    "Another great example to consider is classifying if something is a wolf or a dog. Give it a try: \n",
    "\n",
    "![Search result dog](illustrations/dogsearch.png)\n",
    "![Search result wolf](illustrations/wolfsearch.png)\n",
    "\n",
    "What you might use to see the different are that wolfs have pointing up ears and thick fur. A neural network could go for totally different features: apparently dogs like to sit in green grass, and wolfs are often found in the snow. If you take the classefier you trained on this data to the local zoo it would likely not recognise the wolfs if there is no snow in the zoo. \n",
    "\n",
    "\n",
    "We are going to take a look at the activations of our layers, and will see if we can detect some nice features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
