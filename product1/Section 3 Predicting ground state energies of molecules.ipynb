{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: predicting the ground state energy of molecules\n",
    "\n",
    "In chapter 1 we learned that Tensorflow does all its computations in its graph, that you first have to define. In chapter 2 we made or first neural network, and made a very simple classifier for the iris dataset. \n",
    "\n",
    "This chapter we are going to make our neural network more interesting. We are going to [predict the ground state energies of molecules](https://www.kaggle.com/haimfeld87/prediction-of-ground-state-energies-of-molecules). \n",
    "\n",
    "http://web.stanford.edu/class/cs20si/lectures/notes_09.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 1: downloading the data, visualising it \n",
    "Welcome to section 3 of our course. In this section we are going to predict the [ground state energies of molecules](https://www.kaggle.com/haimfeld87/prediction-of-ground-state-energies-of-molecules). \n",
    "\n",
    "In a [blogpost and paper the author explains what kind of data we are working with](https://burakhimmetoglu.com/machine-learning-meets-quantum-mechanics/). An interesting quote is: \n",
    "> This peculiar nonlinear dependence is impossible to model with simple linear models, thus learning algorithms such as neural networks and boosted regression trees are a perfect match for such a task.\n",
    "\n",
    "Below we will start visualising our data, and you will see that it's difficult for yourself to intuitively predict what the ground energy state of the molecules will be. This makes this dataset a great challenge for us. \n",
    "![Vis ground energy state](http://www.pinchofintelligence.com/wp-content/uploads/2017/08/ground-energies.png)\n",
    "\n",
    "\n",
    "\n",
    "I imagine that the author is also quite happy with us, as he states:\n",
    "> I am looking for Kagglers to find the best model and reduce mean squared error as much as possible!\n",
    "\n",
    "Looks like we now know the function we have to minimise. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and loading data\n",
    "The data is here: https://www.kaggle.com/burakhmmtgl/energy-molecule\n",
    "Download it, unzip it, and place it in the datasets folder...\n",
    "\n",
    "Image called downloaddata1.png here...\n",
    "\n",
    "To load the data I'm going to use a [package called Pandas](http://pandas.pydata.org/). This package is great for reading datasets, and getting an initial understanding of what you are working with. Although we are not doing to discuss all features Pandas has, I just want you to use it one time, and know that it's out there. Pandas is already installed in the Docker image I provided, so let's dive right in: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('robobohr.csv/roboBohr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0          0          1          2          3          4  \\\n",
      "0           0  73.516695  17.817765  12.469551  12.458130  12.454607   \n",
      "1           1  73.516695  20.649126  18.527789  17.891535  17.887995   \n",
      "2           2  73.516695  17.830377  12.512263  12.404775  12.394493   \n",
      "3           3  73.516695  17.875810  17.871259  17.862402  17.850920   \n",
      "4           4  73.516695  17.883818  17.868256  17.864221  17.818540   \n",
      "\n",
      "           5          6          7          8    ...      1267  1268  1269  \\\n",
      "0  12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   0.5   \n",
      "1  17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   0.0   \n",
      "2  12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   0.0   \n",
      "3  17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   0.0   \n",
      "4  12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   0.0   \n",
      "\n",
      "   1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
      "0   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
      "1   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
      "2   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
      "3   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
      "4   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
      "\n",
      "[5 rows x 1278 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(df)\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to predict is the variable in the last axis, called Eat. To do this we are going to use all features, except for the ID, and the pubchem_id. Let's use pandas to remove these columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(['Unnamed: 0', 'pubchem_id'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also check if there is any missing data: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Eat'], axis = 1)\n",
    "Y = df['Eat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to visualise your data. One way to do this is building a scatter plot: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for x in range(30):\n",
    "    \n",
    "    df.plot.scatter(x,x+1,c=-1, colormap=plt.get_cmap('CMRmap'))\n",
    "    plt.show()\n",
    "df.plot.scatter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data\n",
    "Like we learned in our previous chapter, it's a good idea to split our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16242\n",
      "12993\n",
      "3249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(len(X))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "datasetX = X_train.values\n",
    "datasetY = Y_train.values\n",
    "#print(datasetX.shape)\n",
    "#print(datasetY.shape)\n",
    "#print(datasetY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: a first approach - the neural network we made in section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with manually constructing our network again, just like we did in section 1. \n",
    "\n",
    "## Simplifying layer building\n",
    "In chapter 1 we made our network using the following code: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_input = 1275\n",
    "n_output = 1\n",
    "inputplaceholder = tf.placeholder(dtype=tf.float32, shape=[None, n_input], name=\"inputplaceholder\")\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_output]))\n",
    "biases = tf.Variable(tf.zeros([n_output]))\n",
    "layer_1 = tf.matmul(inputplaceholder, weights)\n",
    "layer_2 = tf.add(layer_1, biases)\n",
    "outputlayer = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "## Which we can simplify to: \n",
    "weights = tf.Variable(tf.random_normal([n_input, n_output]))\n",
    "biases = tf.Variable(tf.zeros([n_output]))\n",
    "\n",
    "outputlayer = tf.nn.sigmoid(tf.add(tf.matmul(inputplaceholder, weights), biases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, typing this out for a big neural network is a lot of work, and it's easy to make errors. This is why you can build functions that perform the same thing for you. \n",
    "\n",
    "You can also use the higher level functions already available in Tensorflow. You can make the same thing using: \n",
    "\n",
    "`tf.layers.dense(inputs=inputplaceholder, units=3, activation=tf.nn.sigmoid, name=\"single_layer_neural_network\") `\n",
    "\n",
    "Another option you might want to consider is the Keras python package. Keras is a high-level API definition for neural networks. It uses Tensorflow as backend to build its functions on. A neural network even more complex than in chapter two can be made like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why stick to Tensorflow\n",
    "It's very likely that for your specific application Keras is good enough (95% sure). However, Tensorflow as been adapting the Keras API specifications, and thus Tensorflow can do roughly the same as Keras. The reason to stick to Tensorflow is that it offers a LOT of extra functionality, which will come in handy for the \"strange\" applications you want to build. \n",
    "\n",
    "Let's create a simple single-layer neural network for our new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.layers.dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"first_dense_layer/Relu:0\", shape=(?, 300), dtype=float32)\n",
      "Tensor(\"prediction_dense_layer/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"mean_squared_error/value:0\", shape=(), dtype=float32)\n",
      "Optimizer: --------\n",
      "name: \"GradientDescent\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent/update_first_dense_layer/kernel/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent/update_first_dense_layer/bias/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent/update_prediction_dense_layer/kernel/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent/update_prediction_dense_layer/bias/ApplyGradientDescent\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1275], name=\"inputplaceholder\")\n",
    "output_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"userdefinedoutput\")\n",
    "\n",
    "dense = tf.layers.dense(inputs=input_pl, units=300, activation=tf.nn.relu, name=\"first_dense_layer\")\n",
    "network_prediction = tf.layers.dense(inputs=dense, units=1, activation=None, name=\"prediction_dense_layer\")\n",
    "\n",
    "print(dense)\n",
    "print(network_prediction)\n",
    "loss = tf.losses.mean_squared_error(output_pl,network_prediction)\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "print(loss)\n",
    "print(\"Optimizer: --------\")\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.1424\n",
      "1.80829e+10\n",
      "6.19705e+33\n",
      "inf\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "zipped = list(zip(datasetX, datasetY))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "for _ in range(10):\n",
    "    datax = list()\n",
    "    datay = list()\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        samp = random.choice(zipped)\n",
    "        datax.append(samp[0])\n",
    "        datay.append([samp[1]])\n",
    "    _, l = sess.run([optimizer,loss], feed_dict={input_pl: datax, output_pl: datay})\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the learning rate\n",
    "As you can see above our loss becomes HIGHER instead of lower. About once a week I see somebody on Stackoverflow ask why this is happening, and why their network is giving nan as output. The answer is: they have to adjust their learning rate.\n",
    "\n",
    "![LR](https://qph.ec.quoracdn.net/main-qimg-3fdd9cba9f37125af1513cce7359d5cf)\n",
    "\n",
    "A nice analogy I like to use is one in which we are walking through misty mountains, and want to reach the deepest valley. Every 5 minutes we determine based on our small observation what path leads down, and continue that way. If we did this every second we could never leave small valleys. If we only did this every day we might walk right out of the valley again...\n",
    "\n",
    "Let's adjust our learning rate tenfold every time till our network is able to learn something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.6298\n",
      "23388.3\n",
      "1.84866e+08\n",
      "6.96234e+06\n",
      "73745.9\n",
      "15.0745\n",
      "9.13892\n",
      "11.6911\n",
      "9.15964\n",
      "8.7387\n",
      "10.3081\n",
      "8.73984\n",
      "3.7277\n",
      "10.907\n",
      "6.63715\n",
      "3.90247\n",
      "6.35785\n",
      "7.31322\n",
      "7.06602\n",
      "8.92422\n",
      "5.42726\n",
      "5.31991\n",
      "4.54314\n",
      "5.24391\n",
      "5.00253\n",
      "4.83912\n",
      "4.12775\n",
      "2.92109\n",
      "3.69472\n",
      "4.79255\n",
      "2.6596\n",
      "5.79554\n",
      "6.71896\n",
      "9.32833\n",
      "7.63546\n",
      "2.75049\n",
      "4.23784\n",
      "5.02964\n",
      "2.42245\n",
      "1.89069\n",
      "6.62599\n",
      "9.18916\n",
      "3.21775\n",
      "5.52249\n",
      "3.07953\n",
      "4.64409\n",
      "2.79369\n",
      "2.94723\n",
      "3.90009\n",
      "4.80753\n",
      "4.06432\n",
      "3.93916\n",
      "4.1235\n",
      "2.80194\n",
      "3.54081\n",
      "2.93596\n",
      "3.19986\n",
      "3.66544\n",
      "4.57736\n",
      "2.5486\n",
      "2.33363\n",
      "2.87099\n",
      "4.00135\n",
      "3.79339\n",
      "5.88899\n",
      "3.03487\n",
      "5.08825\n",
      "4.28325\n",
      "4.68931\n",
      "3.71945\n",
      "3.78615\n",
      "2.50575\n",
      "3.30231\n",
      "3.9835\n",
      "9.40058\n",
      "6.35499\n",
      "4.15723\n",
      "3.18969\n",
      "3.69838\n",
      "3.16087\n",
      "3.98623\n",
      "2.72149\n",
      "2.86606\n",
      "3.20644\n",
      "2.44972\n",
      "2.94869\n",
      "2.8722\n",
      "2.15478\n",
      "2.33837\n",
      "4.53753\n",
      "3.03143\n",
      "2.95529\n",
      "3.18317\n",
      "2.72946\n",
      "3.59408\n",
      "2.50363\n",
      "1.6413\n",
      "3.01369\n",
      "5.15217\n",
      "2.59173\n",
      "3.46669\n",
      "2.59793\n",
      "3.34344\n",
      "7.03603\n",
      "8.65026\n",
      "3.35921\n",
      "4.4433\n",
      "6.00104\n",
      "2.27789\n",
      "3.76752\n",
      "2.34774\n",
      "1.31526\n",
      "2.37359\n",
      "2.33859\n",
      "9.02334\n",
      "2.7187\n",
      "3.60152\n",
      "2.72727\n",
      "3.22954\n",
      "1.53945\n",
      "4.0576\n",
      "2.58843\n",
      "2.06233\n",
      "2.22476\n",
      "2.4022\n",
      "2.64492\n",
      "3.7411\n",
      "3.65317\n",
      "2.0091\n",
      "4.10028\n",
      "2.06635\n",
      "2.2315\n",
      "3.32777\n",
      "6.86138\n",
      "6.63554\n",
      "3.74944\n",
      "2.52699\n",
      "1.87021\n",
      "3.13108\n",
      "1.58229\n",
      "1.0483\n",
      "2.2849\n",
      "1.77711\n",
      "5.32072\n",
      "2.93134\n",
      "2.1938\n",
      "1.92663\n",
      "3.04423\n",
      "4.24418\n",
      "1.77147\n",
      "3.01187\n",
      "3.37721\n",
      "4.05667\n",
      "2.49268\n",
      "3.77035\n",
      "5.41616\n",
      "6.59924\n",
      "1.73903\n",
      "1.73481\n",
      "2.88397\n",
      "2.93308\n",
      "1.70996\n",
      "1.93732\n",
      "2.03084\n",
      "1.65664\n",
      "3.29869\n",
      "4.56389\n",
      "2.27172\n",
      "1.78195\n",
      "1.45421\n",
      "1.62378\n",
      "7.56992\n",
      "6.13244\n",
      "6.80951\n",
      "15.2274\n",
      "23.1085\n",
      "10.7954\n",
      "7.47733\n",
      "5.83279\n",
      "3.51415\n",
      "4.35147\n",
      "5.35642\n",
      "1.82068\n",
      "1.81725\n",
      "3.52984\n",
      "2.30865\n",
      "2.6785\n",
      "2.94964\n",
      "4.41856\n",
      "2.63208\n",
      "4.25474\n",
      "2.44254\n",
      "4.77731\n",
      "4.49701\n",
      "2.02394\n",
      "1.41154\n",
      "3.5358\n",
      "4.4027\n",
      "1.31789\n",
      "1.59849\n",
      "1.94583\n",
      "9.4297\n",
      "4.56869\n",
      "2.9223\n",
      "3.17724\n",
      "1.43625\n",
      "1.46684\n",
      "3.25196\n",
      "1.61808\n",
      "1.26817\n",
      "1.16278\n",
      "1.44613\n",
      "2.19918\n",
      "2.8889\n",
      "6.27047\n",
      "8.12768\n",
      "1.70417\n",
      "2.77521\n",
      "5.25271\n",
      "5.35013\n",
      "1.87998\n",
      "2.65299\n",
      "1.11333\n",
      "2.62838\n",
      "2.59169\n",
      "1.23536\n",
      "1.73268\n",
      "4.16238\n",
      "2.33868\n",
      "2.0359\n",
      "2.81614\n",
      "4.26612\n",
      "2.11961\n",
      "3.78758\n",
      "4.45183\n",
      "2.28901\n",
      "2.13737\n",
      "3.29952\n",
      "2.443\n",
      "3.61896\n",
      "4.04961\n",
      "2.32504\n",
      "2.62089\n",
      "1.80916\n",
      "2.34052\n",
      "1.3733\n",
      "2.64789\n",
      "1.77394\n",
      "1.94179\n",
      "1.35452\n",
      "0.848712\n",
      "4.66722\n",
      "4.93725\n",
      "1.58339\n",
      "2.31171\n",
      "5.08558\n",
      "3.13488\n",
      "2.51645\n",
      "1.29214\n",
      "3.32264\n",
      "2.1665\n",
      "1.6207\n",
      "3.07482\n",
      "2.25532\n",
      "1.79569\n",
      "1.62051\n",
      "1.59571\n",
      "1.83541\n",
      "1.98488\n",
      "3.02953\n",
      "1.77236\n",
      "1.73324\n",
      "3.44025\n",
      "2.63486\n",
      "1.17136\n",
      "1.21451\n",
      "2.69908\n",
      "1.94272\n",
      "2.33092\n",
      "5.35123\n",
      "1.89213\n",
      "2.66704\n",
      "1.79198\n",
      "1.28426\n",
      "2.67956\n",
      "1.64158\n",
      "1.54101\n",
      "1.8725\n",
      "2.23901\n",
      "4.78154\n",
      "1.1381\n",
      "1.12633\n",
      "5.2814\n",
      "4.49523\n",
      "1.94387\n",
      "5.26272\n",
      "1.28085\n",
      "1.66534\n",
      "1.86501\n",
      "1.60835\n",
      "1.87096\n",
      "1.54286\n",
      "3.64327\n",
      "2.10029\n",
      "1.48777\n",
      "1.68058\n",
      "3.4625\n",
      "1.24195\n",
      "1.67638\n",
      "1.53783\n",
      "2.49362\n",
      "2.23845\n",
      "2.04766\n",
      "1.19922\n",
      "2.92698\n",
      "1.58509\n",
      "4.17127\n",
      "2.11318\n",
      "2.86795\n",
      "3.188\n",
      "3.59969\n",
      "3.92161\n",
      "1.89369\n",
      "1.80406\n",
      "3.11648\n",
      "1.86391\n",
      "1.12079\n",
      "1.35169\n",
      "2.13255\n",
      "2.7517\n",
      "3.07991\n",
      "1.10304\n",
      "3.20271\n",
      "1.1117\n",
      "2.26373\n",
      "2.9268\n",
      "2.93085\n",
      "1.96094\n",
      "1.71516\n",
      "1.43223\n",
      "3.56323\n",
      "2.08259\n",
      "2.13041\n",
      "2.5262\n",
      "2.69997\n",
      "1.64963\n",
      "1.67415\n",
      "1.09859\n",
      "1.63388\n",
      "1.50988\n",
      "1.45322\n",
      "1.75473\n",
      "1.94683\n",
      "1.34582\n",
      "2.82524\n",
      "2.01921\n",
      "0.592453\n",
      "1.38479\n",
      "1.96063\n",
      "0.823822\n",
      "2.19134\n",
      "2.63628\n",
      "1.90403\n",
      "1.96972\n",
      "2.01373\n",
      "3.24393\n",
      "2.63936\n",
      "5.43644\n",
      "4.14635\n",
      "1.95291\n",
      "3.38187\n",
      "4.10565\n",
      "2.52207\n",
      "4.04441\n",
      "1.83166\n",
      "1.02057\n",
      "1.32988\n",
      "1.34668\n",
      "1.9267\n",
      "2.47839\n",
      "0.926872\n",
      "1.83781\n",
      "1.2899\n",
      "1.63475\n",
      "1.37264\n",
      "2.57294\n",
      "1.15928\n",
      "1.49159\n",
      "2.12135\n",
      "2.29667\n",
      "1.20413\n",
      "1.6637\n",
      "1.99464\n",
      "1.60812\n",
      "1.5682\n",
      "1.53956\n",
      "1.56417\n",
      "2.11525\n",
      "1.88731\n",
      "5.20004\n",
      "4.15139\n",
      "2.80887\n",
      "2.36334\n",
      "1.72215\n",
      "1.733\n",
      "2.07223\n",
      "1.27542\n",
      "1.70407\n",
      "2.25207\n",
      "1.42499\n",
      "2.44326\n",
      "1.39196\n",
      "1.60868\n",
      "0.81821\n",
      "1.6538\n",
      "1.07126\n",
      "1.51111\n",
      "7.41766\n",
      "5.36296\n",
      "1.14722\n",
      "1.70527\n",
      "0.681594\n",
      "1.6175\n",
      "2.00934\n",
      "0.813279\n",
      "1.11228\n",
      "1.65385\n",
      "1.60188\n",
      "1.02911\n",
      "0.836748\n",
      "0.858018\n",
      "0.925067\n",
      "4.03493\n",
      "9.66681\n",
      "3.64919\n",
      "2.8687\n",
      "4.31234\n",
      "2.04124\n",
      "2.84253\n",
      "3.54707\n",
      "3.55202\n",
      "1.77343\n",
      "2.33655\n",
      "1.2123\n",
      "1.00374\n",
      "1.47035\n",
      "3.31729\n",
      "2.16702\n",
      "1.44929\n",
      "1.47438\n",
      "0.808034\n",
      "1.07736\n",
      "3.13483\n",
      "1.35793\n",
      "1.50544\n",
      "1.53338\n",
      "1.87208\n",
      "1.36533\n",
      "3.86872\n",
      "1.19187\n",
      "1.88734\n",
      "1.63674\n",
      "0.922749\n",
      "1.21052\n",
      "1.4841\n",
      "2.57224\n",
      "1.4852\n",
      "1.79982\n",
      "0.976871\n",
      "2.60203\n",
      "2.59113\n",
      "1.01611\n",
      "2.66639\n",
      "1.5066\n",
      "1.71146\n",
      "3.85987\n",
      "3.76323\n",
      "1.79589\n",
      "5.44581\n",
      "3.93604\n",
      "2.58298\n",
      "1.69587\n",
      "3.96003\n",
      "1.41925\n",
      "1.7774\n",
      "1.14333\n",
      "1.40779\n",
      "1.1347\n",
      "1.79657\n",
      "1.44744\n",
      "1.21402\n",
      "1.67351\n",
      "0.94948\n",
      "1.8785\n",
      "2.23756\n",
      "1.70564\n",
      "1.53491\n",
      "1.77987\n",
      "2.58235\n",
      "2.39695\n",
      "2.91221\n",
      "1.93243\n",
      "1.52681\n",
      "0.817609\n",
      "2.34963\n",
      "1.43394\n",
      "4.41729\n",
      "2.18462\n",
      "1.52871\n",
      "2.44904\n",
      "1.17118\n",
      "3.47816\n",
      "2.4246\n",
      "1.66323\n",
      "2.58172\n",
      "1.31799\n",
      "1.33034\n",
      "3.0269\n",
      "2.07571\n",
      "1.18395\n",
      "1.44898\n",
      "2.76477\n",
      "1.14559\n",
      "0.768697\n",
      "1.58497\n",
      "1.29098\n",
      "0.708539\n",
      "1.06347\n",
      "1.10686\n",
      "1.19775\n",
      "1.08226\n",
      "2.7087\n",
      "0.994761\n",
      "1.49632\n",
      "1.4514\n",
      "0.784779\n",
      "3.88096\n",
      "1.23658\n",
      "2.05971\n",
      "2.36128\n",
      "5.89368\n",
      "6.74937\n",
      "7.92598\n",
      "3.82499\n",
      "1.59544\n",
      "1.53213\n",
      "1.1796\n",
      "1.99671\n",
      "1.03948\n",
      "2.08872\n",
      "1.58039\n",
      "1.08537\n",
      "3.24486\n",
      "1.18928\n",
      "1.01342\n",
      "1.11602\n",
      "1.66024\n",
      "1.46736\n",
      "0.968001\n",
      "1.08868\n",
      "2.32867\n",
      "1.49346\n",
      "1.37298\n",
      "1.4586\n",
      "1.76633\n",
      "1.3991\n",
      "1.29396\n",
      "2.02282\n",
      "1.45205\n",
      "0.467073\n",
      "2.20376\n",
      "1.76784\n",
      "2.04522\n",
      "1.04644\n",
      "1.19648\n",
      "1.56758\n",
      "0.890392\n",
      "2.00824\n",
      "0.759003\n",
      "1.38887\n",
      "1.71226\n",
      "2.01062\n",
      "1.313\n",
      "1.20051\n",
      "0.971088\n",
      "2.96131\n",
      "3.59754\n",
      "1.69884\n",
      "2.23427\n",
      "1.48384\n",
      "1.32154\n",
      "1.06447\n",
      "1.784\n",
      "0.646008\n",
      "2.22791\n",
      "1.3755\n",
      "1.44907\n",
      "1.74764\n",
      "1.03638\n",
      "0.579647\n",
      "1.37391\n",
      "0.68027\n",
      "0.856615\n",
      "2.84683\n",
      "6.07372\n",
      "2.4584\n",
      "0.91247\n",
      "2.74983\n",
      "2.19464\n",
      "1.02523\n",
      "9.76678\n",
      "2.58373\n",
      "2.34164\n",
      "1.20833\n",
      "2.22928\n",
      "2.5225\n",
      "3.74824\n",
      "3.26638\n",
      "1.79645\n",
      "2.52492\n",
      "4.62266\n",
      "2.34856\n",
      "2.48108\n",
      "2.03641\n",
      "3.34456\n",
      "2.80787\n",
      "1.31557\n",
      "1.88049\n",
      "2.07614\n",
      "1.80207\n",
      "1.27198\n",
      "1.95513\n",
      "1.60287\n",
      "1.72279\n",
      "1.95741\n",
      "2.17137\n",
      "2.22812\n",
      "1.34218\n",
      "1.13771\n",
      "0.998087\n",
      "2.19283\n",
      "1.16546\n",
      "1.90059\n",
      "0.992663\n",
      "2.6024\n",
      "1.62162\n",
      "0.859639\n",
      "2.14342\n",
      "1.87042\n",
      "1.63831\n",
      "0.794327\n",
      "0.824995\n",
      "2.29668\n",
      "0.719357\n",
      "0.88123\n",
      "2.34895\n",
      "0.898056\n",
      "1.47073\n",
      "1.69512\n",
      "1.40311\n",
      "3.04482\n",
      "2.62639\n",
      "2.79687\n",
      "1.4641\n",
      "1.18705\n",
      "1.69256\n",
      "2.61313\n",
      "2.24031\n",
      "1.50027\n",
      "5.92629\n",
      "4.10534\n",
      "1.13236\n",
      "0.86208\n",
      "2.43076\n",
      "2.51163\n",
      "2.49281\n",
      "2.18002\n",
      "2.79514\n",
      "1.12905\n",
      "1.08563\n",
      "2.24457\n",
      "1.52339\n",
      "1.48752\n",
      "3.23318\n",
      "1.16803\n",
      "1.41261\n",
      "2.99424\n",
      "0.5257\n",
      "1.60242\n",
      "1.41881\n",
      "0.613145\n",
      "1.28791\n",
      "2.8285\n",
      "1.54814\n",
      "1.06764\n",
      "0.868052\n",
      "0.642144\n",
      "1.69835\n",
      "1.09551\n",
      "1.74313\n",
      "1.69206\n",
      "0.933321\n",
      "0.850464\n",
      "1.76462\n",
      "1.05152\n",
      "0.861651\n",
      "1.08671\n",
      "0.631179\n",
      "1.21501\n",
      "0.981384\n",
      "1.51517\n",
      "1.06413\n",
      "0.922269\n",
      "1.14425\n",
      "2.03863\n",
      "0.677717\n",
      "1.34212\n",
      "1.50171\n",
      "1.06177\n",
      "2.30938\n",
      "4.01952\n",
      "1.38286\n",
      "0.832431\n",
      "2.39503\n",
      "0.785773\n",
      "1.52245\n",
      "0.842053\n",
      "1.87179\n",
      "2.14309\n",
      "0.994457\n",
      "1.34304\n",
      "2.47956\n",
      "1.51374\n",
      "1.41913\n",
      "1.16106\n",
      "1.09837\n",
      "4.38974\n",
      "1.23098\n",
      "1.43253\n",
      "1.6963\n",
      "1.75998\n",
      "0.828362\n",
      "1.49255\n",
      "1.00419\n",
      "0.741617\n",
      "1.33906\n",
      "1.49431\n",
      "1.06415\n",
      "2.50089\n",
      "1.51346\n",
      "1.45032\n",
      "2.83317\n",
      "1.45738\n",
      "4.80953\n",
      "2.34969\n",
      "2.26895\n",
      "1.93284\n",
      "1.7021\n",
      "3.42643\n",
      "2.45438\n",
      "0.992543\n",
      "6.8515\n",
      "3.38783\n",
      "2.25971\n",
      "2.39921\n",
      "2.12909\n",
      "2.69691\n",
      "3.79767\n",
      "2.07273\n",
      "1.11444\n",
      "0.991171\n",
      "1.17896\n",
      "1.34195\n",
      "2.96259\n",
      "2.08878\n",
      "1.04262\n",
      "1.29859\n",
      "1.074\n",
      "2.37071\n",
      "2.99148\n",
      "1.51882\n",
      "1.42884\n",
      "1.23883\n",
      "1.0012\n",
      "1.3249\n",
      "1.39584\n",
      "1.92468\n",
      "2.9169\n",
      "1.477\n",
      "1.86268\n",
      "2.67578\n",
      "1.03348\n",
      "1.00178\n",
      "0.92812\n",
      "2.09123\n",
      "0.730505\n",
      "0.554578\n",
      "0.783067\n",
      "1.28108\n",
      "0.789451\n",
      "3.9717\n",
      "0.742089\n",
      "1.00091\n",
      "3.29282\n",
      "0.806938\n",
      "0.687237\n",
      "0.81566\n",
      "1.272\n",
      "1.01372\n",
      "0.690446\n",
      "0.90322\n",
      "1.36495\n",
      "1.28716\n",
      "1.01236\n",
      "0.887812\n",
      "1.14323\n",
      "1.32448\n",
      "1.24836\n",
      "1.24545\n",
      "1.07772\n",
      "1.11017\n",
      "1.27467\n",
      "1.44135\n",
      "1.11539\n",
      "1.36812\n",
      "1.3994\n",
      "1.06204\n",
      "0.954131\n",
      "2.26354\n",
      "2.7511\n",
      "0.678612\n",
      "0.731203\n",
      "2.69112\n",
      "2.22714\n",
      "1.52693\n",
      "1.11097\n",
      "0.852696\n",
      "1.69431\n",
      "3.37174\n",
      "1.28928\n",
      "0.956738\n",
      "0.935101\n",
      "4.88406\n",
      "1.43583\n",
      "1.42\n",
      "1.24168\n",
      "0.836296\n",
      "1.11882\n",
      "1.30996\n",
      "0.407711\n",
      "0.551001\n",
      "1.40476\n",
      "2.27649\n",
      "1.72713\n",
      "1.53245\n",
      "0.984959\n",
      "1.69311\n",
      "1.34779\n",
      "1.31624\n",
      "1.2238\n",
      "2.30764\n",
      "1.75897\n",
      "1.15741\n",
      "2.64659\n",
      "4.26816\n",
      "2.19216\n",
      "1.26676\n",
      "2.76477\n",
      "1.79372\n",
      "2.23145\n",
      "1.68362\n",
      "3.56299\n",
      "1.05231\n",
      "1.65375\n",
      "1.22037\n",
      "1.35344\n",
      "1.42899\n",
      "1.11232\n",
      "1.4524\n",
      "0.912963\n",
      "1.14548\n",
      "1.35883\n",
      "2.23722\n",
      "1.02024\n",
      "0.778854\n",
      "0.709521\n",
      "0.661355\n",
      "1.00904\n",
      "0.72028\n",
      "1.40347\n",
      "1.3901\n",
      "1.03352\n",
      "2.80884\n",
      "1.94417\n",
      "1.09292\n",
      "2.25834\n",
      "2.51619\n",
      "1.66224\n",
      "0.870261\n",
      "0.95688\n",
      "0.840811\n",
      "1.31142\n",
      "2.22826\n",
      "1.31454\n",
      "0.876308\n",
      "2.08296\n",
      "1.17314\n",
      "0.981876\n",
      "0.841584\n",
      "0.797634\n",
      "0.875794\n",
      "0.815865\n",
      "1.2425\n",
      "1.42551\n",
      "0.480354\n",
      "3.40584\n",
      "2.80107\n",
      "1.78158\n",
      "1.68182\n",
      "0.920924\n",
      "1.18765\n",
      "3.02845\n",
      "1.12605\n",
      "0.722027\n",
      "0.656735\n",
      "1.27546\n",
      "1.34338\n",
      "1.63276\n",
      "0.926687\n",
      "1.78379\n",
      "1.68917\n",
      "1.75742\n",
      "0.934843\n",
      "0.925369\n",
      "1.62137\n",
      "1.14734\n",
      "2.08802\n",
      "0.590154\n",
      "1.17436\n",
      "1.02412\n",
      "1.37501\n",
      "0.956997\n",
      "0.461389\n",
      "0.996485\n",
      "0.776259\n",
      "1.93094\n",
      "1.45929\n",
      "0.897824\n",
      "0.827899\n",
      "1.06381\n",
      "0.499854\n",
      "1.00472\n",
      "0.855143\n",
      "6.8774\n",
      "1.80082\n",
      "1.10542\n",
      "0.745022\n",
      "0.833693\n",
      "2.70159\n",
      "2.09592\n",
      "1.46455\n",
      "0.812715\n",
      "0.708712\n",
      "1.40706\n",
      "1.34499\n",
      "2.54428\n",
      "1.63221\n",
      "0.556662\n",
      "1.43239\n",
      "0.784706\n",
      "1.90973\n",
      "1.04168\n",
      "0.887016\n",
      "0.690782\n",
      "0.957085\n",
      "1.36514\n",
      "0.931352\n",
      "1.40314\n",
      "1.64229\n",
      "0.770143\n",
      "0.627634\n",
      "2.45439\n",
      "1.04537\n",
      "1.51305\n",
      "1.04055\n",
      "1.14946\n",
      "2.03817\n",
      "0.400663\n",
      "1.22322\n",
      "1.16099\n",
      "1.08202\n",
      "1.51128\n",
      "1.31464\n",
      "1.1354\n",
      "0.801219\n",
      "1.25601\n",
      "0.619557\n",
      "1.5964\n",
      "1.11514\n",
      "1.4055\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0005).minimize(loss)\n",
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)\n",
    "\n",
    "zipped = list(zip(datasetX, datasetY))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "loss_history = list()\n",
    "for _ in range(1000):\n",
    "    datax = list()\n",
    "    datay = list()\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        samp = random.choice(zipped)\n",
    "        datax.append(samp[0])\n",
    "        datay.append([samp[1]])\n",
    "    _, l = sess.run([optimizer,loss], feed_dict={input_pl: datax, output_pl: datay})\n",
    "    print(l)\n",
    "    loss_history.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f585c002198>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f585e6dfe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising your graph\n",
    "When working with Tensorflow it's easy to make a mistake. For example, when adding an extra layer to your network it's easy to forget to connect it to other layers. A cool feature of our Jupyter notebook is that you can visualise the graph you made. The source of this code can be found in [this Stackoverflow answer](https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter/38192374#38192374). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.724249559628024&quot;).pbtxt = 'node {\\n  name: &quot;inputplaceholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 1275\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;userdefinedoutput&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\373\\\\004\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.06172133982181549\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.06172133982181549\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1275\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;first_dense_layer/kernel&quot;\\n  input: &quot;first_dense_layer/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;first_dense_layer/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;first_dense_layer/bias&quot;\\n  input: &quot;first_dense_layer/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;first_dense_layer/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;inputplaceholder&quot;\\n  input: &quot;first_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;first_dense_layer/MatMul&quot;\\n  input: &quot;first_dense_layer/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;first_dense_layer/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;first_dense_layer/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.14118623733520508\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.14118623733520508\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;prediction_dense_layer/kernel&quot;\\n  input: &quot;prediction_dense_layer/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;prediction_dense_layer/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;prediction_dense_layer/bias&quot;\\n  input: &quot;prediction_dense_layer/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;prediction_dense_layer/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  input: &quot;prediction_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;prediction_dense_layer/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;prediction_dense_layer/MatMul&quot;\\n  input: &quot;prediction_dense_layer/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/SquaredDifference&quot;\\n  op: &quot;SquaredDifference&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/assert_broadcastable/weights&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/assert_broadcastable/weights/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/assert_broadcastable/weights/rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/assert_broadcastable/values/shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/assert_broadcastable/values/rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  op: &quot;NoOp&quot;\\n}\\nnode {\\n  name: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;mean_squared_error/Mul&quot;\\n  input: &quot;mean_squared_error/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/Equal/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  input: &quot;mean_squared_error/num_present/Equal/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/ones_like/Shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;mean_squared_error/num_present/ones_like/Shape&quot;\\n  input: &quot;mean_squared_error/num_present/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/num_present/Equal&quot;\\n  input: &quot;mean_squared_error/num_present/zeros_like&quot;\\n  input: &quot;mean_squared_error/num_present/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/assert_broadcastable/weights/shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/assert_broadcastable/weights/rank&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/assert_broadcastable/values/shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/assert_broadcastable/values/rank&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/ones_like/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  input: &quot;^mean_squared_error/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  input: &quot;^mean_squared_error/num_present/broadcast_weights/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like/Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/broadcast_weights&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/num_present/Select&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/num_present&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights&quot;\\n  input: &quot;mean_squared_error/num_present/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Const_1&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;mean_squared_error/Sum&quot;\\n  input: &quot;mean_squared_error/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Greater/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Greater&quot;\\n  op: &quot;Greater&quot;\\n  input: &quot;mean_squared_error/num_present&quot;\\n  input: &quot;mean_squared_error/Greater/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Equal/y&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;mean_squared_error/num_present&quot;\\n  input: &quot;mean_squared_error/Equal/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/ones_like/Shape&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/ones_like/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/ones_like&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;mean_squared_error/ones_like/Shape&quot;\\n  input: &quot;mean_squared_error/ones_like/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;mean_squared_error/ones_like&quot;\\n  input: &quot;mean_squared_error/num_present&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/div&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;mean_squared_error/Sum_1&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^mean_squared_error/assert_broadcastable/static_scalar_check_success&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mean_squared_error/value&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;mean_squared_error/div&quot;\\n  input: &quot;mean_squared_error/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/value_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mean_squared_error/value_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients/mean_squared_error/value_grad/zeros_like&quot;\\n  input: &quot;gradients/Fill&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients/mean_squared_error/value_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/value_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/value_grad/Select_1&quot;\\n  input: &quot;^gradients/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/value_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Shape&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/RealDiv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/RealDiv&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Sum&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;mean_squared_error/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/RealDiv_1&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Neg&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/RealDiv_2&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/RealDiv_1&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/RealDiv_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/mul&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Sum_1&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/div_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/div_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/Reshape_1&quot;\\n  input: &quot;^gradients/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/div_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Select_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Select_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/mean_squared_error/Select_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Select_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients/mean_squared_error/Select_grad/zeros_like&quot;\\n  input: &quot;gradients/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients/mean_squared_error/Select_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Select_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/Select_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/Select_grad/Select_1&quot;\\n  input: &quot;^gradients/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/Select_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_1_grad/Tile&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Shape&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_grad/Tile&quot;\\n  input: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/mul&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Sum&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;gradients/mean_squared_error/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/mul_1&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mean_squared_error/num_present_grad/Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/num_present_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mean_squared_error/num_present_grad/Tile&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/num_present/Select&quot;\\n  input: &quot;gradients/mean_squared_error/num_present_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  input: &quot;^gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  input: &quot;gradients/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  input: &quot;^gradients/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  input: &quot;^gradients/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;prediction_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  input: &quot;gradients/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/first_dense_layer/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;inputplaceholder&quot;\\n  input: &quot;gradients/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/first_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.05000000074505806\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^first_dense_layer/kernel/Assign&quot;\\n  input: &quot;^first_dense_layer/bias/Assign&quot;\\n  input: &quot;^prediction_dense_layer/kernel/Assign&quot;\\n  input: &quot;^prediction_dense_layer/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients_1/Shape&quot;\\n  input: &quot;gradients_1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/value_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients_1/Fill&quot;\\n  input: &quot;gradients_1/mean_squared_error/value_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients_1/mean_squared_error/value_grad/zeros_like&quot;\\n  input: &quot;gradients_1/Fill&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients_1/mean_squared_error/value_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients_1/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/value_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/value_grad/Select_1&quot;\\n  input: &quot;^gradients_1/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/value_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Shape&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/RealDiv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_1/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/RealDiv&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;mean_squared_error/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/RealDiv_1&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Neg&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/RealDiv_2&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/RealDiv_1&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/RealDiv_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Sum_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/div_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/div_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/Reshape_1&quot;\\n  input: &quot;^gradients_1/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/div_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Select_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Select_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/Select_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Select_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients_1/mean_squared_error/Select_grad/zeros_like&quot;\\n  input: &quot;gradients_1/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Select_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Select_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/Select_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/Select_grad/Select_1&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/Select_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_1_grad/Tile&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_grad/Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Shape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_grad/Tile&quot;\\n  input: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;gradients_1/mean_squared_error/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/mul_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present_grad/Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present_grad/Tile&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/num_present/Select&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  input: &quot;^gradients_1/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  input: &quot;gradients_1/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  input: &quot;^gradients_1/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients_1/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  input: &quot;^gradients_1/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;prediction_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/first_dense_layer/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;inputplaceholder&quot;\\n  input: &quot;gradients_1/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/first_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0005000000237487257\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;gradients_1/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;gradients_1/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;gradients_1/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent_1/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init_1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^first_dense_layer/kernel/Assign&quot;\\n  input: &quot;^first_dense_layer/bias/Assign&quot;\\n  input: &quot;^prediction_dense_layer/kernel/Assign&quot;\\n  input: &quot;^prediction_dense_layer/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients_2/Shape&quot;\\n  input: &quot;gradients_2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/value_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients_2/Fill&quot;\\n  input: &quot;gradients_2/mean_squared_error/value_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients_2/mean_squared_error/value_grad/zeros_like&quot;\\n  input: &quot;gradients_2/Fill&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients_2/mean_squared_error/value_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients_2/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/value_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/value_grad/Select_1&quot;\\n  input: &quot;^gradients_2/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/value_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Shape&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/RealDiv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_2/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/RealDiv&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;mean_squared_error/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/RealDiv_1&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Neg&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/RealDiv_2&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/RealDiv_1&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/RealDiv_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Sum_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/div_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/div_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/Reshape_1&quot;\\n  input: &quot;^gradients_2/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/div_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Select_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Select_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/Select_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Select_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients_2/mean_squared_error/Select_grad/zeros_like&quot;\\n  input: &quot;gradients_2/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Select_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Select_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/Select_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/Select_grad/Select_1&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/Select_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_1_grad/Tile&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_grad/Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Shape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_grad/Tile&quot;\\n  input: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;gradients_2/mean_squared_error/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/mul_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present_grad/Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present_grad/Tile&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/num_present/Select&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  input: &quot;^gradients_2/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  input: &quot;gradients_2/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  input: &quot;^gradients_2/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients_2/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  input: &quot;^gradients_2/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;prediction_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_2/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_2/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_2/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_2/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/first_dense_layer/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_2/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_2/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;inputplaceholder&quot;\\n  input: &quot;gradients_2/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_2/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_2/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_2/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/first_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_2/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_2/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_2/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_2/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_2/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0005000000237487257\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_2/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent_2/learning_rate&quot;\\n  input: &quot;gradients_2/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_2/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/bias&quot;\\n  input: &quot;GradientDescent_2/learning_rate&quot;\\n  input: &quot;gradients_2/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_2/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent_2/learning_rate&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_2/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/bias&quot;\\n  input: &quot;GradientDescent_2/learning_rate&quot;\\n  input: &quot;gradients_2/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_2&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent_2/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_2/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_2/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_2/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init_2&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^first_dense_layer/kernel/Assign&quot;\\n  input: &quot;^first_dense_layer/bias/Assign&quot;\\n  input: &quot;^prediction_dense_layer/kernel/Assign&quot;\\n  input: &quot;^prediction_dense_layer/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;init_3&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^first_dense_layer/kernel/Assign&quot;\\n  input: &quot;^first_dense_layer/bias/Assign&quot;\\n  input: &quot;^prediction_dense_layer/kernel/Assign&quot;\\n  input: &quot;^prediction_dense_layer/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients_3/Shape&quot;\\n  input: &quot;gradients_3/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/value_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients_3/Fill&quot;\\n  input: &quot;gradients_3/mean_squared_error/value_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Greater&quot;\\n  input: &quot;gradients_3/mean_squared_error/value_grad/zeros_like&quot;\\n  input: &quot;gradients_3/Fill&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients_3/mean_squared_error/value_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/value_grad/Select&quot;\\n  input: &quot;^gradients_3/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/value_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/value_grad/Select_1&quot;\\n  input: &quot;^gradients_3/mean_squared_error/value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/value_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Shape&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/RealDiv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_3/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/RealDiv&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;mean_squared_error/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/RealDiv_1&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Neg&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/RealDiv_2&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/RealDiv_1&quot;\\n  input: &quot;mean_squared_error/Select&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/RealDiv_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Sum_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/div_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/div_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/Reshape_1&quot;\\n  input: &quot;^gradients_3/mean_squared_error/div_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/div_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_1_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Select_grad/zeros_like&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Select_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/Select_grad/zeros_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Select_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;mean_squared_error/Equal&quot;\\n  input: &quot;gradients_3/mean_squared_error/Select_grad/zeros_like&quot;\\n  input: &quot;gradients_3/mean_squared_error/div_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Select_grad/Select_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Select_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/Select_grad/Select&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/Select_grad/Select&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/Select_grad/Select_1&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Select_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/Select_grad/Select_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_1_grad/Tile&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_grad/Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Shape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_grad/Tile&quot;\\n  input: &quot;mean_squared_error/ToFloat_3/x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/SquaredDifference&quot;\\n  input: &quot;gradients_3/mean_squared_error/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/mul_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/Select_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present_grad/Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present_grad/Tile&quot;\\n  input: &quot;mean_squared_error/num_present/broadcast_weights/ones_like&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;mean_squared_error/num_present/Select&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/mul_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Sum_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n  input: &quot;^gradients_3/mean_squared_error/num_present/broadcast_weights_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/num_present/broadcast_weights_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/scalar&quot;\\n  input: &quot;gradients_3/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;prediction_dense_layer/BiasAdd&quot;\\n  input: &quot;userdefinedoutput&quot;\\n  input: &quot;^gradients_3/mean_squared_error/Mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/mul&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/mul_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Sum_1&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n  input: &quot;^gradients_3/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n  input: &quot;^gradients_3/mean_squared_error/SquaredDifference_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/SquaredDifference_grad/Neg&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/mean_squared_error/SquaredDifference_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/mean_squared_error/SquaredDifference_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/prediction_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;prediction_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/prediction_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_3/prediction_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/prediction_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_3/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_3/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/first_dense_layer/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_3/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/first_dense_layer/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_3/first_dense_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/first_dense_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_3/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;first_dense_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;inputplaceholder&quot;\\n  input: &quot;gradients_3/first_dense_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_3/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_3/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/first_dense_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_3/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/first_dense_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_3/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_3/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_3/first_dense_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_3/first_dense_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_3/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0005000000237487257\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_3/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent_3/learning_rate&quot;\\n  input: &quot;gradients_3/first_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_3/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;first_dense_layer/bias&quot;\\n  input: &quot;GradientDescent_3/learning_rate&quot;\\n  input: &quot;gradients_3/first_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@first_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_3/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/kernel&quot;\\n  input: &quot;GradientDescent_3/learning_rate&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_3/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;prediction_dense_layer/bias&quot;\\n  input: &quot;GradientDescent_3/learning_rate&quot;\\n  input: &quot;gradients_3/prediction_dense_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@prediction_dense_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_3&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent_3/update_first_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_3/update_first_dense_layer/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_3/update_prediction_dense_layer/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_3/update_prediction_dense_layer/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init_4&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^first_dense_layer/kernel/Assign&quot;\\n  input: &quot;^first_dense_layer/bias/Assign&quot;\\n  input: &quot;^prediction_dense_layer/kernel/Assign&quot;\\n  input: &quot;^prediction_dense_layer/bias/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.724249559628024&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer neural network\n",
    "So far we only made a single-layer neural network. Let's try to create a multi-layer neural network, and see if this improves our error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1275], name=\"inputplaceholder\")\n",
    "output_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"userdefinedoutput\")\n",
    "\n",
    "activation_function = tf.nn.relu\n",
    "dense = tf.layers.dense(inputs=input_pl, units=300, activation=activation_function, name=\"first_dense_layer\")\n",
    "dense2 = tf.layers.dense(inputs=dense, units=150, activation=activation_function, name=\"second_dense_layer\")\n",
    "dense3 = tf.layers.dense(inputs=dense2, units=50, activation=activation_function, name=\"third_dense_layer\")\n",
    "dense4 = tf.layers.dense(inputs=dense3, units=16, activation=activation_function, name=\"fourth_dense_layer\")\n",
    "outputnetwork = tf.layers.dense(inputs=dense4, units=1, activation=None, name=\"prediction_dense_layer\")\n",
    "\n",
    "loss = tf.losses.mean_squared_error(output_pl,outputnetwork)\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.957\n",
      "55.762\n",
      "33.4379\n",
      "17.8555\n",
      "22.5956\n",
      "11.7726\n",
      "6.28476\n",
      "6.3221\n",
      "9.9888\n",
      "5.66833\n",
      "9.97976\n",
      "7.94989\n",
      "9.2701\n",
      "8.00904\n",
      "5.75573\n",
      "5.3754\n",
      "5.32311\n",
      "6.37321\n",
      "5.59849\n",
      "7.32941\n",
      "4.87412\n",
      "4.12798\n",
      "3.17282\n",
      "6.17139\n",
      "14.4891\n",
      "11.5342\n",
      "9.07369\n",
      "5.08301\n",
      "9.66924\n",
      "9.54167\n",
      "5.3235\n",
      "3.9867\n",
      "8.78198\n",
      "4.00472\n",
      "2.98764\n",
      "2.77655\n",
      "6.92563\n",
      "6.86062\n",
      "6.00287\n",
      "3.45765\n",
      "2.49817\n",
      "7.54812\n",
      "4.83152\n",
      "4.84309\n",
      "3.36135\n",
      "3.09152\n",
      "3.69519\n",
      "7.44834\n",
      "3.46704\n",
      "4.7096\n",
      "2.7428\n",
      "3.7784\n",
      "4.50703\n",
      "2.66228\n",
      "3.49187\n",
      "8.23225\n",
      "4.51404\n",
      "4.44308\n",
      "5.2911\n",
      "2.76804\n",
      "2.63468\n",
      "4.08239\n",
      "2.83263\n",
      "3.34318\n",
      "2.04563\n",
      "3.61331\n",
      "5.43886\n",
      "4.69728\n",
      "2.59246\n",
      "5.19054\n",
      "4.78275\n",
      "2.84947\n",
      "4.30109\n",
      "2.16363\n",
      "1.9237\n",
      "2.80218\n",
      "3.91406\n",
      "4.21771\n",
      "6.73355\n",
      "4.38135\n",
      "1.87734\n",
      "2.76753\n",
      "2.93833\n",
      "8.55829\n",
      "3.43664\n",
      "5.31786\n",
      "8.75077\n",
      "1.97783\n",
      "1.69371\n",
      "2.62367\n",
      "5.00541\n",
      "5.94822\n",
      "3.56092\n",
      "5.88687\n",
      "1.84796\n",
      "3.16752\n",
      "2.71057\n",
      "4.9391\n",
      "3.27184\n",
      "3.46018\n",
      "7.49798\n",
      "2.98587\n",
      "5.80054\n",
      "3.41928\n",
      "3.28428\n",
      "1.12782\n",
      "5.29681\n",
      "4.59511\n",
      "2.12767\n",
      "3.14023\n",
      "3.92576\n",
      "4.06274\n",
      "3.67515\n",
      "2.21684\n",
      "5.15105\n",
      "5.04393\n",
      "1.52701\n",
      "2.35426\n",
      "2.93632\n",
      "1.28559\n",
      "2.68016\n",
      "4.37315\n",
      "2.50138\n",
      "4.00077\n",
      "4.43041\n",
      "3.08529\n",
      "5.45753\n",
      "2.4061\n",
      "2.7809\n",
      "2.22953\n",
      "1.80889\n",
      "1.89226\n",
      "3.48339\n",
      "5.23998\n",
      "5.09363\n",
      "6.16268\n",
      "8.20242\n",
      "4.0555\n",
      "5.65861\n",
      "3.09992\n",
      "3.20146\n",
      "1.60062\n",
      "11.2501\n",
      "11.618\n",
      "1.96035\n",
      "2.34616\n",
      "2.10355\n",
      "2.1805\n",
      "10.3\n",
      "2.63159\n",
      "5.36916\n",
      "1.34386\n",
      "4.31151\n",
      "3.87058\n",
      "2.42263\n",
      "2.20209\n",
      "2.30256\n",
      "2.34204\n",
      "1.61904\n",
      "2.12761\n",
      "4.77771\n",
      "1.94473\n",
      "2.95749\n",
      "1.51089\n",
      "2.77957\n",
      "2.06022\n",
      "2.95586\n",
      "1.99763\n",
      "5.06963\n",
      "4.63865\n",
      "1.52444\n",
      "6.48693\n",
      "1.47306\n",
      "2.19969\n",
      "2.80525\n",
      "5.30701\n",
      "7.96488\n",
      "1.01552\n",
      "2.68442\n",
      "1.98742\n",
      "3.43317\n",
      "2.56098\n",
      "2.56435\n",
      "1.36611\n",
      "0.973307\n",
      "2.32383\n",
      "3.19717\n",
      "2.37006\n",
      "3.61284\n",
      "2.50993\n",
      "3.39413\n",
      "1.45407\n",
      "3.97345\n",
      "4.24829\n",
      "7.02994\n",
      "5.09244\n",
      "1.82716\n",
      "3.77975\n",
      "1.26568\n",
      "4.59446\n",
      "2.22299\n",
      "2.35065\n",
      "5.17622\n",
      "1.59397\n",
      "4.64375\n",
      "8.40337\n",
      "1.88151\n",
      "2.62052\n",
      "1.93189\n",
      "2.19876\n",
      "1.78724\n",
      "1.80215\n",
      "2.00756\n",
      "2.29049\n",
      "1.34297\n",
      "1.24129\n",
      "1.74072\n",
      "3.89312\n",
      "2.77898\n",
      "2.45761\n",
      "1.94747\n",
      "17.4154\n",
      "4.33895\n",
      "2.57154\n",
      "1.915\n",
      "3.4065\n",
      "2.65552\n",
      "5.95245\n",
      "4.93399\n",
      "4.46646\n",
      "4.22982\n",
      "2.90772\n",
      "2.12927\n",
      "2.52946\n",
      "2.15845\n",
      "4.02629\n",
      "2.48734\n",
      "4.0594\n",
      "7.588\n",
      "3.75993\n",
      "3.28745\n",
      "3.11976\n",
      "5.69589\n",
      "2.92271\n",
      "4.52863\n",
      "1.82215\n",
      "1.76245\n",
      "2.42668\n",
      "47.9132\n",
      "26.2347\n",
      "8.48404\n",
      "13.2306\n",
      "3.09511\n",
      "1.9977\n",
      "1.98548\n",
      "1.65074\n",
      "3.06413\n",
      "3.15135\n",
      "4.2047\n",
      "2.63506\n",
      "2.47069\n",
      "2.26389\n",
      "2.00386\n",
      "2.28566\n",
      "3.29271\n",
      "2.83729\n",
      "2.03454\n",
      "3.44496\n",
      "5.45628\n",
      "1.49805\n",
      "2.18559\n",
      "2.17563\n",
      "6.43872\n",
      "3.31004\n",
      "2.33076\n",
      "1.90677\n",
      "3.24265\n",
      "3.80556\n",
      "2.57089\n",
      "0.913952\n",
      "1.85317\n",
      "2.57166\n",
      "3.80214\n",
      "1.83066\n",
      "1.45966\n",
      "1.75344\n",
      "23.484\n",
      "12.7425\n",
      "4.00259\n",
      "2.50761\n",
      "2.09458\n",
      "3.5221\n",
      "2.31898\n",
      "3.81712\n",
      "1.66834\n",
      "2.19031\n",
      "2.65498\n",
      "2.69316\n",
      "1.91589\n",
      "1.50557\n",
      "2.2808\n",
      "2.31206\n",
      "1.87109\n",
      "3.87807\n",
      "3.59297\n",
      "2.5696\n",
      "6.33618\n",
      "2.10892\n",
      "8.30399\n",
      "5.21615\n",
      "2.38165\n",
      "3.4379\n",
      "7.01685\n",
      "4.25291\n",
      "1.81087\n",
      "4.23377\n",
      "3.69982\n",
      "2.75023\n",
      "1.60779\n",
      "1.65987\n",
      "3.8668\n",
      "4.02857\n",
      "4.28463\n",
      "2.03193\n",
      "8.30136\n",
      "8.51392\n",
      "2.00976\n",
      "2.22348\n",
      "1.81265\n",
      "1.3603\n",
      "1.4604\n",
      "2.38033\n",
      "1.40857\n",
      "1.69552\n",
      "3.25784\n",
      "2.66702\n",
      "2.16617\n",
      "2.16961\n",
      "3.86114\n",
      "1.9685\n",
      "2.84501\n",
      "2.08564\n",
      "1.56774\n",
      "5.96041\n",
      "3.0607\n",
      "2.33618\n",
      "1.39407\n",
      "2.03535\n",
      "2.60122\n",
      "1.69148\n",
      "2.37539\n",
      "1.44233\n",
      "57.4178\n",
      "43.9674\n",
      "4.22334\n",
      "2.68817\n",
      "4.69073\n",
      "2.65187\n",
      "2.28498\n",
      "2.1739\n",
      "1.84806\n",
      "2.78844\n",
      "2.61412\n",
      "2.17261\n",
      "3.79923\n",
      "1.58087\n",
      "2.9755\n",
      "2.41853\n",
      "3.10923\n",
      "2.78871\n",
      "2.90605\n",
      "3.46528\n",
      "1.68885\n",
      "3.31569\n",
      "2.07187\n",
      "1.7814\n",
      "1.78623\n",
      "2.25839\n",
      "2.94661\n",
      "4.44836\n",
      "3.26584\n",
      "1.96934\n",
      "4.71589\n",
      "4.44638\n",
      "3.41368\n",
      "1.57955\n",
      "1.49454\n",
      "1.48287\n",
      "2.40832\n",
      "4.88942\n",
      "2.13598\n",
      "1.7846\n",
      "1.80577\n",
      "2.55089\n",
      "1.56633\n",
      "2.08974\n",
      "2.18885\n",
      "2.40598\n",
      "2.29233\n",
      "1.50508\n",
      "2.12353\n",
      "2.3371\n",
      "4.63543\n",
      "2.49667\n",
      "2.0124\n",
      "1.2499\n",
      "1.62298\n",
      "2.04186\n",
      "2.66756\n",
      "1.73504\n",
      "2.1834\n",
      "1.70109\n",
      "2.45176\n",
      "1.8252\n",
      "1.14574\n",
      "1.76581\n",
      "1.66854\n",
      "1.70265\n",
      "1.80503\n",
      "2.69769\n",
      "1.82399\n",
      "1.38559\n",
      "2.75285\n",
      "1.17325\n",
      "1.8442\n",
      "1.02177\n",
      "2.87387\n",
      "2.72034\n",
      "2.14659\n",
      "1.99916\n",
      "4.40834\n",
      "3.54287\n",
      "2.11612\n",
      "1.56418\n",
      "1.22265\n",
      "1.29872\n",
      "1.90121\n",
      "1.79695\n",
      "2.33545\n",
      "1.37348\n",
      "1.8728\n",
      "1.85273\n",
      "2.93332\n",
      "3.46116\n",
      "2.80283\n",
      "3.06305\n",
      "1.87124\n",
      "2.11475\n",
      "2.27475\n",
      "2.56089\n",
      "1.28941\n",
      "1.89617\n",
      "6.29452\n",
      "4.29142\n",
      "2.29216\n",
      "1.93072\n",
      "1.56704\n",
      "2.47542\n",
      "2.6906\n",
      "3.21785\n",
      "1.79043\n",
      "2.62184\n",
      "3.16126\n",
      "2.14768\n",
      "2.22505\n",
      "1.76741\n",
      "1.73339\n",
      "1.7177\n",
      "1.41378\n",
      "1.59686\n",
      "3.66551\n",
      "1.87895\n",
      "1.45157\n",
      "1.10442\n",
      "17.8434\n",
      "8.13308\n",
      "2.93995\n",
      "1.68166\n",
      "2.06607\n",
      "2.22153\n",
      "2.12908\n",
      "1.4647\n",
      "3.01804\n",
      "1.58256\n",
      "3.26766\n",
      "2.21581\n",
      "2.31387\n",
      "1.20346\n",
      "1.29613\n",
      "1.67185\n",
      "3.55218\n",
      "1.76256\n",
      "1.83012\n",
      "2.41944\n",
      "1.96869\n",
      "2.3744\n",
      "1.41289\n",
      "1.61943\n",
      "1.20265\n",
      "2.22264\n",
      "1.76033\n",
      "1.57102\n",
      "1.37349\n",
      "2.15318\n",
      "1.88566\n",
      "3.54053\n",
      "1.88541\n",
      "1.79481\n",
      "1.81522\n",
      "3.69385\n",
      "1.6836\n",
      "2.53297\n",
      "1.93429\n",
      "2.70192\n",
      "1.96977\n",
      "1.40731\n",
      "2.66848\n",
      "1.75492\n",
      "1.86925\n",
      "1.46319\n",
      "1.46088\n",
      "2.09941\n",
      "1.86118\n",
      "1.50961\n",
      "2.74775\n",
      "1.67578\n",
      "1.403\n",
      "2.71103\n",
      "2.18712\n",
      "2.08247\n",
      "1.68033\n",
      "1.43065\n",
      "2.24581\n",
      "4.07187\n",
      "2.10711\n",
      "2.40018\n",
      "1.46184\n",
      "5.37555\n",
      "2.80753\n",
      "2.05269\n",
      "0.876317\n",
      "1.98617\n",
      "9.14935\n",
      "2.81894\n",
      "2.38095\n",
      "2.17916\n",
      "2.28598\n",
      "2.00862\n",
      "1.75481\n",
      "3.20614\n",
      "2.67612\n",
      "1.71607\n",
      "0.485693\n",
      "1.73743\n",
      "2.48258\n",
      "0.857127\n",
      "1.07759\n",
      "2.02641\n",
      "8.9114\n",
      "2.8817\n",
      "1.30664\n",
      "1.35726\n",
      "0.55249\n",
      "1.4293\n",
      "1.73987\n",
      "4.84697\n",
      "2.69013\n",
      "4.01957\n",
      "3.98749\n",
      "3.01525\n",
      "3.1605\n",
      "2.82969\n",
      "2.04969\n",
      "1.27126\n",
      "1.46294\n",
      "1.70579\n",
      "1.73635\n",
      "1.7961\n",
      "1.30454\n",
      "1.68948\n",
      "3.48527\n",
      "2.44116\n",
      "1.55227\n",
      "1.25597\n",
      "2.27873\n",
      "1.65514\n",
      "1.3041\n",
      "3.54707\n",
      "12.3424\n",
      "3.2651\n",
      "1.23111\n",
      "1.20483\n",
      "8.39464\n",
      "3.33\n",
      "1.41875\n",
      "1.08908\n",
      "1.79361\n",
      "2.16943\n",
      "1.64783\n",
      "2.47429\n",
      "1.49273\n",
      "8.26869\n",
      "3.12681\n",
      "6.66397\n",
      "3.1847\n",
      "4.24839\n",
      "3.51393\n",
      "2.25697\n",
      "1.05835\n",
      "1.71063\n",
      "0.93854\n",
      "1.18372\n",
      "2.12852\n",
      "1.86123\n",
      "1.92912\n",
      "4.53307\n",
      "4.57028\n",
      "1.68922\n",
      "1.53012\n",
      "0.947261\n",
      "3.08158\n",
      "1.30534\n",
      "1.93549\n",
      "1.0035\n",
      "1.47061\n",
      "2.13709\n",
      "2.70249\n",
      "2.793\n",
      "1.41899\n",
      "1.06076\n",
      "2.09846\n",
      "1.52302\n",
      "1.21695\n",
      "1.46155\n",
      "1.11758\n",
      "1.49758\n",
      "0.789267\n",
      "2.14435\n",
      "1.65736\n",
      "0.723641\n",
      "1.51391\n",
      "0.986674\n",
      "0.698202\n",
      "1.56538\n",
      "1.07626\n",
      "2.90484\n",
      "2.00207\n",
      "1.15244\n",
      "1.76771\n",
      "1.96424\n",
      "1.97862\n",
      "2.34223\n",
      "2.0654\n",
      "2.53138\n",
      "2.53872\n",
      "1.94241\n",
      "1.63359\n",
      "2.14402\n",
      "2.01461\n",
      "2.79253\n",
      "3.06753\n",
      "1.36614\n",
      "10.6612\n",
      "1.99237\n",
      "5.03187\n",
      "5.51533\n",
      "1.89264\n",
      "2.74785\n",
      "1.10414\n",
      "1.85021\n",
      "1.81488\n",
      "1.3683\n",
      "0.882955\n",
      "1.74168\n",
      "1.3683\n",
      "1.56189\n",
      "1.43948\n",
      "1.28422\n",
      "1.82885\n",
      "0.816526\n",
      "1.52914\n",
      "1.34508\n",
      "2.30597\n",
      "1.36542\n",
      "1.7003\n",
      "1.03399\n",
      "0.986236\n",
      "1.43705\n",
      "1.4846\n",
      "2.35023\n",
      "2.13426\n",
      "1.38882\n",
      "3.75975\n",
      "2.53365\n",
      "1.90465\n",
      "1.56796\n",
      "3.03882\n",
      "1.12504\n",
      "1.42329\n",
      "1.05844\n",
      "0.839608\n",
      "1.29806\n",
      "1.97518\n",
      "2.45491\n",
      "1.36761\n",
      "3.6721\n",
      "0.805059\n",
      "0.814272\n",
      "2.74888\n",
      "2.19889\n",
      "1.25788\n",
      "1.77794\n",
      "1.82341\n",
      "3.18284\n",
      "1.6399\n",
      "1.49766\n",
      "1.2048\n",
      "1.51824\n",
      "1.7089\n",
      "2.01773\n",
      "0.82637\n",
      "1.62767\n",
      "6.53845\n",
      "2.75244\n",
      "2.70646\n",
      "2.15274\n",
      "1.4104\n",
      "1.94602\n",
      "0.915801\n",
      "1.49844\n",
      "1.33599\n",
      "1.30954\n",
      "0.932067\n",
      "1.23079\n",
      "1.28013\n",
      "0.591967\n",
      "1.27039\n",
      "3.82177\n",
      "3.3043\n",
      "1.95711\n",
      "0.975405\n",
      "0.922334\n",
      "1.24828\n",
      "1.52177\n",
      "36.9744\n",
      "34.8608\n",
      "2.34131\n",
      "2.07804\n",
      "2.38272\n",
      "1.71708\n",
      "0.922522\n",
      "1.79813\n",
      "12.2301\n",
      "5.49904\n",
      "2.5474\n",
      "2.45255\n",
      "3.80377\n",
      "2.87238\n",
      "4.41292\n",
      "2.21171\n",
      "1.70416\n",
      "0.602948\n",
      "0.922346\n",
      "1.01555\n",
      "2.82468\n",
      "0.899769\n",
      "1.73844\n",
      "3.57885\n",
      "1.26253\n",
      "0.679389\n",
      "1.74543\n",
      "1.79523\n",
      "1.92714\n",
      "0.864696\n",
      "1.92948\n",
      "1.67683\n",
      "2.19936\n",
      "1.15464\n",
      "1.18002\n",
      "1.89375\n",
      "1.33579\n",
      "1.88092\n",
      "2.46747\n",
      "1.44055\n",
      "1.6218\n",
      "2.11727\n",
      "1.42975\n",
      "1.8654\n",
      "1.6794\n",
      "1.55592\n",
      "1.25213\n",
      "0.805263\n",
      "1.11743\n",
      "4.44412\n",
      "2.59178\n",
      "1.07492\n",
      "1.34246\n",
      "1.31505\n",
      "3.20017\n",
      "3.13245\n",
      "2.23339\n",
      "1.35079\n",
      "0.830114\n",
      "1.29595\n",
      "1.36305\n",
      "0.60081\n",
      "1.06338\n",
      "5.44521\n",
      "3.60553\n",
      "2.04499\n",
      "2.80365\n",
      "1.19639\n",
      "3.07869\n",
      "1.54857\n",
      "1.95993\n",
      "1.78532\n",
      "1.17351\n",
      "1.72477\n",
      "1.15416\n",
      "1.06905\n",
      "0.986597\n",
      "2.26\n",
      "1.61774\n",
      "1.2054\n",
      "1.94118\n",
      "1.20503\n",
      "2.05152\n",
      "2.724\n",
      "1.80343\n",
      "0.810496\n",
      "3.46165\n",
      "1.77091\n",
      "1.28851\n",
      "1.91668\n",
      "1.52967\n",
      "2.71354\n",
      "0.690975\n",
      "1.71244\n",
      "1.15164\n",
      "3.10022\n",
      "2.50864\n",
      "1.66789\n",
      "1.02875\n",
      "3.50856\n",
      "1.16943\n",
      "1.22204\n",
      "1.48135\n",
      "1.37201\n",
      "1.23868\n",
      "1.04463\n",
      "0.884095\n",
      "1.1364\n",
      "1.58997\n",
      "2.95765\n",
      "10.3719\n",
      "4.99304\n",
      "1.20491\n",
      "2.04739\n",
      "1.96664\n",
      "1.83238\n",
      "2.20999\n",
      "1.48342\n",
      "1.40118\n",
      "1.26903\n",
      "1.5881\n",
      "1.37196\n",
      "2.17141\n",
      "1.77829\n",
      "1.50371\n",
      "1.51478\n",
      "1.56585\n",
      "2.2292\n",
      "1.81316\n",
      "1.84492\n",
      "2.48631\n",
      "3.2506\n",
      "2.07955\n",
      "2.95861\n",
      "2.60068\n",
      "1.12765\n",
      "1.71997\n",
      "4.52502\n",
      "1.50414\n",
      "1.02335\n",
      "1.65096\n",
      "1.08936\n",
      "12.1461\n",
      "3.64698\n",
      "2.22929\n",
      "1.83456\n",
      "1.79075\n",
      "1.86907\n",
      "1.33832\n",
      "1.45026\n",
      "3.09202\n",
      "1.99112\n",
      "1.93473\n",
      "1.01844\n",
      "17.4133\n",
      "24.0983\n",
      "5.1921\n",
      "2.61242\n",
      "2.9825\n",
      "3.12051\n",
      "2.08902\n",
      "1.51102\n",
      "2.33975\n",
      "2.07782\n",
      "3.57437\n",
      "2.17533\n",
      "2.0401\n",
      "0.7949\n",
      "0.789928\n",
      "1.67314\n",
      "1.31655\n",
      "1.08403\n",
      "4.67374\n",
      "2.77739\n",
      "1.59606\n",
      "1.31358\n",
      "2.46894\n",
      "3.56962\n",
      "2.99475\n",
      "1.24772\n",
      "1.81358\n",
      "1.71911\n",
      "1.93407\n",
      "0.983162\n",
      "1.46318\n",
      "1.70772\n",
      "1.56275\n",
      "2.1041\n",
      "1.70244\n",
      "2.56514\n",
      "0.79954\n",
      "1.71541\n",
      "1.20273\n",
      "1.67831\n",
      "1.46564\n",
      "1.25584\n",
      "1.11368\n",
      "1.56032\n",
      "1.43186\n",
      "2.15435\n",
      "3.21667\n",
      "2.09132\n",
      "1.84904\n",
      "2.25755\n",
      "1.62366\n",
      "0.996728\n",
      "1.10954\n",
      "1.08424\n",
      "1.55732\n",
      "2.19138\n",
      "1.92254\n",
      "1.22832\n",
      "1.74263\n",
      "1.34065\n",
      "1.92193\n",
      "0.967241\n",
      "2.27126\n",
      "1.15166\n",
      "1.28925\n",
      "2.78981\n",
      "1.36689\n",
      "1.53463\n",
      "1.37207\n",
      "1.17678\n",
      "1.30597\n",
      "2.67473\n",
      "2.72046\n",
      "0.874636\n",
      "1.26617\n",
      "0.855285\n",
      "0.923902\n",
      "1.8055\n",
      "1.61475\n",
      "1.30429\n",
      "0.548152\n",
      "1.45662\n",
      "1.26036\n",
      "1.36522\n",
      "2.60852\n",
      "3.90355\n",
      "1.90512\n",
      "0.548194\n",
      "1.2965\n",
      "1.32294\n",
      "1.43878\n",
      "0.985802\n",
      "0.718608\n",
      "1.77838\n",
      "1.09684\n",
      "1.20952\n",
      "0.686949\n",
      "0.791068\n",
      "0.982508\n",
      "1.38657\n",
      "1.77889\n",
      "1.34302\n",
      "1.10454\n",
      "0.827691\n",
      "0.937698\n",
      "0.861969\n",
      "1.72649\n",
      "0.973126\n",
      "1.68778\n",
      "1.20935\n",
      "1.31577\n",
      "1.11713\n",
      "1.37799\n",
      "0.989264\n",
      "0.904027\n",
      "1.96978\n",
      "0.954398\n",
      "1.42211\n",
      "1.62887\n",
      "1.77276\n",
      "4.46142\n",
      "2.64792\n",
      "3.21023\n",
      "1.24652\n",
      "0.812384\n",
      "1.72657\n",
      "0.887293\n",
      "0.873055\n",
      "2.41447\n",
      "11.1052\n",
      "2.43628\n",
      "1.36491\n",
      "1.86639\n",
      "1.52919\n",
      "0.754386\n",
      "0.629566\n",
      "1.03717\n",
      "1.40331\n",
      "1.196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06693\n",
      "1.14719\n",
      "1.03603\n",
      "0.687029\n",
      "0.921749\n",
      "0.609674\n",
      "0.847948\n",
      "1.43307\n",
      "1.4291\n",
      "1.59431\n",
      "2.44916\n",
      "1.57449\n",
      "0.838721\n",
      "1.04888\n",
      "1.52745\n",
      "0.800292\n",
      "1.52943\n",
      "0.839737\n",
      "0.481515\n",
      "0.897455\n",
      "0.641201\n",
      "1.10792\n",
      "2.74674\n",
      "1.82911\n",
      "1.10807\n",
      "1.44931\n",
      "1.10517\n",
      "0.798728\n",
      "1.47039\n",
      "1.12355\n",
      "1.09549\n",
      "0.751925\n",
      "1.57039\n",
      "2.02019\n",
      "1.17936\n",
      "0.936556\n",
      "2.20485\n",
      "1.47591\n",
      "1.58788\n",
      "1.06017\n",
      "0.895881\n",
      "3.60673\n",
      "3.38689\n",
      "1.97515\n",
      "2.65037\n",
      "1.12146\n",
      "0.959207\n",
      "0.818635\n",
      "1.02037\n",
      "0.73511\n",
      "1.15689\n",
      "0.584635\n",
      "1.72986\n",
      "2.45855\n",
      "2.31206\n",
      "1.61794\n",
      "0.990872\n",
      "1.00429\n",
      "0.953915\n",
      "0.569837\n",
      "3.50507\n",
      "2.14893\n",
      "2.84812\n",
      "3.05546\n",
      "1.07727\n",
      "1.13074\n",
      "0.771375\n",
      "1.14577\n",
      "1.38144\n",
      "1.05608\n",
      "1.71773\n",
      "1.89971\n",
      "0.995343\n",
      "4.3187\n",
      "2.44633\n",
      "2.55411\n",
      "3.86259\n",
      "3.06716\n",
      "2.64757\n",
      "2.4735\n",
      "1.18898\n",
      "1.47174\n",
      "2.28829\n",
      "0.390016\n",
      "0.777939\n",
      "1.92818\n",
      "1.39475\n",
      "0.943905\n",
      "0.727526\n",
      "1.55795\n",
      "1.18542\n",
      "2.29942\n",
      "1.06648\n",
      "1.08864\n",
      "2.16751\n",
      "1.07514\n",
      "1.56956\n",
      "1.53882\n",
      "0.546004\n",
      "0.775001\n",
      "1.31311\n",
      "1.69643\n",
      "0.975579\n",
      "1.08031\n",
      "1.14075\n",
      "1.77385\n",
      "1.73806\n",
      "4.49818\n",
      "2.68152\n",
      "2.28221\n",
      "1.72098\n",
      "2.44851\n",
      "2.26869\n",
      "3.00265\n",
      "3.03237\n",
      "3.72266\n",
      "2.09277\n",
      "1.79405\n",
      "0.961559\n",
      "0.869454\n",
      "1.53498\n",
      "0.96393\n",
      "1.21889\n",
      "3.43128\n",
      "2.57381\n",
      "1.29826\n",
      "2.24534\n",
      "1.10036\n",
      "0.697981\n",
      "1.20885\n",
      "0.831281\n",
      "1.61585\n",
      "1.39383\n",
      "1.05219\n",
      "1.1499\n",
      "0.767309\n",
      "0.694871\n",
      "0.857592\n",
      "1.03766\n",
      "0.652849\n",
      "0.839984\n",
      "1.64642\n",
      "1.38764\n",
      "0.877636\n",
      "1.12936\n",
      "2.61448\n",
      "1.67135\n",
      "0.444122\n",
      "0.884926\n",
      "1.45572\n",
      "0.990383\n",
      "1.00467\n",
      "1.35129\n",
      "1.49058\n",
      "1.52585\n",
      "0.821807\n",
      "3.536\n",
      "2.94292\n",
      "2.21024\n",
      "1.65622\n",
      "0.97162\n",
      "1.04173\n",
      "0.891631\n",
      "1.41856\n",
      "1.15216\n",
      "1.3542\n",
      "1.09471\n",
      "1.05717\n",
      "0.597191\n",
      "1.01666\n",
      "1.00293\n",
      "1.07875\n",
      "1.03904\n",
      "0.727859\n",
      "1.94899\n",
      "1.07286\n",
      "1.10669\n",
      "1.52446\n",
      "1.31983\n",
      "1.1648\n",
      "1.87474\n",
      "2.34517\n",
      "1.44628\n",
      "2.52624\n",
      "2.5775\n",
      "19.4489\n",
      "36.8064\n",
      "5.7896\n",
      "2.86777\n",
      "2.33037\n",
      "3.58276\n",
      "1.07146\n",
      "1.94251\n",
      "2.13711\n",
      "2.65721\n",
      "1.37152\n",
      "1.83924\n",
      "1.96843\n",
      "3.10285\n",
      "2.26412\n",
      "1.19061\n",
      "1.12371\n",
      "1.12986\n",
      "1.55631\n",
      "1.86069\n",
      "2.11762\n",
      "2.04929\n",
      "2.7889\n",
      "2.83987\n",
      "1.31958\n",
      "1.32818\n",
      "0.769795\n",
      "1.1938\n",
      "0.619825\n",
      "1.09631\n",
      "1.54798\n",
      "2.10064\n",
      "0.950137\n",
      "1.23329\n",
      "1.05189\n",
      "1.46893\n",
      "1.24126\n",
      "0.986134\n",
      "1.33067\n",
      "1.62098\n",
      "0.854973\n",
      "1.06237\n",
      "2.34071\n",
      "1.46278\n",
      "1.85579\n",
      "1.64028\n",
      "1.31056\n",
      "0.678957\n",
      "0.872224\n",
      "0.968131\n",
      "0.647682\n",
      "1.16467\n",
      "2.93651\n",
      "2.98431\n",
      "13.0232\n",
      "3.20737\n",
      "1.96791\n",
      "1.2469\n",
      "1.74807\n",
      "1.28991\n",
      "2.03659\n",
      "2.39852\n",
      "0.664464\n",
      "0.61256\n",
      "0.710918\n",
      "4.67457\n",
      "1.90499\n",
      "0.787086\n",
      "1.05216\n",
      "1.31274\n",
      "1.81591\n",
      "2.00904\n",
      "1.48041\n",
      "0.585214\n",
      "1.5752\n",
      "1.14963\n",
      "0.556064\n",
      "1.58028\n",
      "1.32378\n",
      "0.60898\n",
      "1.01716\n",
      "0.956298\n",
      "1.19651\n",
      "1.98109\n",
      "1.53726\n",
      "0.928162\n",
      "2.05467\n",
      "1.18723\n",
      "0.983488\n",
      "0.794849\n",
      "1.03712\n",
      "1.02151\n",
      "0.619684\n",
      "1.79993\n",
      "1.23836\n",
      "2.58889\n",
      "3.01216\n",
      "1.65662\n",
      "1.1328\n",
      "1.43138\n",
      "1.11365\n",
      "1.22327\n",
      "1.89668\n",
      "3.01663\n",
      "1.92945\n",
      "3.00813\n",
      "3.00337\n",
      "3.20769\n",
      "1.00619\n",
      "1.0228\n",
      "1.10339\n",
      "0.980546\n",
      "0.663142\n",
      "0.860888\n",
      "0.795852\n",
      "2.71566\n",
      "1.79604\n",
      "1.79099\n",
      "1.22142\n",
      "1.17052\n",
      "5.7407\n",
      "4.40906\n",
      "3.10077\n",
      "2.06702\n",
      "1.37552\n",
      "1.53183\n",
      "1.25844\n",
      "0.716867\n",
      "2.62261\n",
      "1.55059\n",
      "1.96395\n",
      "2.05887\n",
      "1.97769\n",
      "0.634287\n",
      "1.30438\n",
      "0.743294\n",
      "1.24141\n",
      "0.926225\n",
      "1.05612\n",
      "0.987791\n",
      "9.67373\n",
      "3.27465\n",
      "1.31396\n",
      "1.27677\n",
      "0.481972\n",
      "2.64951\n",
      "0.693578\n",
      "1.1081\n",
      "0.983751\n",
      "2.26025\n",
      "1.6753\n",
      "0.639055\n",
      "0.99084\n",
      "1.24231\n",
      "3.75112\n",
      "3.0794\n",
      "2.84321\n",
      "4.58414\n",
      "2.22029\n",
      "0.991497\n",
      "0.966762\n",
      "0.776634\n",
      "1.14368\n",
      "0.673896\n",
      "1.69685\n",
      "1.16817\n",
      "1.52925\n",
      "0.702536\n",
      "1.8031\n",
      "1.17972\n",
      "0.957842\n",
      "0.893436\n",
      "0.940773\n",
      "1.037\n",
      "1.41802\n",
      "0.755625\n",
      "0.755271\n",
      "1.5249\n",
      "1.82638\n",
      "1.72881\n",
      "0.983143\n",
      "1.1651\n",
      "0.756742\n",
      "1.5536\n",
      "1.71231\n",
      "3.75272\n",
      "2.16256\n",
      "0.508602\n",
      "0.573124\n",
      "1.31672\n",
      "0.51556\n",
      "0.884518\n",
      "0.932993\n",
      "0.722663\n",
      "0.89088\n",
      "1.09201\n",
      "0.723659\n",
      "0.6087\n",
      "1.22422\n",
      "3.22379\n",
      "0.634718\n",
      "0.9583\n",
      "0.571777\n",
      "0.759513\n",
      "2.731\n",
      "1.95049\n",
      "0.632108\n",
      "0.482742\n",
      "0.647245\n",
      "0.82225\n",
      "1.43111\n",
      "1.49625\n",
      "0.658787\n",
      "0.508804\n",
      "0.676567\n",
      "1.03502\n",
      "0.8673\n",
      "2.3015\n",
      "0.995286\n",
      "0.734999\n",
      "0.519161\n",
      "0.691606\n",
      "1.37581\n",
      "3.83166\n",
      "1.56568\n",
      "5.3011\n",
      "2.52572\n",
      "1.55788\n",
      "1.52477\n",
      "2.35349\n",
      "1.40125\n",
      "2.22547\n",
      "1.35828\n",
      "1.26569\n",
      "1.65032\n",
      "1.69451\n",
      "3.3068\n",
      "1.12431\n",
      "1.88532\n",
      "1.40746\n",
      "1.87114\n",
      "1.43287\n",
      "1.45158\n",
      "2.32836\n",
      "0.720105\n",
      "1.28422\n",
      "0.806647\n",
      "0.460074\n",
      "1.59263\n",
      "1.29045\n",
      "1.03294\n",
      "1.17563\n",
      "1.8996\n",
      "2.15462\n",
      "0.758624\n",
      "0.466486\n",
      "0.735318\n",
      "1.27167\n",
      "1.30053\n",
      "2.43071\n",
      "2.09863\n",
      "1.05168\n",
      "1.17065\n",
      "0.775801\n",
      "1.0215\n",
      "0.787577\n",
      "2.36965\n",
      "1.53551\n",
      "2.39195\n",
      "2.65917\n",
      "2.84762\n",
      "2.57073\n",
      "1.02638\n",
      "1.10298\n",
      "0.634377\n",
      "0.947026\n",
      "6.16782\n",
      "2.46636\n",
      "1.0649\n",
      "0.916368\n",
      "1.0325\n",
      "0.69504\n",
      "0.615394\n",
      "1.05449\n",
      "0.864891\n",
      "0.594286\n",
      "1.03808\n",
      "1.15291\n",
      "0.805191\n",
      "1.23362\n",
      "2.52226\n",
      "78.1695\n",
      "160.28\n",
      "34.8667\n",
      "17.1839\n",
      "11.1054\n",
      "8.73854\n",
      "4.19614\n",
      "3.39354\n",
      "5.84985\n",
      "3.88801\n",
      "4.45732\n",
      "7.58915\n",
      "5.88785\n",
      "4.63543\n",
      "7.01003\n",
      "4.68807\n",
      "4.1638\n",
      "2.97159\n",
      "2.71479\n",
      "4.20754\n",
      "3.79227\n",
      "2.43683\n",
      "5.09173\n",
      "3.19962\n",
      "2.82192\n",
      "2.59117\n",
      "3.09926\n",
      "4.65067\n",
      "4.07229\n",
      "3.41358\n",
      "2.51378\n",
      "4.17785\n",
      "4.39386\n",
      "5.98444\n",
      "3.37904\n",
      "10.7362\n",
      "4.88287\n",
      "6.86165\n",
      "2.7009\n",
      "2.85772\n",
      "3.22169\n",
      "3.35735\n",
      "2.44807\n",
      "2.02332\n",
      "3.69986\n",
      "1.83554\n",
      "2.36653\n",
      "2.30675\n",
      "2.07849\n",
      "0.955193\n",
      "2.18581\n",
      "4.95151\n",
      "3.62983\n",
      "2.77114\n",
      "1.42662\n",
      "1.5195\n",
      "3.67223\n",
      "1.24132\n",
      "1.36235\n",
      "1.20997\n",
      "1.45035\n",
      "1.33376\n",
      "1.04547\n",
      "1.81802\n",
      "3.05231\n",
      "2.4564\n",
      "1.32777\n",
      "1.09771\n",
      "3.16134\n",
      "2.29174\n",
      "1.12379\n",
      "3.05487\n",
      "1.18642\n",
      "1.405\n",
      "1.78169\n",
      "2.39398\n",
      "1.46625\n",
      "1.02801\n",
      "1.8124\n",
      "1.27421\n",
      "1.2486\n",
      "0.851382\n",
      "0.969686\n",
      "1.41414\n",
      "2.98461\n",
      "1.22331\n",
      "1.30301\n",
      "1.77353\n",
      "1.76073\n",
      "3.35951\n",
      "2.69417\n",
      "1.4043\n",
      "1.23216\n",
      "1.2508\n",
      "1.93125\n",
      "1.36538\n",
      "2.49642\n",
      "1.56336\n",
      "1.74568\n",
      "0.647002\n",
      "0.830814\n",
      "0.913275\n",
      "1.16275\n",
      "0.887482\n",
      "1.50299\n",
      "1.07024\n",
      "1.51523\n",
      "1.11901\n",
      "1.34563\n",
      "0.769213\n",
      "1.23175\n",
      "2.0056\n",
      "1.07004\n",
      "1.83386\n",
      "2.69766\n",
      "1.73598\n",
      "1.66281\n",
      "1.5206\n",
      "1.29327\n",
      "3.07752\n",
      "1.47876\n",
      "1.73719\n",
      "1.29033\n",
      "1.73812\n",
      "1.19605\n",
      "1.2648\n",
      "1.13868\n",
      "0.905383\n",
      "2.78482\n",
      "2.70614\n",
      "0.926199\n",
      "0.805032\n",
      "1.4991\n",
      "1.65202\n",
      "2.30257\n",
      "4.80423\n",
      "2.56656\n",
      "1.08814\n",
      "1.40957\n",
      "1.35945\n",
      "20.3267\n",
      "16.7093\n",
      "2.89407\n",
      "1.71256\n",
      "1.56712\n",
      "1.77427\n",
      "1.0313\n",
      "2.33388\n",
      "1.87386\n",
      "2.78444\n",
      "1.82996\n",
      "0.952883\n",
      "1.01629\n",
      "2.15172\n",
      "0.668107\n",
      "1.80339\n",
      "2.49604\n",
      "1.93122\n",
      "1.60085\n",
      "1.05273\n",
      "1.39473\n",
      "2.19833\n",
      "1.49796\n",
      "2.11947\n",
      "3.20222\n",
      "1.85245\n",
      "1.38456\n",
      "0.786646\n",
      "0.926304\n",
      "1.99523\n",
      "1.47565\n",
      "0.852281\n",
      "2.11117\n",
      "1.27514\n",
      "1.10184\n",
      "1.23275\n",
      "1.82706\n",
      "1.55645\n",
      "1.05625\n",
      "0.941237\n",
      "0.998592\n",
      "0.971251\n",
      "1.54533\n",
      "1.09687\n",
      "0.886298\n",
      "1.06879\n",
      "1.13319\n",
      "1.24963\n",
      "2.47453\n",
      "3.02346\n",
      "5.35848\n",
      "7.43223\n",
      "1.67242\n",
      "1.86444\n",
      "0.917959\n",
      "2.08932\n",
      "1.68054\n",
      "2.20656\n",
      "0.716034\n",
      "1.24755\n",
      "1.26766\n",
      "2.15406\n",
      "1.2563\n",
      "1.06683\n",
      "1.82966\n",
      "1.47657\n",
      "1.83173\n",
      "1.00478\n",
      "0.545375\n",
      "1.11946\n",
      "1.12475\n",
      "0.836429\n",
      "0.668608\n",
      "1.49081\n",
      "2.07091\n",
      "2.17491\n",
      "0.951771\n",
      "0.730072\n",
      "3.18365\n",
      "1.72973\n",
      "1.71893\n",
      "1.37524\n",
      "1.21206\n",
      "1.16562\n",
      "1.00104\n",
      "0.987812\n",
      "1.11913\n",
      "4.66867\n",
      "1.80582\n",
      "3.47145\n",
      "1.97833\n",
      "1.63542\n",
      "1.47815\n",
      "0.893868\n",
      "0.809392\n",
      "2.30151\n",
      "1.23335\n",
      "0.87758\n",
      "1.18898\n",
      "1.53874\n",
      "2.16111\n",
      "1.31816\n",
      "0.606075\n",
      "1.69237\n",
      "0.963506\n",
      "3.51772\n",
      "2.07529\n",
      "0.978916\n",
      "0.980727\n",
      "0.75204\n",
      "0.911363\n",
      "6.27423\n",
      "1.5848\n",
      "1.56638\n",
      "6.09583\n",
      "16.2003\n",
      "1.21354\n",
      "2.89382\n",
      "0.922666\n",
      "1.13503\n",
      "1.41751\n",
      "3.26601\n",
      "1.67179\n",
      "1.07385\n",
      "0.896438\n",
      "1.23578\n",
      "1.26283\n",
      "1.54104\n",
      "1.91239\n",
      "2.79485\n",
      "1.11283\n",
      "0.796879\n",
      "0.90967\n",
      "1.18831\n",
      "1.42096\n",
      "1.03005\n",
      "1.56925\n",
      "1.09296\n",
      "1.65868\n",
      "0.891779\n",
      "1.27051\n",
      "1.12056\n",
      "0.780406\n",
      "1.56438\n",
      "1.705\n",
      "1.14924\n",
      "1.24346\n",
      "1.15577\n",
      "0.508862\n",
      "0.533865\n",
      "1.36022\n",
      "1.40332\n",
      "2.04354\n",
      "2.67835\n",
      "2.99398\n",
      "1.70031\n",
      "0.840817\n",
      "1.28021\n",
      "2.33566\n",
      "1.09885\n",
      "1.06285\n",
      "0.960609\n",
      "2.67024\n",
      "2.50236\n",
      "1.41591\n",
      "1.06609\n",
      "1.03011\n",
      "0.845648\n",
      "0.930424\n",
      "0.397112\n",
      "0.746474\n",
      "1.90449\n",
      "2.56734\n",
      "3.65983\n",
      "2.49711\n",
      "1.69029\n",
      "1.87755\n",
      "1.33908\n",
      "1.47104\n",
      "0.515994\n",
      "1.13545\n",
      "1.24651\n",
      "1.0197\n",
      "2.29325\n",
      "0.721552\n",
      "0.915428\n",
      "1.33378\n",
      "1.08081\n",
      "0.795988\n",
      "0.807527\n",
      "0.702823\n",
      "1.04798\n",
      "0.694667\n",
      "0.975902\n",
      "2.12635\n",
      "1.34661\n",
      "1.69517\n",
      "0.824652\n",
      "0.634052\n",
      "1.27307\n",
      "1.06346\n",
      "0.93303\n",
      "0.966708\n",
      "0.58438\n",
      "0.525872\n",
      "0.728565\n",
      "3.64721\n",
      "1.44396\n",
      "1.23898\n",
      "1.13197\n",
      "1.54255\n",
      "1.94483\n",
      "1.73672\n",
      "1.18336\n",
      "1.1891\n",
      "1.12787\n",
      "1.60222\n",
      "1.30825\n",
      "1.35603\n",
      "5.24493\n",
      "3.07517\n",
      "2.43936\n",
      "1.69662\n",
      "0.808627\n",
      "1.09745\n",
      "1.18119\n",
      "1.36708\n",
      "0.729246\n",
      "0.831147\n",
      "0.646453\n",
      "1.28345\n",
      "1.64984\n",
      "0.835601\n",
      "1.04886\n",
      "0.767685\n",
      "1.03659\n",
      "1.70447\n",
      "1.01043\n",
      "1.15232\n",
      "0.844544\n",
      "0.656785\n",
      "0.774448\n",
      "1.93192\n",
      "1.05841\n",
      "1.4575\n",
      "1.16022\n",
      "0.699023\n",
      "0.761343\n",
      "0.808761\n",
      "0.95173\n",
      "1.2697\n",
      "1.75792\n",
      "0.91817\n",
      "1.8759\n",
      "0.722491\n",
      "1.12204\n",
      "0.590001\n",
      "0.914552\n",
      "0.977654\n",
      "0.82518\n",
      "0.467111\n",
      "0.646527\n",
      "1.61387\n",
      "5.91982\n",
      "4.79234\n",
      "0.783365\n",
      "1.65571\n",
      "0.698176\n",
      "0.512105\n",
      "0.340172\n",
      "2.29573\n",
      "1.36032\n",
      "0.915402\n",
      "1.18189\n",
      "1.42522\n",
      "1.79589\n",
      "1.00723\n",
      "1.17101\n",
      "1.35424\n",
      "1.20543\n",
      "1.00548\n",
      "1.04732\n",
      "2.32996\n",
      "0.774352\n",
      "1.53863\n",
      "0.773703\n",
      "0.843942\n",
      "0.683203\n",
      "1.42212\n",
      "0.533187\n",
      "0.840807\n",
      "1.14412\n",
      "0.948008\n",
      "1.58332\n",
      "2.96412\n",
      "1.58889\n",
      "0.897572\n",
      "0.92221\n",
      "0.687288\n",
      "0.982714\n",
      "0.733366\n",
      "0.839258\n",
      "0.939948\n",
      "0.938321\n",
      "0.583694\n",
      "0.377649\n",
      "0.927696\n",
      "2.28102\n",
      "1.64311\n",
      "0.677389\n",
      "0.516417\n",
      "0.924855\n",
      "5.13635\n",
      "2.35132\n",
      "3.10603\n",
      "3.456\n",
      "5.32273\n",
      "1.80471\n",
      "1.20717\n",
      "0.53425\n",
      "0.881812\n",
      "1.28652\n",
      "4.22524\n",
      "3.10495\n",
      "2.37908\n",
      "1.45342\n",
      "0.830462\n",
      "0.911802\n",
      "1.34065\n",
      "1.12063\n",
      "0.940988\n",
      "2.43819\n",
      "1.09944\n",
      "1.03786\n",
      "0.861947\n",
      "0.615856\n",
      "1.2014\n",
      "1.30261\n",
      "1.34054\n",
      "1.61891\n",
      "1.30783\n",
      "1.62152\n",
      "1.46626\n",
      "0.966597\n",
      "0.686314\n",
      "2.89881\n",
      "3.92722\n",
      "2.58857\n",
      "1.85837\n",
      "0.972912\n",
      "1.71154\n",
      "2.75111\n",
      "1.45865\n",
      "0.84364\n",
      "0.924263\n",
      "0.763697\n",
      "0.634234\n",
      "0.639787\n",
      "0.688245\n",
      "1.94738\n",
      "0.765605\n",
      "1.10614\n",
      "4.88404\n",
      "3.41015\n",
      "1.47307\n",
      "1.15268\n",
      "1.11617\n",
      "0.683643\n",
      "1.70471\n",
      "0.926506\n",
      "0.994123\n",
      "0.865585\n",
      "1.11939\n",
      "0.945473\n",
      "1.12344\n",
      "0.63433\n",
      "0.556137\n",
      "1.02441\n",
      "1.26293\n",
      "1.17883\n",
      "2.74787\n",
      "1.67354\n",
      "5.74413\n",
      "2.95311\n",
      "0.863424\n",
      "0.868124\n",
      "1.34619\n",
      "0.367554\n",
      "1.48984\n",
      "0.911404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442168\n",
      "0.877776\n",
      "1.10997\n",
      "0.433477\n",
      "0.723186\n",
      "1.13485\n",
      "0.444617\n",
      "0.873883\n",
      "0.650526\n",
      "1.17096\n",
      "1.43781\n",
      "1.05257\n",
      "0.536882\n",
      "0.901498\n",
      "1.82514\n",
      "0.971154\n",
      "0.502215\n",
      "0.750841\n",
      "0.896204\n",
      "0.542809\n",
      "0.534429\n",
      "0.482253\n",
      "0.669551\n",
      "0.591252\n",
      "0.871272\n",
      "0.752137\n",
      "6.4934\n",
      "6.98534\n",
      "1.50465\n",
      "0.526227\n",
      "0.83502\n",
      "1.21888\n",
      "1.00125\n",
      "0.699738\n",
      "2.51193\n",
      "1.04415\n",
      "1.58574\n",
      "0.485408\n",
      "0.965401\n",
      "0.573007\n",
      "0.553746\n",
      "1.48121\n",
      "0.78287\n",
      "0.778131\n",
      "1.07209\n",
      "1.12707\n",
      "1.87655\n",
      "0.956387\n",
      "0.921711\n",
      "0.817011\n",
      "1.08138\n",
      "1.56256\n",
      "1.06734\n",
      "0.454266\n",
      "0.625077\n",
      "0.89914\n",
      "0.759957\n",
      "1.10454\n",
      "0.705865\n",
      "3.52388\n",
      "2.25258\n",
      "1.87811\n",
      "1.64848\n",
      "1.19415\n",
      "0.797132\n",
      "0.697112\n",
      "0.602055\n",
      "1.09659\n",
      "0.596801\n",
      "0.988886\n",
      "0.617697\n",
      "1.20938\n",
      "0.689025\n",
      "1.04222\n",
      "1.38866\n",
      "0.937241\n",
      "0.674501\n",
      "1.58074\n",
      "2.04932\n",
      "0.667473\n",
      "0.468475\n",
      "1.75967\n",
      "0.836133\n",
      "0.861644\n",
      "0.899246\n",
      "1.47484\n",
      "0.967229\n",
      "0.299432\n",
      "1.31768\n",
      "1.06314\n",
      "0.581636\n",
      "0.48213\n",
      "0.508953\n",
      "0.511669\n",
      "1.25681\n",
      "1.58648\n",
      "1.19386\n",
      "7.24687\n",
      "4.08929\n",
      "1.28598\n",
      "6.14528\n",
      "2.7463\n",
      "0.769496\n",
      "0.949194\n",
      "0.736223\n",
      "0.67226\n",
      "0.772018\n",
      "0.693033\n",
      "0.965949\n",
      "0.59774\n",
      "1.38914\n",
      "0.884726\n",
      "0.61626\n",
      "1.06918\n",
      "0.605663\n",
      "0.938139\n",
      "0.580249\n",
      "1.24075\n",
      "0.580949\n",
      "1.10925\n",
      "0.893557\n",
      "1.30576\n",
      "0.555932\n",
      "0.685795\n",
      "0.566616\n",
      "0.36157\n",
      "1.00334\n",
      "0.501652\n",
      "0.522369\n",
      "0.983532\n",
      "0.831236\n",
      "1.61899\n",
      "1.82339\n",
      "0.92554\n",
      "1.91696\n",
      "2.39428\n",
      "2.22804\n",
      "1.54399\n",
      "8.80703\n",
      "11.1182\n",
      "2.93848\n",
      "1.18777\n",
      "1.33511\n",
      "2.51828\n",
      "3.52392\n",
      "3.80948\n",
      "1.91868\n",
      "1.10175\n",
      "0.765816\n",
      "1.02684\n",
      "0.762729\n",
      "4.52959\n",
      "3.33648\n",
      "1.07779\n",
      "1.1145\n",
      "0.885006\n",
      "0.811417\n",
      "1.46543\n",
      "0.931551\n",
      "0.690711\n",
      "0.998258\n",
      "0.967055\n",
      "0.986323\n",
      "0.438272\n",
      "0.896569\n",
      "1.11929\n",
      "1.55497\n",
      "1.33741\n",
      "0.955679\n",
      "0.531578\n",
      "0.664183\n",
      "0.713173\n",
      "1.06109\n",
      "0.803257\n",
      "0.92092\n",
      "1.31369\n",
      "1.44267\n",
      "1.259\n",
      "0.758235\n",
      "1.31863\n",
      "0.629151\n",
      "1.41258\n",
      "1.53776\n",
      "9.15129\n",
      "14.2278\n",
      "6.26673\n",
      "3.66782\n",
      "0.621353\n",
      "1.39961\n",
      "1.44563\n",
      "1.82826\n",
      "1.07632\n",
      "1.16712\n",
      "1.5284\n",
      "0.830164\n",
      "1.30965\n",
      "1.38698\n",
      "0.658298\n",
      "1.29217\n",
      "1.10976\n",
      "1.52419\n",
      "1.55512\n",
      "1.72652\n",
      "1.3039\n",
      "2.10025\n",
      "1.21257\n",
      "0.666861\n",
      "0.449431\n",
      "0.978078\n",
      "0.968229\n",
      "1.09408\n",
      "0.48752\n",
      "0.972264\n",
      "1.54639\n",
      "0.732836\n",
      "0.752087\n",
      "1.13779\n",
      "0.813947\n",
      "1.67343\n",
      "1.01958\n",
      "0.6868\n",
      "0.502905\n",
      "1.19326\n",
      "0.817168\n",
      "0.681413\n",
      "1.36034\n",
      "12.0111\n",
      "1.65536\n",
      "0.851056\n",
      "0.555112\n",
      "1.15156\n",
      "1.16483\n",
      "0.845034\n",
      "2.76716\n",
      "2.40299\n",
      "1.41415\n",
      "0.974491\n",
      "0.983258\n",
      "0.706338\n",
      "0.620816\n",
      "1.11558\n",
      "0.640049\n",
      "0.934926\n",
      "1.04917\n",
      "1.33968\n",
      "1.06695\n",
      "1.0185\n",
      "0.777444\n",
      "1.04767\n",
      "0.84214\n",
      "0.615706\n",
      "0.646971\n",
      "0.922776\n",
      "0.568561\n",
      "0.314117\n",
      "0.995085\n",
      "1.17129\n",
      "1.16816\n",
      "2.05173\n",
      "1.98221\n",
      "1.16373\n",
      "1.34362\n",
      "0.736913\n",
      "0.683915\n",
      "0.989108\n",
      "1.38587\n",
      "0.972661\n",
      "0.63241\n",
      "1.07231\n",
      "1.76163\n",
      "1.2781\n",
      "0.652504\n",
      "1.15171\n",
      "0.90546\n",
      "0.722813\n",
      "0.727929\n",
      "0.837968\n",
      "0.833582\n",
      "0.380829\n",
      "1.39308\n",
      "0.748356\n",
      "0.712629\n",
      "1.91233\n",
      "0.997332\n",
      "1.01199\n",
      "0.863804\n",
      "0.719223\n",
      "9.60255\n",
      "2.12042\n",
      "0.989647\n",
      "0.845762\n",
      "1.8001\n",
      "1.63481\n",
      "0.584239\n",
      "0.609568\n",
      "0.678405\n",
      "0.908745\n",
      "0.743917\n",
      "0.338136\n",
      "1.19932\n",
      "0.946052\n",
      "1.08095\n",
      "0.855189\n",
      "1.76047\n",
      "1.75394\n",
      "0.689132\n",
      "0.87448\n",
      "0.847533\n",
      "0.777067\n",
      "1.08755\n",
      "0.431927\n",
      "0.602411\n",
      "0.873665\n",
      "1.44636\n",
      "1.19827\n",
      "1.01431\n",
      "1.09739\n",
      "0.997583\n",
      "0.434986\n",
      "0.480476\n",
      "0.75713\n",
      "0.820387\n",
      "0.666955\n",
      "0.533711\n",
      "1.35347\n",
      "0.796232\n",
      "0.85481\n",
      "0.607094\n",
      "0.401469\n",
      "1.0722\n",
      "0.651718\n",
      "0.992629\n",
      "1.02319\n",
      "1.159\n",
      "0.715985\n",
      "0.773838\n",
      "1.17733\n",
      "0.646207\n",
      "0.627816\n",
      "0.859798\n",
      "1.30852\n",
      "1.48945\n",
      "1.23786\n",
      "0.483406\n",
      "1.69096\n",
      "0.87874\n",
      "0.530814\n",
      "2.88834\n",
      "5.37633\n",
      "0.970339\n",
      "0.495972\n",
      "0.622008\n",
      "0.680956\n",
      "1.83825\n",
      "0.724732\n",
      "0.72906\n",
      "1.13137\n",
      "0.871711\n",
      "0.653082\n",
      "1.32741\n",
      "1.80865\n",
      "1.62272\n",
      "1.67207\n",
      "0.854663\n",
      "0.623057\n",
      "0.454732\n",
      "0.579917\n",
      "1.60813\n",
      "0.676624\n",
      "0.320683\n",
      "1.25768\n",
      "0.748839\n",
      "0.415764\n",
      "0.52655\n",
      "1.04814\n",
      "0.708468\n",
      "0.857396\n",
      "0.418765\n",
      "0.938476\n",
      "0.366485\n",
      "0.441521\n",
      "0.815178\n",
      "0.639263\n",
      "1.06549\n",
      "0.865743\n",
      "0.998187\n",
      "1.039\n",
      "1.22194\n",
      "0.823644\n",
      "0.665283\n",
      "1.46182\n",
      "0.786535\n",
      "1.00855\n",
      "0.82981\n",
      "0.641423\n",
      "1.68755\n",
      "2.14794\n",
      "0.940077\n",
      "1.12569\n",
      "1.03008\n",
      "0.768126\n",
      "0.762415\n",
      "2.33291\n",
      "1.27315\n",
      "0.885051\n",
      "0.481397\n",
      "0.558656\n",
      "1.07503\n",
      "1.35447\n",
      "2.30807\n",
      "1.34889\n",
      "1.81996\n",
      "1.0295\n",
      "0.775114\n",
      "0.930473\n",
      "0.743861\n",
      "0.883474\n",
      "0.813552\n",
      "0.627772\n",
      "0.698668\n",
      "2.18152\n",
      "1.21036\n",
      "1.13667\n",
      "0.544767\n",
      "52.4426\n",
      "83.1949\n",
      "27.0064\n",
      "25.1919\n",
      "3.67088\n",
      "1.74743\n",
      "0.819053\n",
      "0.563444\n",
      "1.21921\n",
      "1.08991\n",
      "3.55922\n",
      "1.74079\n",
      "0.938883\n",
      "2.12301\n",
      "1.06734\n",
      "1.59639\n",
      "0.782367\n",
      "0.779101\n",
      "1.29998\n",
      "2.06191\n",
      "8.03398\n",
      "7.02895\n",
      "2.2455\n",
      "1.53483\n",
      "1.64384\n",
      "1.23571\n",
      "1.97134\n",
      "1.01047\n",
      "1.41946\n",
      "2.04367\n",
      "0.823811\n",
      "0.811002\n",
      "0.763159\n",
      "0.849619\n",
      "0.62697\n",
      "0.811872\n",
      "1.20016\n",
      "0.722401\n",
      "1.15825\n",
      "1.06564\n",
      "0.767769\n",
      "0.881836\n",
      "2.0069\n",
      "0.66076\n",
      "0.878195\n",
      "1.08427\n",
      "0.577145\n",
      "1.131\n",
      "2.1747\n",
      "1.41004\n",
      "0.937546\n",
      "1.16168\n",
      "0.679569\n",
      "1.01636\n",
      "0.865619\n",
      "0.689424\n",
      "1.1621\n",
      "1.34438\n",
      "1.00805\n",
      "0.503647\n",
      "0.973374\n",
      "0.527282\n",
      "1.23999\n",
      "1.09986\n",
      "0.752353\n",
      "1.03593\n",
      "0.458908\n",
      "0.447027\n",
      "1.01887\n",
      "1.41405\n",
      "1.19039\n",
      "0.89408\n",
      "0.785664\n",
      "0.407119\n",
      "0.594628\n",
      "0.555811\n",
      "0.953769\n",
      "0.979828\n",
      "0.910008\n",
      "0.707257\n",
      "0.937621\n",
      "0.928394\n",
      "1.33792\n",
      "0.758405\n",
      "1.38346\n",
      "1.06833\n",
      "0.747269\n",
      "1.29225\n",
      "1.26992\n",
      "0.762251\n",
      "0.716007\n",
      "0.64286\n",
      "1.19295\n",
      "0.749715\n",
      "0.889833\n",
      "0.946878\n",
      "2.08143\n",
      "0.575576\n",
      "0.393095\n",
      "0.773468\n",
      "1.15043\n",
      "0.334306\n",
      "0.852553\n",
      "1.55492\n",
      "2.02646\n",
      "1.09491\n",
      "1.3546\n",
      "0.87584\n",
      "0.563127\n",
      "0.623099\n",
      "0.875992\n",
      "1.10374\n",
      "1.2702\n",
      "0.865151\n",
      "0.653736\n",
      "2.35033\n",
      "1.10374\n",
      "0.414287\n",
      "0.829318\n",
      "1.14108\n",
      "0.643983\n",
      "1.20065\n",
      "1.8106\n",
      "1.38572\n",
      "0.461981\n",
      "0.814188\n",
      "0.643097\n",
      "0.871683\n",
      "1.04406\n",
      "1.01999\n",
      "0.927363\n",
      "0.608542\n",
      "0.719939\n",
      "1.17009\n",
      "0.968241\n",
      "0.743927\n",
      "0.57669\n",
      "1.01276\n",
      "0.829527\n",
      "0.572856\n",
      "1.7515\n",
      "0.716191\n",
      "0.474975\n",
      "0.579535\n",
      "1.99699\n",
      "0.706889\n",
      "0.696616\n",
      "1.15167\n",
      "4.6818\n",
      "0.727609\n",
      "0.795911\n",
      "7.60714\n",
      "1.21907\n",
      "2.05093\n",
      "1.05041\n",
      "1.09784\n",
      "1.48322\n",
      "0.913036\n",
      "1.52263\n",
      "1.01847\n",
      "0.478134\n",
      "0.977085\n",
      "1.35957\n",
      "1.19635\n",
      "1.00864\n",
      "4.66346\n",
      "1.32713\n",
      "0.881136\n",
      "0.604163\n",
      "1.08415\n",
      "1.37025\n",
      "0.755926\n",
      "0.737809\n",
      "0.94835\n",
      "0.920263\n",
      "0.805382\n",
      "1.60764\n",
      "1.61972\n",
      "1.31431\n",
      "0.767188\n",
      "0.939743\n",
      "0.996645\n",
      "3.16914\n",
      "1.63366\n",
      "0.45744\n",
      "0.50544\n",
      "1.00215\n",
      "1.0077\n",
      "1.03016\n",
      "12.8338\n",
      "13.094\n",
      "7.21988\n",
      "3.95862\n",
      "1.69545\n",
      "0.974997\n",
      "0.597135\n",
      "0.812582\n",
      "0.817144\n",
      "0.761451\n",
      "0.774258\n",
      "0.880655\n",
      "0.940433\n",
      "0.386756\n",
      "0.492578\n",
      "0.798267\n",
      "1.075\n",
      "0.86561\n",
      "1.12714\n",
      "1.40248\n",
      "1.09816\n",
      "0.548942\n",
      "0.568577\n",
      "0.979048\n",
      "0.458264\n",
      "0.68236\n",
      "0.762186\n",
      "0.823413\n",
      "0.871791\n",
      "0.68691\n",
      "0.99312\n",
      "0.625861\n",
      "0.541735\n",
      "0.833268\n",
      "0.572572\n",
      "0.31135\n",
      "0.742586\n",
      "0.96384\n",
      "1.44256\n",
      "0.391006\n",
      "0.470171\n",
      "0.628944\n",
      "0.801679\n",
      "0.607023\n",
      "1.18794\n",
      "0.919388\n",
      "1.52447\n",
      "2.09502\n",
      "0.800502\n",
      "0.83237\n",
      "0.848947\n",
      "3.25863\n",
      "0.837864\n",
      "1.81539\n",
      "0.530308\n",
      "1.73903\n",
      "1.42553\n",
      "1.46752\n",
      "0.511729\n",
      "0.472117\n",
      "0.535906\n",
      "0.350221\n",
      "0.656318\n",
      "0.465969\n",
      "0.62815\n",
      "0.989581\n",
      "0.626214\n",
      "0.696945\n",
      "0.644863\n",
      "0.88159\n",
      "0.929809\n",
      "0.97794\n",
      "1.14064\n",
      "0.917506\n",
      "1.11562\n",
      "1.36595\n",
      "0.927082\n",
      "1.01765\n",
      "1.26923\n",
      "0.482557\n",
      "0.424313\n",
      "0.555464\n",
      "1.73451\n",
      "0.863434\n",
      "0.599645\n",
      "0.603586\n",
      "0.657688\n",
      "1.10842\n",
      "0.846807\n",
      "1.61098\n",
      "1.08776\n",
      "5.04016\n",
      "7.44035\n",
      "2.2164\n",
      "1.72106\n",
      "1.20166\n",
      "0.832827\n",
      "0.514994\n",
      "0.791196\n",
      "2.18286\n",
      "0.776618\n",
      "0.587913\n",
      "0.949969\n",
      "1.13651\n",
      "1.05149\n",
      "0.755636\n",
      "0.725368\n",
      "0.862852\n",
      "0.73523\n",
      "0.388431\n",
      "0.640868\n",
      "0.567106\n",
      "1.03091\n",
      "0.496324\n",
      "0.903428\n",
      "0.782861\n",
      "0.612114\n",
      "0.649405\n",
      "0.670145\n",
      "0.619159\n",
      "0.974722\n",
      "1.05953\n",
      "0.357318\n",
      "0.51881\n",
      "0.511406\n",
      "0.294507\n",
      "0.516693\n",
      "1.1318\n",
      "0.568013\n",
      "0.98756\n",
      "1.46016\n",
      "0.647109\n",
      "0.400704\n",
      "0.688937\n",
      "0.533183\n",
      "0.550242\n",
      "0.543933\n",
      "0.479178\n",
      "0.996719\n",
      "0.761992\n",
      "4.19956\n",
      "2.34903\n",
      "1.50544\n",
      "1.49792\n",
      "1.48955\n",
      "1.00071\n",
      "0.692684\n",
      "0.7823\n",
      "0.682038\n",
      "0.710086\n",
      "0.779117\n",
      "0.697754\n",
      "0.966076\n",
      "0.384666\n",
      "0.752543\n",
      "0.750471\n",
      "0.489811\n",
      "0.556197\n",
      "0.439539\n",
      "0.614545\n",
      "0.661934\n",
      "0.443536\n",
      "0.988693\n",
      "1.08852\n",
      "0.515692\n",
      "0.663289\n",
      "0.739907\n",
      "0.99968\n",
      "1.25555\n",
      "0.457812\n",
      "0.765245\n",
      "1.58541\n",
      "0.819797\n",
      "0.525799\n",
      "0.492529\n",
      "1.01079\n",
      "0.990595\n",
      "0.651368\n",
      "0.593041\n",
      "0.34943\n",
      "1.24761\n",
      "0.646317\n",
      "0.647054\n",
      "1.31335\n",
      "0.455779\n",
      "0.389827\n",
      "0.371507\n",
      "0.737719\n",
      "0.544817\n",
      "0.743938\n",
      "0.599199\n",
      "0.651085\n",
      "0.429294\n",
      "0.666288\n",
      "1.16898\n",
      "0.580302\n",
      "0.555023\n",
      "0.508555\n",
      "0.652504\n",
      "0.609284\n",
      "0.360462\n",
      "0.485625\n",
      "0.940279\n",
      "0.5763\n",
      "1.89359\n",
      "0.921265\n",
      "0.883061\n",
      "1.4643\n",
      "1.08308\n",
      "0.931776\n",
      "0.55449\n",
      "0.558963\n",
      "0.506431\n",
      "0.375025\n",
      "0.537579\n",
      "0.438771\n",
      "0.491315\n",
      "0.599377\n",
      "0.261553\n",
      "0.825983\n",
      "1.20551\n",
      "0.783798\n",
      "0.860798\n",
      "1.16164\n",
      "0.870777\n",
      "10.2771\n",
      "42.7704\n",
      "12.1324\n",
      "2.62156\n",
      "1.462\n",
      "2.94619\n",
      "3.94066\n",
      "2.84544\n",
      "2.451\n",
      "3.16017\n",
      "3.15362\n",
      "2.68533\n",
      "1.4087\n",
      "0.91078\n",
      "0.575508\n",
      "0.818517\n",
      "1.18848\n",
      "0.668813\n",
      "0.72274\n",
      "1.14901\n",
      "0.794752\n",
      "0.447994\n",
      "0.601074\n",
      "2.19109\n",
      "0.401072\n",
      "0.584026\n",
      "0.678796\n",
      "1.06688\n",
      "0.695279\n",
      "0.607945\n",
      "1.04178\n",
      "1.03007\n",
      "1.00117\n",
      "0.755379\n",
      "0.575402\n",
      "0.733768\n",
      "0.551669\n",
      "1.40661\n",
      "1.62877\n",
      "1.30356\n",
      "1.59321\n",
      "0.614878\n",
      "0.665569\n",
      "0.486819\n",
      "3.02706\n",
      "4.14629\n",
      "0.823545\n",
      "0.66466\n",
      "0.620573\n",
      "1.43201\n",
      "0.890223\n",
      "0.527961\n",
      "0.875475\n",
      "0.945651\n",
      "1.02214\n",
      "0.644784\n",
      "1.86226\n",
      "0.639269\n",
      "0.257589\n",
      "1.05231\n",
      "1.73377\n",
      "1.68587\n",
      "1.37311\n",
      "0.825093\n",
      "0.813517\n",
      "0.807135\n",
      "0.5315\n",
      "0.582521\n",
      "0.642875\n",
      "0.426862\n",
      "0.254813\n",
      "0.627753\n",
      "0.281803\n",
      "1.09358\n",
      "0.992849\n",
      "0.583594\n",
      "0.599747\n",
      "0.790745\n",
      "0.909509\n",
      "0.625122\n",
      "0.592319\n",
      "0.710557\n",
      "0.31518\n",
      "0.594268\n",
      "0.36962\n",
      "0.622061\n",
      "0.896955\n",
      "0.50525\n",
      "0.66937\n",
      "0.996829\n",
      "1.02279\n",
      "1.36689\n",
      "1.34901\n",
      "1.07409\n",
      "0.703812\n",
      "1.01872\n",
      "0.392808\n",
      "0.936281\n",
      "0.658306\n",
      "0.970547\n",
      "0.502513\n",
      "0.812041\n",
      "1.19922\n",
      "0.860019\n",
      "0.978124\n",
      "1.51349\n",
      "1.47566\n",
      "2.2655\n",
      "2.63099\n",
      "2.12552\n",
      "2.78785\n",
      "0.8075\n",
      "0.545126\n",
      "0.892694\n",
      "0.723072\n",
      "1.63387\n",
      "1.49214\n",
      "0.721532\n",
      "0.668378\n",
      "0.433519\n",
      "1.04909\n",
      "0.525204\n",
      "1.21412\n",
      "0.888766\n",
      "3.04956\n",
      "2.52433\n",
      "0.59868\n",
      "0.693841\n",
      "1.30287\n",
      "0.872337\n",
      "0.439651\n",
      "0.823264\n",
      "2.31488\n",
      "1.84071\n",
      "1.32118\n",
      "1.78672\n",
      "1.75509\n",
      "1.70274\n",
      "0.893143\n",
      "0.360253\n",
      "1.1038\n",
      "0.571679\n",
      "0.511073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.627697\n",
      "0.565172\n",
      "0.394845\n",
      "0.848094\n",
      "1.09491\n",
      "0.942903\n",
      "1.0741\n",
      "0.530193\n",
      "0.733032\n",
      "0.706772\n",
      "0.612096\n",
      "0.787466\n",
      "0.665485\n",
      "0.498797\n",
      "1.01453\n",
      "0.659775\n",
      "1.44289\n",
      "0.500422\n",
      "1.28188\n",
      "1.81709\n",
      "1.36294\n",
      "0.497428\n",
      "0.714537\n",
      "0.624533\n",
      "0.378282\n",
      "0.385574\n",
      "0.766607\n",
      "0.952233\n",
      "1.23217\n",
      "0.642658\n",
      "0.413898\n",
      "0.455395\n",
      "0.620472\n",
      "0.659955\n",
      "0.819609\n",
      "0.757786\n",
      "0.401389\n",
      "0.597105\n",
      "0.723675\n",
      "1.34011\n",
      "0.766505\n",
      "0.786348\n",
      "0.302816\n",
      "0.459963\n",
      "0.343648\n",
      "0.751965\n",
      "0.416017\n",
      "0.812471\n",
      "1.81345\n",
      "2.02501\n",
      "0.830642\n",
      "1.52403\n",
      "0.668112\n",
      "0.674044\n",
      "1.35983\n",
      "1.30116\n",
      "1.29972\n",
      "0.550823\n",
      "1.04398\n",
      "0.893356\n",
      "0.74318\n",
      "0.878094\n",
      "0.353607\n",
      "0.591705\n",
      "0.879239\n",
      "1.06761\n",
      "1.01216\n",
      "0.362479\n",
      "0.466173\n",
      "1.2566\n",
      "1.27989\n",
      "4.47992\n",
      "2.25869\n",
      "2.06158\n",
      "1.10947\n",
      "0.413664\n",
      "0.806731\n",
      "0.750832\n",
      "0.78102\n",
      "1.75803\n",
      "0.612012\n",
      "0.721715\n",
      "0.738648\n",
      "0.462995\n",
      "1.07426\n",
      "0.864174\n",
      "0.894959\n",
      "11.5057\n",
      "2.06664\n",
      "1.63709\n",
      "1.76161\n",
      "1.39124\n",
      "0.980941\n",
      "0.926962\n",
      "1.19762\n",
      "1.23733\n",
      "0.849782\n",
      "0.476133\n",
      "1.39313\n",
      "3.71617\n",
      "1.21665\n",
      "3.15408\n",
      "0.952749\n",
      "0.475597\n",
      "0.33859\n",
      "0.675977\n",
      "0.524887\n",
      "0.871596\n",
      "0.656582\n",
      "0.500581\n",
      "0.582622\n",
      "1.02156\n",
      "0.989965\n",
      "1.065\n",
      "0.807601\n",
      "0.590332\n",
      "0.668423\n",
      "0.663104\n",
      "0.272195\n",
      "1.0421\n",
      "0.589151\n",
      "1.09804\n",
      "0.814998\n",
      "0.385803\n",
      "0.490642\n",
      "1.00783\n",
      "0.39338\n",
      "0.575356\n",
      "0.412828\n",
      "0.416818\n",
      "1.59025\n",
      "0.657741\n",
      "0.83938\n",
      "0.816066\n",
      "1.34723\n",
      "1.27378\n",
      "0.89093\n",
      "0.833947\n",
      "0.884807\n",
      "2.08183\n",
      "3.17717\n",
      "2.57608\n",
      "1.33975\n",
      "0.941007\n",
      "0.349577\n",
      "1.03633\n",
      "0.532391\n",
      "0.235945\n",
      "1.01137\n",
      "0.586843\n",
      "0.998039\n",
      "1.14199\n",
      "0.745269\n",
      "0.637307\n",
      "0.987355\n",
      "0.90181\n",
      "0.847909\n",
      "1.17975\n",
      "1.02941\n",
      "0.973556\n",
      "0.786692\n",
      "0.842999\n",
      "0.632005\n",
      "0.515462\n",
      "0.536796\n",
      "1.22073\n",
      "0.540806\n",
      "0.460158\n",
      "0.790994\n",
      "0.732366\n",
      "0.478262\n",
      "0.445635\n",
      "0.790224\n",
      "1.26307\n",
      "0.78838\n",
      "0.397545\n",
      "0.390925\n",
      "0.661104\n",
      "0.403539\n",
      "0.489288\n",
      "0.220266\n",
      "0.387169\n",
      "0.618151\n",
      "0.619792\n",
      "0.786019\n",
      "0.392694\n",
      "1.66569\n",
      "0.520944\n",
      "0.679361\n",
      "0.521302\n",
      "0.663823\n",
      "0.56574\n",
      "0.501774\n",
      "0.76512\n",
      "0.660788\n",
      "0.440207\n",
      "0.223663\n",
      "0.626543\n",
      "0.448707\n",
      "0.52433\n",
      "1.34629\n",
      "0.949836\n",
      "0.787004\n",
      "0.474827\n",
      "0.768464\n",
      "0.690363\n",
      "0.368156\n",
      "0.405454\n",
      "0.446137\n",
      "0.344372\n",
      "0.351106\n",
      "0.508071\n",
      "0.693922\n",
      "0.558599\n",
      "0.571969\n",
      "0.541339\n",
      "0.639712\n",
      "1.29612\n",
      "0.640002\n",
      "0.852778\n",
      "0.722577\n",
      "0.790258\n",
      "0.7989\n",
      "0.651228\n",
      "0.295643\n",
      "0.5086\n",
      "0.462083\n",
      "0.679599\n",
      "0.794159\n",
      "0.382001\n",
      "0.853305\n",
      "0.626463\n",
      "0.522283\n",
      "0.811333\n",
      "0.333777\n",
      "0.737829\n",
      "0.516781\n",
      "0.535244\n",
      "0.343763\n",
      "0.571189\n",
      "1.02201\n",
      "0.507154\n",
      "0.489542\n",
      "0.309616\n",
      "0.43507\n",
      "0.510526\n",
      "0.726652\n",
      "1.02975\n",
      "0.688595\n",
      "0.583728\n",
      "0.320714\n",
      "0.460185\n",
      "0.52667\n",
      "0.728451\n",
      "0.380187\n",
      "0.622749\n",
      "0.761918\n",
      "1.32682\n",
      "0.621975\n",
      "0.635719\n",
      "0.831587\n",
      "0.674059\n",
      "0.473444\n",
      "1.43431\n",
      "0.422773\n",
      "0.647879\n",
      "1.00287\n",
      "0.966211\n",
      "1.4813\n",
      "0.905162\n",
      "0.964354\n",
      "0.949299\n",
      "0.912174\n",
      "0.860368\n",
      "0.665141\n",
      "1.01653\n",
      "0.535961\n",
      "1.39656\n",
      "0.923181\n",
      "0.663716\n",
      "0.684936\n",
      "0.561463\n",
      "0.95782\n",
      "0.616243\n",
      "0.804913\n",
      "0.806858\n",
      "1.23817\n",
      "0.490553\n",
      "1.74328\n",
      "1.16836\n",
      "0.754677\n",
      "0.625431\n",
      "0.519942\n",
      "0.523418\n",
      "0.287784\n",
      "0.332474\n",
      "0.499696\n",
      "0.392769\n",
      "0.670109\n",
      "0.527424\n",
      "2.16941\n",
      "6.33702\n",
      "5.26508\n",
      "6.389\n",
      "5.33034\n",
      "3.05651\n",
      "1.70608\n",
      "1.12724\n",
      "1.25127\n",
      "1.29599\n",
      "0.699712\n",
      "0.313102\n",
      "1.00234\n",
      "0.706294\n",
      "0.963742\n",
      "0.91207\n",
      "1.17517\n",
      "1.85991\n",
      "1.07492\n",
      "1.26985\n",
      "0.485138\n",
      "0.864909\n",
      "1.15657\n",
      "1.829\n",
      "1.41731\n",
      "0.839143\n",
      "0.497629\n",
      "0.338903\n",
      "1.13988\n",
      "0.477015\n",
      "1.89965\n",
      "1.32077\n",
      "2.73621\n",
      "2.40827\n",
      "0.933832\n",
      "1.2522\n",
      "1.25283\n",
      "0.607697\n",
      "0.696808\n",
      "0.918491\n",
      "0.424702\n",
      "0.456626\n",
      "0.498671\n",
      "0.51018\n",
      "0.36989\n",
      "0.493264\n",
      "0.397546\n",
      "0.341207\n",
      "0.833093\n",
      "1.75205\n",
      "1.29593\n",
      "0.707628\n",
      "1.30402\n",
      "1.14738\n",
      "1.17407\n",
      "0.975954\n",
      "0.759906\n",
      "0.309949\n",
      "1.18592\n",
      "0.711383\n",
      "0.704441\n",
      "0.928072\n",
      "0.615623\n",
      "0.609413\n",
      "0.772011\n",
      "0.895898\n",
      "0.887863\n",
      "0.476128\n",
      "4.50322\n",
      "1.03213\n",
      "1.07321\n",
      "0.915409\n",
      "0.728689\n",
      "0.550191\n",
      "0.380532\n",
      "0.247499\n",
      "0.652444\n",
      "0.571786\n",
      "0.338009\n",
      "0.469477\n",
      "1.23205\n",
      "0.803026\n",
      "0.722582\n",
      "0.415918\n",
      "0.347818\n",
      "0.478859\n",
      "0.686949\n",
      "0.458191\n",
      "0.467095\n",
      "0.405316\n",
      "0.774614\n",
      "0.93055\n",
      "0.603744\n",
      "0.358774\n",
      "1.22333\n",
      "0.460894\n",
      "0.791313\n",
      "0.421008\n",
      "0.578574\n",
      "0.833588\n",
      "0.506102\n",
      "0.376117\n",
      "0.633844\n",
      "3.6926\n",
      "3.78137\n",
      "2.82253\n",
      "3.1251\n",
      "3.91613\n",
      "2.71176\n",
      "0.920001\n",
      "1.07358\n",
      "0.954253\n",
      "1.11941\n",
      "0.486932\n",
      "0.443556\n",
      "0.887515\n",
      "0.600798\n",
      "0.634722\n",
      "0.771927\n",
      "0.381863\n",
      "0.458043\n",
      "0.48213\n",
      "0.77614\n",
      "0.797874\n",
      "0.519165\n",
      "0.543666\n",
      "0.35628\n",
      "0.401897\n",
      "0.71783\n",
      "0.879127\n",
      "0.432415\n",
      "0.85481\n",
      "0.927295\n",
      "0.686628\n",
      "0.986268\n",
      "1.16427\n",
      "0.480624\n",
      "0.484278\n",
      "0.570934\n",
      "0.51377\n",
      "0.852987\n",
      "0.528703\n",
      "0.352486\n",
      "0.730699\n",
      "0.338543\n",
      "0.552025\n",
      "1.03814\n",
      "0.779817\n",
      "2.14284\n",
      "1.59162\n",
      "2.07614\n",
      "2.47618\n",
      "1.1626\n",
      "0.913528\n",
      "1.12424\n",
      "0.731539\n",
      "0.577142\n",
      "0.936861\n",
      "0.94766\n",
      "0.488505\n",
      "0.841593\n",
      "0.781963\n",
      "0.838373\n",
      "0.596149\n",
      "0.571956\n",
      "0.822468\n",
      "0.826883\n",
      "0.867061\n",
      "0.985458\n",
      "0.31258\n",
      "0.539967\n",
      "0.765458\n",
      "0.996558\n",
      "0.59402\n",
      "0.852079\n",
      "0.805375\n",
      "0.749696\n",
      "0.600179\n",
      "1.10803\n",
      "1.22087\n",
      "1.42868\n",
      "1.28402\n",
      "0.70337\n",
      "0.735266\n",
      "0.31164\n",
      "0.37953\n",
      "1.35313\n",
      "1.386\n",
      "1.01965\n",
      "0.797579\n",
      "0.857264\n",
      "0.478016\n",
      "0.433369\n",
      "0.431308\n",
      "0.505773\n",
      "0.796783\n",
      "0.822487\n",
      "0.466567\n",
      "0.871334\n",
      "0.861223\n",
      "0.950636\n",
      "0.741931\n",
      "1.03357\n",
      "0.918572\n",
      "1.72326\n",
      "0.893641\n",
      "0.912852\n",
      "0.85851\n",
      "0.64945\n",
      "0.746045\n",
      "1.37483\n",
      "1.08241\n",
      "0.998368\n",
      "0.517349\n",
      "0.535513\n",
      "0.338079\n",
      "0.303373\n",
      "0.452479\n",
      "0.754015\n",
      "0.564746\n",
      "0.291898\n",
      "1.12539\n",
      "0.456251\n",
      "0.593998\n",
      "1.26596\n",
      "0.656723\n",
      "0.524911\n",
      "0.488753\n",
      "0.270837\n",
      "0.975882\n",
      "1.17715\n",
      "1.37363\n",
      "1.51438\n",
      "1.61049\n",
      "1.45758\n",
      "1.33765\n",
      "0.655402\n",
      "0.521572\n",
      "0.371908\n",
      "0.714687\n",
      "0.800539\n",
      "1.26311\n",
      "0.76885\n",
      "0.742435\n",
      "1.03509\n",
      "0.320504\n",
      "0.425585\n",
      "0.782516\n",
      "0.687796\n",
      "0.606703\n",
      "0.345513\n",
      "0.529464\n",
      "0.496473\n",
      "0.36679\n",
      "0.460356\n",
      "0.793159\n",
      "0.715511\n",
      "0.924771\n",
      "1.44228\n",
      "1.20175\n",
      "0.487895\n",
      "0.404004\n",
      "0.691841\n",
      "1.15147\n",
      "0.896683\n",
      "0.235544\n",
      "0.585063\n",
      "0.657187\n",
      "0.52556\n",
      "0.886188\n",
      "0.649575\n",
      "0.798545\n",
      "0.404471\n",
      "0.500066\n",
      "0.723917\n",
      "0.585793\n",
      "0.449103\n",
      "2.26542\n",
      "0.708376\n",
      "0.764332\n",
      "0.66659\n",
      "0.545377\n",
      "0.459927\n",
      "0.471524\n",
      "0.461262\n",
      "1.32687\n",
      "1.89296\n",
      "1.90266\n",
      "0.457547\n",
      "0.568645\n",
      "1.93444\n",
      "1.90396\n",
      "1.51056\n",
      "1.18574\n",
      "2.92346\n",
      "4.57467\n",
      "2.73553\n",
      "0.898801\n",
      "0.483027\n",
      "1.34348\n",
      "0.518646\n",
      "0.390617\n",
      "0.353032\n",
      "0.637601\n",
      "0.781146\n",
      "0.833426\n",
      "0.260053\n",
      "0.412518\n",
      "0.226597\n",
      "0.562288\n",
      "0.541796\n",
      "0.84892\n",
      "0.672276\n",
      "0.458327\n",
      "0.773358\n",
      "1.79256\n",
      "0.475446\n",
      "0.797672\n",
      "1.01413\n",
      "1.31721\n",
      "0.493899\n",
      "0.426916\n",
      "1.23148\n",
      "1.35241\n",
      "0.894332\n",
      "0.844078\n",
      "0.474376\n",
      "0.596369\n",
      "0.658765\n",
      "0.555023\n",
      "0.625992\n",
      "0.409352\n",
      "0.482737\n",
      "0.886985\n",
      "0.494475\n",
      "0.308112\n",
      "0.340874\n",
      "0.473389\n",
      "0.495648\n",
      "0.628151\n",
      "0.495316\n",
      "0.622572\n",
      "0.597675\n",
      "0.392705\n",
      "0.57198\n",
      "0.84073\n",
      "0.507306\n",
      "0.344612\n",
      "0.475259\n",
      "0.445384\n",
      "0.94312\n",
      "0.362221\n",
      "0.323296\n",
      "1.55177\n",
      "2.61712\n",
      "1.47952\n",
      "1.13732\n",
      "0.885085\n",
      "5.64419\n",
      "1.28012\n",
      "1.04533\n",
      "0.682927\n",
      "0.804606\n",
      "0.801743\n",
      "0.981466\n",
      "0.791892\n",
      "1.08982\n",
      "0.513158\n",
      "0.666114\n",
      "0.56118\n",
      "0.546783\n",
      "0.720586\n",
      "0.652348\n",
      "0.93589\n",
      "0.454676\n",
      "0.410202\n",
      "0.901728\n",
      "0.626628\n",
      "1.01937\n",
      "1.08048\n",
      "1.05461\n",
      "0.293712\n",
      "0.698368\n",
      "0.399847\n",
      "0.63646\n",
      "0.832973\n",
      "0.638887\n",
      "0.395454\n",
      "0.430391\n",
      "0.236897\n",
      "0.496375\n",
      "0.729837\n",
      "0.521301\n",
      "0.557045\n",
      "0.668781\n",
      "0.300388\n",
      "0.363623\n",
      "0.234573\n",
      "0.597851\n",
      "1.09074\n",
      "0.616266\n",
      "0.953299\n",
      "0.412019\n",
      "0.345232\n",
      "0.441645\n",
      "0.419179\n",
      "0.427731\n",
      "0.658687\n",
      "0.404533\n",
      "0.294598\n",
      "0.565427\n",
      "0.370889\n",
      "0.376832\n",
      "0.686988\n",
      "0.7205\n",
      "0.683628\n",
      "0.583713\n",
      "0.966208\n",
      "1.23734\n",
      "1.25901\n",
      "0.824997\n",
      "0.57612\n",
      "2.67925\n",
      "1.5831\n",
      "1.20492\n",
      "0.920226\n",
      "0.680131\n",
      "0.666677\n",
      "0.515005\n",
      "1.092\n",
      "0.437111\n",
      "2.71997\n",
      "0.939165\n",
      "1.79717\n",
      "1.51672\n",
      "1.1569\n",
      "0.875788\n",
      "0.54942\n",
      "0.505138\n",
      "1.27935\n",
      "0.914439\n",
      "0.535789\n",
      "0.27444\n",
      "0.310403\n",
      "0.81428\n",
      "0.535243\n",
      "0.351104\n",
      "1.33502\n",
      "0.391845\n",
      "0.449425\n",
      "0.359675\n",
      "0.37394\n",
      "0.544122\n",
      "0.454689\n",
      "1.55079\n",
      "0.515792\n",
      "1.042\n",
      "0.771793\n",
      "0.588394\n",
      "0.370178\n",
      "0.530252\n",
      "0.396902\n",
      "0.7988\n",
      "0.415285\n",
      "0.327662\n",
      "0.510797\n",
      "0.740492\n",
      "0.566847\n",
      "0.623089\n",
      "1.19912\n",
      "0.432631\n",
      "0.324968\n",
      "0.64171\n",
      "0.900154\n",
      "0.707672\n",
      "1.03428\n",
      "0.447116\n",
      "0.613282\n",
      "0.705342\n",
      "0.250719\n",
      "0.57351\n",
      "0.594169\n",
      "0.433277\n",
      "0.285969\n",
      "0.753536\n",
      "0.523435\n",
      "0.541866\n",
      "0.908148\n",
      "0.359114\n",
      "0.991461\n",
      "0.892724\n",
      "1.08304\n",
      "0.463722\n",
      "0.198693\n",
      "0.520393\n",
      "0.600348\n",
      "0.744568\n",
      "1.44649\n",
      "0.737311\n",
      "0.705477\n",
      "0.788004\n",
      "1.64902\n",
      "0.723288\n",
      "0.582856\n",
      "1.01729\n",
      "0.680163\n",
      "0.697819\n",
      "0.838907\n",
      "0.897089\n",
      "0.685401\n",
      "0.581453\n",
      "0.246973\n",
      "0.947618\n",
      "0.430968\n",
      "0.37758\n",
      "0.292758\n",
      "0.534784\n",
      "0.40411\n",
      "0.522061\n",
      "1.44059\n",
      "0.314964\n",
      "0.741122\n",
      "0.73639\n",
      "0.804711\n",
      "0.39035\n",
      "0.431834\n",
      "0.785698\n",
      "0.3164\n",
      "0.373829\n",
      "0.457433\n",
      "0.367071\n",
      "0.973109\n",
      "0.788566\n",
      "0.490091\n",
      "0.188572\n",
      "0.633118\n",
      "0.481278\n",
      "0.388123\n",
      "0.651302\n",
      "0.451437\n",
      "0.33454\n",
      "0.672683\n",
      "0.732394\n",
      "1.28186\n",
      "1.57867\n",
      "1.29666\n",
      "0.93599\n",
      "0.696482\n",
      "1.23803\n",
      "0.305066\n",
      "0.28958\n",
      "0.369745\n",
      "0.281964\n",
      "0.623227\n",
      "0.927289\n",
      "0.69436\n",
      "0.615247\n",
      "0.560645\n",
      "0.498237\n",
      "1.6116\n",
      "1.10473\n",
      "0.283172\n",
      "0.862443\n",
      "0.329011\n",
      "1.02245\n",
      "1.12147\n",
      "0.907109\n",
      "0.35744\n",
      "0.330107\n",
      "0.422639\n",
      "0.550207\n",
      "0.276697\n",
      "0.173673\n",
      "0.472021\n",
      "0.897799\n",
      "0.628625\n",
      "0.454123\n",
      "0.963294\n",
      "0.352567\n",
      "0.556841\n",
      "0.295408\n",
      "0.680576\n",
      "0.527993\n",
      "0.40156\n",
      "0.457395\n",
      "1.20887\n",
      "0.668643\n",
      "0.551574\n",
      "0.329198\n",
      "0.40659\n",
      "0.555727\n",
      "0.392029\n",
      "0.266367\n",
      "0.476766\n",
      "0.776364\n",
      "0.459166\n",
      "0.821094\n",
      "0.398369\n",
      "0.600324\n",
      "0.381815\n",
      "0.364257\n",
      "0.464828\n",
      "1.02359\n",
      "0.650018\n",
      "0.56016\n",
      "0.318178\n",
      "0.556472\n",
      "0.932324\n",
      "0.377334\n",
      "0.984784\n",
      "1.0085\n",
      "1.0086\n",
      "0.736938\n",
      "0.463364\n",
      "1.02535\n",
      "1.09859\n",
      "1.78312\n",
      "1.80301\n",
      "1.7566\n",
      "1.48196\n",
      "0.88604\n",
      "0.718243\n",
      "0.323594\n",
      "0.618809\n",
      "12.1112\n",
      "6.32035\n",
      "5.68964\n",
      "3.95226\n",
      "3.19738\n",
      "1.49348\n",
      "1.10783\n",
      "0.588507\n",
      "0.425413\n",
      "0.433066\n",
      "2.44917\n",
      "1.27284\n",
      "0.954043\n",
      "1.10939\n",
      "0.449062\n",
      "0.75628\n",
      "0.476804\n",
      "0.774163\n",
      "0.575106\n",
      "0.691354\n",
      "0.520577\n",
      "0.46167\n",
      "0.590677\n",
      "0.644504\n",
      "0.518299\n",
      "0.463401\n",
      "0.675742\n",
      "0.470725\n",
      "7.71262\n",
      "1.84087\n",
      "0.716352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.355321\n",
      "0.551395\n",
      "0.564351\n",
      "0.929727\n",
      "1.35679\n",
      "0.625339\n",
      "0.863229\n",
      "0.323636\n",
      "0.469646\n",
      "1.56658\n",
      "0.455623\n",
      "0.403214\n",
      "0.506108\n",
      "0.340015\n",
      "0.419847\n",
      "0.26646\n",
      "0.728726\n",
      "0.611199\n",
      "0.834523\n",
      "0.461916\n",
      "7.80063\n",
      "13.0201\n",
      "4.65688\n",
      "2.96661\n",
      "1.60524\n",
      "1.4864\n",
      "0.917153\n",
      "1.38632\n",
      "0.353491\n",
      "1.48483\n",
      "1.53533\n",
      "0.879167\n",
      "0.48646\n",
      "0.896665\n",
      "1.13569\n",
      "0.684406\n",
      "0.715705\n",
      "0.528922\n",
      "0.58659\n",
      "0.496789\n",
      "0.493616\n",
      "0.610293\n",
      "0.448609\n",
      "0.271086\n",
      "0.795813\n",
      "1.26079\n",
      "0.748927\n",
      "0.336881\n",
      "0.842901\n",
      "0.770531\n",
      "0.92565\n",
      "0.642415\n",
      "0.832754\n",
      "1.35094\n",
      "0.831159\n",
      "0.808885\n",
      "0.450859\n",
      "1.03287\n",
      "0.50726\n",
      "0.712985\n",
      "0.477848\n",
      "0.466387\n",
      "0.508157\n",
      "0.559483\n",
      "1.17737\n",
      "0.285961\n",
      "0.856481\n",
      "0.904151\n",
      "0.578592\n",
      "0.362297\n",
      "0.602113\n",
      "0.450856\n",
      "0.947319\n",
      "0.817234\n",
      "0.596562\n",
      "0.844293\n",
      "0.578828\n",
      "0.888885\n",
      "0.709738\n",
      "0.958089\n",
      "1.07102\n",
      "0.40732\n",
      "0.844317\n",
      "0.491109\n",
      "0.435921\n",
      "0.658287\n",
      "1.05372\n",
      "0.358093\n",
      "0.439704\n",
      "0.830124\n",
      "0.548604\n",
      "0.517404\n",
      "0.584848\n",
      "2.68626\n",
      "0.955136\n",
      "0.779077\n",
      "0.636995\n",
      "0.879498\n",
      "0.478342\n",
      "0.635106\n",
      "0.390041\n",
      "0.484304\n",
      "0.544633\n",
      "0.297898\n",
      "0.705549\n",
      "0.990772\n",
      "3.93655\n",
      "3.37317\n",
      "2.16751\n",
      "1.10857\n",
      "0.702773\n",
      "0.801479\n",
      "0.283079\n",
      "0.640161\n",
      "0.58209\n",
      "1.3623\n",
      "0.965121\n",
      "0.611106\n",
      "0.476192\n",
      "0.465785\n",
      "0.318467\n",
      "0.592705\n",
      "0.358465\n",
      "0.360042\n",
      "1.47221\n",
      "1.9494\n",
      "0.717767\n",
      "0.845712\n",
      "1.27507\n",
      "1.09379\n",
      "0.333131\n",
      "0.871392\n",
      "0.626686\n",
      "0.725041\n",
      "0.909929\n",
      "0.585633\n",
      "0.349328\n",
      "1.11745\n",
      "0.542598\n",
      "0.272923\n",
      "0.699919\n",
      "0.291318\n",
      "0.662803\n",
      "0.984487\n",
      "0.253133\n",
      "0.878185\n",
      "0.447966\n",
      "1.07408\n",
      "5.94968\n",
      "3.55156\n",
      "2.6932\n",
      "2.4351\n",
      "2.99553\n",
      "2.3682\n",
      "1.29227\n",
      "0.417676\n",
      "0.412443\n",
      "0.617597\n",
      "0.953304\n",
      "0.605524\n",
      "0.726141\n",
      "0.647537\n",
      "0.852269\n",
      "0.564209\n",
      "0.545114\n",
      "0.266226\n",
      "1.00934\n",
      "0.604259\n",
      "0.339443\n",
      "0.342243\n",
      "0.745505\n",
      "0.774339\n",
      "0.381275\n",
      "0.356086\n",
      "0.157545\n",
      "0.69337\n",
      "0.294633\n",
      "0.493985\n",
      "0.446781\n",
      "0.446423\n",
      "0.533467\n",
      "0.605483\n",
      "0.373744\n",
      "0.377057\n",
      "0.419173\n",
      "0.76679\n",
      "0.386908\n",
      "0.340552\n",
      "0.549793\n",
      "0.605886\n",
      "0.20359\n",
      "0.585332\n",
      "0.457127\n",
      "0.223127\n",
      "0.620951\n",
      "0.584182\n",
      "0.961315\n",
      "0.17077\n",
      "0.413922\n",
      "0.397043\n",
      "0.590593\n",
      "0.444128\n",
      "2.07545\n",
      "2.78802\n",
      "1.61554\n",
      "0.567789\n",
      "0.777679\n",
      "0.774045\n",
      "1.03811\n",
      "0.622136\n",
      "0.529577\n",
      "1.02956\n",
      "1.69045\n",
      "1.08512\n",
      "0.673797\n",
      "0.697032\n",
      "0.595665\n",
      "0.256237\n",
      "0.291064\n",
      "0.76247\n",
      "0.65834\n",
      "0.444956\n",
      "0.686683\n",
      "0.426516\n",
      "0.344178\n",
      "0.368696\n",
      "1.30042\n",
      "0.703037\n",
      "0.451825\n",
      "0.266052\n",
      "0.124973\n",
      "0.386723\n",
      "0.524723\n",
      "0.537622\n",
      "0.628229\n",
      "0.431338\n",
      "0.823468\n",
      "0.567075\n",
      "0.200831\n",
      "0.693982\n",
      "2.37645\n",
      "2.86494\n",
      "1.17657\n",
      "1.3992\n",
      "2.42914\n",
      "1.69186\n",
      "2.26988\n",
      "2.36119\n",
      "1.53112\n",
      "1.32082\n",
      "0.411394\n",
      "0.372459\n",
      "0.205679\n",
      "0.972985\n",
      "1.5504\n",
      "0.810016\n",
      "0.5571\n",
      "0.38458\n",
      "0.269319\n",
      "0.426915\n",
      "0.440082\n",
      "0.560038\n",
      "0.544577\n",
      "0.358306\n",
      "0.640102\n",
      "0.463379\n",
      "0.412198\n",
      "0.71452\n",
      "0.370725\n",
      "0.334712\n",
      "0.311877\n",
      "1.47987\n",
      "0.729053\n",
      "0.815849\n",
      "0.512997\n",
      "0.28044\n",
      "0.376404\n",
      "0.63379\n",
      "0.909701\n",
      "0.478347\n",
      "1.17669\n",
      "0.912787\n",
      "0.809507\n",
      "5.36324\n",
      "1.36773\n",
      "1.14849\n",
      "1.03425\n",
      "0.234784\n",
      "0.822059\n",
      "0.382693\n",
      "0.273757\n",
      "0.373503\n",
      "0.601397\n",
      "0.511425\n",
      "0.611246\n",
      "0.509209\n",
      "0.36073\n",
      "0.502725\n",
      "0.78323\n",
      "0.244658\n",
      "0.600103\n",
      "1.182\n",
      "0.486582\n",
      "0.334314\n",
      "0.565335\n",
      "0.409631\n",
      "0.426302\n",
      "0.764588\n",
      "0.499948\n",
      "0.431857\n",
      "0.44934\n",
      "0.710471\n",
      "0.573655\n",
      "0.608555\n",
      "0.837302\n",
      "0.58793\n",
      "0.574482\n",
      "0.309858\n",
      "0.627952\n",
      "0.652516\n",
      "1.76809\n",
      "0.746711\n",
      "0.545153\n",
      "0.218864\n",
      "0.451785\n",
      "0.433362\n",
      "0.615986\n",
      "0.818679\n",
      "0.855929\n",
      "0.3483\n",
      "0.314786\n",
      "1.00459\n",
      "0.756616\n",
      "0.514748\n",
      "0.403006\n",
      "0.523146\n",
      "0.377201\n",
      "2.62375\n",
      "0.633504\n",
      "1.07118\n",
      "0.92311\n",
      "0.814294\n",
      "0.435195\n",
      "0.627908\n",
      "0.236153\n",
      "0.163033\n",
      "2.39028\n",
      "0.846561\n",
      "0.621152\n",
      "0.795164\n",
      "1.16896\n",
      "0.960665\n",
      "2.69312\n",
      "0.699587\n",
      "0.53897\n",
      "0.437191\n",
      "0.518677\n",
      "0.291476\n",
      "1.39279\n",
      "1.20343\n",
      "0.78201\n",
      "1.2483\n",
      "0.709347\n",
      "0.374225\n",
      "0.300526\n",
      "0.559634\n",
      "1.84393\n",
      "1.2841\n",
      "0.567968\n",
      "0.45643\n",
      "0.43762\n",
      "0.850404\n",
      "0.533838\n",
      "0.722155\n",
      "1.28731\n",
      "0.795175\n",
      "0.663391\n",
      "0.549496\n",
      "1.79994\n",
      "1.71733\n",
      "1.11704\n",
      "1.30306\n",
      "0.576957\n",
      "0.676944\n",
      "0.636732\n",
      "0.793546\n",
      "0.963711\n",
      "0.624473\n",
      "0.499135\n",
      "0.424375\n",
      "0.313085\n",
      "0.523705\n",
      "0.587319\n",
      "0.429821\n",
      "0.574795\n",
      "0.550014\n",
      "0.353388\n",
      "0.797712\n",
      "0.481191\n",
      "0.468293\n",
      "0.202283\n",
      "1.04224\n",
      "0.203896\n",
      "0.169077\n",
      "0.796334\n",
      "0.219295\n",
      "0.440305\n",
      "0.639299\n",
      "0.662452\n",
      "0.588638\n",
      "0.452812\n",
      "0.658208\n",
      "0.290048\n",
      "0.897332\n",
      "0.507432\n",
      "0.220563\n",
      "0.445356\n",
      "1.07109\n",
      "1.1349\n",
      "0.94019\n",
      "0.541545\n",
      "10.5258\n",
      "1.61309\n",
      "1.72229\n",
      "2.82823\n",
      "1.08151\n",
      "0.452606\n",
      "0.729128\n",
      "0.439927\n",
      "0.727855\n",
      "1.3885\n",
      "0.801472\n",
      "0.453131\n",
      "0.514395\n",
      "0.767147\n",
      "0.715387\n",
      "0.723649\n",
      "1.50187\n",
      "1.19027\n",
      "0.82958\n",
      "0.482292\n",
      "0.371371\n",
      "0.232723\n",
      "0.346763\n",
      "0.256881\n",
      "0.499792\n",
      "0.296608\n",
      "0.545164\n",
      "0.860389\n",
      "0.391194\n",
      "0.95617\n",
      "0.545244\n",
      "0.727132\n",
      "0.312895\n",
      "0.538178\n",
      "0.299083\n",
      "0.568654\n",
      "0.483392\n",
      "0.33295\n",
      "0.838871\n",
      "0.449355\n",
      "0.37068\n",
      "0.839664\n",
      "1.07981\n",
      "0.970307\n",
      "0.571714\n",
      "0.488718\n",
      "0.390204\n",
      "0.308083\n",
      "0.267836\n",
      "0.522081\n",
      "0.328953\n",
      "0.232143\n",
      "0.596892\n",
      "0.469952\n",
      "0.554583\n",
      "0.928244\n",
      "0.667027\n",
      "0.557648\n",
      "0.253097\n",
      "0.570116\n",
      "0.387217\n",
      "0.432807\n",
      "0.494217\n",
      "0.474938\n",
      "0.666595\n",
      "0.525859\n",
      "0.599551\n",
      "0.449156\n",
      "0.512021\n",
      "0.396229\n",
      "1.06364\n",
      "1.17253\n",
      "2.42797\n",
      "1.30198\n",
      "2.07795\n",
      "1.48307\n",
      "0.603965\n",
      "0.632353\n",
      "0.482059\n",
      "0.559612\n",
      "0.413593\n",
      "0.502602\n",
      "0.460533\n",
      "0.438926\n",
      "0.393511\n",
      "0.482388\n",
      "0.674325\n",
      "0.512214\n",
      "0.341661\n",
      "0.709746\n",
      "0.771503\n",
      "1.03342\n",
      "0.349295\n",
      "0.322682\n",
      "0.379694\n",
      "0.217886\n",
      "0.899149\n",
      "0.917393\n",
      "0.959311\n",
      "0.643906\n",
      "0.477475\n",
      "0.353159\n",
      "0.642679\n",
      "0.491948\n",
      "0.482664\n",
      "0.522081\n",
      "0.235096\n",
      "0.325663\n",
      "0.361953\n",
      "0.285412\n",
      "0.314856\n",
      "0.292346\n",
      "0.605503\n",
      "0.312077\n",
      "0.873125\n",
      "0.475899\n",
      "0.393455\n",
      "0.281908\n",
      "0.67592\n",
      "0.43422\n",
      "0.588009\n",
      "0.631149\n",
      "0.660175\n",
      "0.681777\n",
      "0.895776\n",
      "0.559873\n",
      "0.357176\n",
      "0.653002\n",
      "0.950758\n",
      "0.469518\n",
      "0.830691\n",
      "0.554178\n",
      "0.698287\n",
      "0.546028\n",
      "0.230623\n",
      "0.519092\n",
      "1.21322\n",
      "0.761342\n",
      "1.52974\n",
      "1.60964\n",
      "1.22334\n",
      "0.516522\n",
      "0.285861\n",
      "0.123655\n",
      "0.369727\n",
      "0.502694\n",
      "0.774233\n",
      "0.291342\n",
      "0.331647\n",
      "0.553192\n",
      "0.460857\n",
      "0.210557\n",
      "0.308263\n",
      "0.379425\n",
      "0.255023\n",
      "0.204921\n",
      "0.530573\n",
      "0.778438\n",
      "0.484796\n",
      "0.255053\n",
      "0.557369\n",
      "1.41934\n",
      "0.459427\n",
      "0.558414\n",
      "0.343654\n",
      "0.32869\n",
      "0.329651\n",
      "0.384617\n",
      "0.365325\n",
      "0.430617\n",
      "0.481404\n",
      "0.671905\n",
      "0.704687\n",
      "0.314856\n",
      "0.421143\n",
      "0.500306\n",
      "0.359171\n",
      "0.3091\n",
      "0.738316\n",
      "0.631814\n",
      "1.82955\n",
      "0.545778\n",
      "0.328622\n",
      "0.612181\n",
      "0.72731\n",
      "0.746145\n",
      "0.638112\n",
      "0.856492\n",
      "0.608675\n",
      "0.553273\n",
      "0.483343\n",
      "1.01377\n",
      "0.917778\n",
      "0.479064\n",
      "0.719378\n",
      "0.86575\n",
      "0.545843\n",
      "1.34642\n",
      "1.35243\n",
      "1.628\n",
      "1.04056\n",
      "0.545884\n",
      "1.8408\n",
      "0.759639\n",
      "0.396456\n",
      "0.358461\n",
      "0.320523\n",
      "0.347123\n",
      "0.762643\n",
      "0.312469\n",
      "0.297658\n",
      "0.32427\n",
      "0.425969\n",
      "0.324803\n",
      "0.841764\n",
      "0.549984\n",
      "0.433772\n",
      "2.02819\n",
      "0.599496\n",
      "0.875508\n",
      "0.644135\n",
      "0.505308\n",
      "0.512233\n",
      "0.805122\n",
      "1.13694\n",
      "0.57854\n",
      "0.392189\n",
      "0.335763\n",
      "1.06702\n",
      "1.25792\n",
      "0.460604\n",
      "0.735146\n",
      "0.695398\n",
      "0.851278\n",
      "1.23844\n",
      "1.25063\n",
      "0.879991\n",
      "0.983542\n",
      "0.364142\n",
      "0.748227\n",
      "0.602164\n",
      "0.305819\n",
      "0.328699\n",
      "0.253661\n",
      "0.376323\n",
      "0.77428\n",
      "0.482568\n",
      "0.516371\n",
      "0.378313\n",
      "0.685084\n",
      "1.04028\n",
      "0.827922\n",
      "0.399979\n",
      "0.791967\n",
      "0.529733\n",
      "0.513815\n",
      "0.562639\n",
      "0.223009\n",
      "0.573204\n",
      "0.397202\n",
      "0.428766\n",
      "0.611026\n",
      "0.413388\n",
      "1.17815\n",
      "1.50801\n",
      "0.927249\n",
      "0.452994\n",
      "0.275703\n",
      "0.294289\n",
      "1.00463\n",
      "0.500088\n",
      "0.418084\n",
      "0.312597\n",
      "0.280404\n",
      "0.593856\n",
      "0.291385\n",
      "0.430737\n",
      "6.68376\n",
      "6.36626\n",
      "2.50072\n",
      "1.81272\n",
      "1.68418\n",
      "1.17269\n",
      "0.787584\n",
      "0.784448\n",
      "0.76538\n",
      "0.569802\n",
      "1.01156\n",
      "0.951672\n",
      "0.631507\n",
      "0.288321\n",
      "0.313274\n",
      "0.457826\n",
      "0.380099\n",
      "0.390454\n",
      "0.486062\n",
      "0.586387\n",
      "0.69586\n",
      "0.290847\n",
      "0.324053\n",
      "0.39593\n",
      "0.164414\n",
      "0.384807\n",
      "0.896794\n",
      "0.27268\n",
      "0.524486\n",
      "0.304863\n",
      "0.367539\n",
      "0.517269\n",
      "0.645459\n",
      "0.495656\n",
      "0.428917\n",
      "0.370395\n",
      "0.306945\n",
      "0.710144\n",
      "0.763247\n",
      "0.471594\n",
      "0.27971\n",
      "0.563333\n",
      "0.858366\n",
      "0.962024\n",
      "1.40001\n",
      "1.16774\n",
      "1.73694\n",
      "1.89861\n",
      "2.0362\n",
      "1.60212\n",
      "0.874511\n",
      "1.00629\n",
      "0.763103\n",
      "0.343878\n",
      "1.64578\n",
      "1.36083\n",
      "1.06203\n",
      "1.11189\n",
      "0.427662\n",
      "0.510081\n",
      "0.205879\n",
      "0.211416\n",
      "0.230775\n",
      "0.375819\n",
      "0.708192\n",
      "0.499124\n",
      "0.216318\n",
      "0.314992\n",
      "1.22985\n",
      "0.316982\n",
      "9.01953\n",
      "1.87981\n",
      "0.892078\n",
      "1.08204\n",
      "0.917722\n",
      "0.506033\n",
      "0.536286\n",
      "1.03471\n",
      "0.871189\n",
      "0.836592\n",
      "0.27275\n",
      "0.478564\n",
      "0.955068\n",
      "0.430581\n",
      "0.901046\n",
      "0.700488\n",
      "0.516182\n",
      "0.982021\n",
      "1.19426\n",
      "0.523557\n",
      "0.261363\n",
      "0.297642\n",
      "0.460571\n",
      "0.360061\n",
      "0.38649\n",
      "0.424968\n",
      "0.549916\n",
      "0.746096\n",
      "0.306233\n",
      "0.64298\n",
      "0.469868\n",
      "0.522497\n",
      "0.417346\n",
      "0.310455\n",
      "0.308671\n",
      "0.613137\n",
      "0.682743\n",
      "0.662837\n",
      "0.407688\n",
      "0.332648\n",
      "0.726826\n",
      "1.13774\n",
      "0.710664\n",
      "0.803552\n",
      "0.37305\n",
      "0.889888\n",
      "0.62481\n",
      "0.32539\n",
      "0.258158\n",
      "1.08556\n",
      "1.12472\n",
      "0.6507\n",
      "0.482069\n",
      "0.512895\n",
      "0.697403\n",
      "0.282999\n",
      "0.256492\n",
      "0.35358\n",
      "0.479618\n",
      "0.491904\n",
      "0.284168\n",
      "0.517597\n",
      "0.299519\n",
      "0.312451\n",
      "0.318843\n",
      "0.870123\n",
      "0.884568\n",
      "0.394085\n",
      "0.540424\n",
      "0.86924\n",
      "0.525687\n",
      "0.603955\n",
      "0.554923\n",
      "0.30035\n",
      "0.464021\n",
      "0.250709\n",
      "0.939249\n",
      "0.491086\n",
      "0.295477\n",
      "0.381015\n",
      "0.454936\n",
      "0.434142\n",
      "0.286673\n",
      "4.3588\n",
      "5.51932\n",
      "2.63632\n",
      "1.36466\n",
      "1.47767\n",
      "2.33323\n",
      "1.11984\n",
      "1.24897\n",
      "0.515339\n",
      "0.324741\n",
      "0.76006\n",
      "0.901169\n",
      "0.561208\n",
      "0.509322\n",
      "0.315265\n",
      "0.733839\n",
      "0.274331\n",
      "0.329211\n",
      "0.284256\n",
      "0.253143\n",
      "0.375774\n",
      "0.625563\n",
      "0.321732\n",
      "0.402079\n",
      "0.29943\n",
      "0.367917\n",
      "0.320319\n",
      "0.41764\n",
      "0.379733\n",
      "1.18761\n",
      "0.692892\n",
      "0.32681\n",
      "0.611374\n",
      "0.807859\n",
      "0.297139\n",
      "0.677603\n",
      "0.287497\n",
      "0.413028\n",
      "0.359784\n",
      "0.59679\n",
      "0.704631\n",
      "0.742923\n",
      "0.964642\n",
      "0.855069\n",
      "1.28225\n",
      "0.700512\n",
      "0.424104\n",
      "0.436548\n",
      "0.7676\n",
      "0.668418\n",
      "0.623675\n",
      "1.34495\n",
      "0.271134\n",
      "0.262087\n",
      "0.354091\n",
      "0.298689\n",
      "0.482071\n",
      "1.81189\n",
      "2.24587\n",
      "1.76721\n",
      "1.86385\n",
      "1.10657\n",
      "0.619862\n",
      "1.05428\n",
      "1.03945\n",
      "0.830008\n",
      "0.997276\n",
      "0.414791\n",
      "0.633451\n",
      "0.644964\n",
      "0.512683\n",
      "0.415901\n",
      "0.35422\n",
      "0.780267\n",
      "0.517734\n",
      "0.448921\n",
      "0.420977\n",
      "0.620287\n",
      "0.340213\n",
      "0.349398\n",
      "0.723899\n",
      "0.897376\n",
      "0.333411\n",
      "0.419398\n",
      "0.285702\n",
      "0.314191\n",
      "0.886043\n",
      "0.835401\n",
      "0.638518\n",
      "0.361342\n",
      "0.570647\n",
      "0.413504\n",
      "0.614183\n",
      "0.604512\n",
      "0.453911\n",
      "0.388304\n",
      "0.465061\n",
      "0.506686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.297393\n",
      "0.39229\n",
      "0.329034\n",
      "0.412881\n",
      "0.567988\n",
      "0.222637\n",
      "0.369124\n",
      "0.278408\n",
      "0.380365\n",
      "0.619119\n",
      "0.467902\n",
      "0.391601\n",
      "0.28057\n",
      "0.307783\n",
      "0.506453\n",
      "0.286681\n",
      "0.333108\n",
      "0.355985\n",
      "0.447693\n",
      "1.27028\n",
      "0.944348\n",
      "0.787156\n",
      "0.519436\n",
      "0.526959\n",
      "0.511304\n",
      "0.316487\n",
      "0.240379\n",
      "0.262987\n",
      "0.362672\n",
      "0.489677\n",
      "0.5083\n",
      "0.268624\n",
      "0.522597\n",
      "0.339011\n",
      "0.46272\n",
      "0.445974\n",
      "0.557301\n",
      "0.90016\n",
      "1.06131\n",
      "1.22669\n",
      "0.525646\n",
      "0.960665\n",
      "1.15124\n",
      "0.59932\n",
      "0.356132\n",
      "0.517008\n",
      "0.201862\n",
      "0.360789\n",
      "0.655307\n",
      "0.527197\n",
      "0.172772\n",
      "0.459184\n",
      "0.426254\n",
      "0.353083\n",
      "0.337187\n",
      "1.28474\n",
      "0.814379\n",
      "0.504501\n",
      "0.43439\n",
      "0.422091\n",
      "0.45139\n",
      "0.63718\n",
      "0.504342\n",
      "0.235757\n",
      "0.210172\n",
      "0.271011\n",
      "0.392351\n",
      "0.520287\n",
      "0.423054\n",
      "0.532791\n",
      "0.944755\n",
      "0.435038\n",
      "0.407723\n",
      "1.83111\n",
      "1.30983\n",
      "0.771358\n",
      "0.602962\n",
      "0.505032\n",
      "0.490258\n",
      "0.442811\n",
      "1.18028\n",
      "0.332807\n",
      "0.706936\n",
      "0.268822\n",
      "0.280656\n",
      "0.51546\n",
      "0.295228\n",
      "0.28792\n",
      "0.736345\n",
      "0.462605\n",
      "0.340578\n",
      "0.20984\n",
      "0.424209\n",
      "0.980213\n",
      "0.283288\n",
      "0.240134\n",
      "0.190257\n",
      "0.627402\n",
      "0.393467\n",
      "0.7268\n",
      "0.360256\n",
      "0.522732\n",
      "0.694994\n",
      "0.773834\n",
      "1.0615\n",
      "0.798045\n",
      "0.554412\n",
      "0.314145\n",
      "0.452175\n",
      "0.518729\n",
      "0.72094\n",
      "0.379232\n",
      "0.828201\n",
      "0.221146\n",
      "0.367799\n",
      "0.368098\n",
      "0.456721\n",
      "0.370545\n",
      "0.584976\n",
      "0.425892\n",
      "0.41334\n",
      "1.03199\n",
      "0.636772\n",
      "0.327868\n",
      "0.897033\n",
      "0.423521\n",
      "0.256688\n",
      "0.526435\n",
      "0.669301\n",
      "0.682639\n",
      "0.718868\n",
      "0.913142\n",
      "1.12975\n",
      "1.07749\n",
      "0.551759\n",
      "0.411586\n",
      "0.726003\n",
      "0.510204\n",
      "0.428124\n",
      "0.328506\n",
      "0.293751\n",
      "0.53154\n",
      "0.779454\n",
      "0.909365\n",
      "0.607058\n",
      "0.534255\n",
      "0.321122\n",
      "0.72295\n",
      "0.727168\n",
      "0.568307\n",
      "2.64495\n",
      "0.757325\n",
      "0.587862\n",
      "0.456379\n",
      "0.640979\n",
      "0.633499\n",
      "0.342715\n",
      "1.25214\n",
      "0.870313\n",
      "0.687424\n",
      "0.462848\n",
      "0.529938\n",
      "0.30609\n",
      "0.310212\n",
      "0.515625\n",
      "0.529305\n",
      "0.396065\n",
      "0.253679\n",
      "0.931142\n",
      "1.45793\n",
      "1.08395\n",
      "0.408625\n",
      "0.469566\n",
      "0.358939\n",
      "0.969873\n",
      "0.515632\n",
      "0.591642\n",
      "1.26196\n",
      "0.539261\n",
      "0.194821\n",
      "0.537953\n",
      "1.49626\n",
      "0.592989\n",
      "0.589676\n",
      "0.264242\n",
      "1.62318\n",
      "2.6344\n",
      "0.898164\n",
      "0.735571\n",
      "1.34483\n",
      "1.08532\n",
      "0.958768\n",
      "0.663801\n",
      "0.549536\n",
      "2.03476\n",
      "0.240189\n",
      "0.662789\n",
      "0.716518\n",
      "0.929905\n",
      "0.566509\n",
      "0.882576\n",
      "1.49929\n",
      "1.2268\n",
      "0.730441\n",
      "0.487448\n",
      "0.273618\n",
      "0.260465\n",
      "0.229141\n",
      "1.9365\n",
      "0.859067\n",
      "0.909167\n",
      "0.67163\n",
      "1.20079\n",
      "0.5848\n",
      "0.503712\n",
      "0.60102\n",
      "0.295403\n",
      "0.222819\n",
      "0.609992\n",
      "0.434283\n",
      "0.173468\n",
      "0.596092\n",
      "0.712614\n",
      "1.1717\n",
      "0.246246\n",
      "0.243944\n",
      "0.382272\n",
      "0.60745\n",
      "0.452739\n",
      "0.569241\n",
      "0.321696\n",
      "0.410462\n",
      "0.154847\n",
      "0.448448\n",
      "0.363017\n",
      "1.26918\n",
      "0.49998\n",
      "0.714322\n",
      "0.5836\n",
      "0.464775\n",
      "0.406991\n",
      "0.558088\n",
      "0.384184\n",
      "0.316471\n",
      "0.483513\n",
      "1.14779\n",
      "1.39829\n",
      "1.48323\n",
      "1.5956\n",
      "0.840641\n",
      "0.796127\n",
      "0.604396\n",
      "0.407953\n",
      "1.47734\n",
      "0.291874\n",
      "0.553998\n",
      "0.190375\n",
      "0.226301\n",
      "1.09387\n",
      "0.495342\n",
      "0.451328\n",
      "0.468698\n",
      "0.515914\n",
      "0.327694\n",
      "0.216136\n",
      "0.608332\n",
      "2.12536\n",
      "0.90331\n",
      "0.296425\n",
      "0.382694\n",
      "0.444675\n",
      "0.423898\n",
      "0.402439\n",
      "0.518004\n",
      "0.275468\n",
      "0.780155\n",
      "1.34627\n",
      "1.18224\n",
      "1.81811\n",
      "1.20297\n",
      "1.20313\n",
      "0.694479\n",
      "0.942053\n",
      "0.596315\n",
      "0.777004\n",
      "1.05894\n",
      "0.626929\n",
      "0.18543\n",
      "0.496605\n",
      "0.749636\n",
      "0.565914\n",
      "0.631228\n",
      "0.733168\n",
      "0.340973\n",
      "0.256508\n",
      "0.552985\n",
      "0.545866\n",
      "0.357727\n",
      "0.842557\n",
      "0.368704\n",
      "0.493674\n",
      "0.425648\n",
      "0.762279\n",
      "0.529546\n",
      "0.733058\n",
      "0.310846\n",
      "1.16154\n",
      "0.616372\n",
      "0.342689\n",
      "0.359717\n",
      "0.253476\n",
      "0.561853\n",
      "4.90267\n",
      "1.37212\n",
      "0.650351\n",
      "0.6118\n",
      "0.471705\n",
      "0.405467\n",
      "0.192896\n",
      "0.287987\n",
      "0.406172\n",
      "0.697512\n",
      "0.415431\n",
      "1.03089\n",
      "0.592135\n",
      "0.492374\n",
      "1.11294\n",
      "1.19602\n",
      "1.55323\n",
      "0.580171\n",
      "0.394\n",
      "0.869209\n",
      "1.45491\n",
      "0.298175\n",
      "0.975084\n",
      "0.440452\n",
      "0.261183\n",
      "0.228408\n",
      "0.531625\n",
      "0.250052\n",
      "0.321457\n",
      "0.289432\n",
      "0.502839\n",
      "0.358815\n",
      "0.299507\n",
      "0.572109\n",
      "0.666026\n",
      "0.304214\n",
      "0.426148\n",
      "0.680654\n",
      "0.490781\n",
      "0.658037\n",
      "0.431857\n",
      "0.378838\n",
      "0.648926\n",
      "0.369917\n",
      "0.311069\n",
      "0.818896\n",
      "0.822862\n",
      "0.548641\n",
      "0.433531\n",
      "0.486125\n",
      "0.978274\n",
      "0.406639\n",
      "0.382669\n",
      "4.39841\n",
      "0.983192\n",
      "0.789426\n",
      "0.311759\n",
      "0.239912\n",
      "0.209028\n",
      "0.17822\n",
      "0.177403\n",
      "0.443916\n",
      "0.548279\n",
      "0.285084\n",
      "0.770374\n",
      "0.71473\n",
      "0.489759\n",
      "0.376244\n",
      "0.419487\n",
      "0.534169\n",
      "0.415648\n",
      "0.600175\n",
      "0.50527\n",
      "0.461367\n",
      "0.175618\n",
      "0.854608\n",
      "0.215624\n",
      "0.309478\n",
      "0.522335\n",
      "0.614902\n",
      "0.490441\n",
      "0.28502\n",
      "0.377654\n",
      "0.375675\n",
      "0.253859\n",
      "0.390902\n",
      "0.579574\n",
      "0.72605\n",
      "0.757229\n",
      "0.844819\n",
      "0.512642\n",
      "0.230317\n",
      "0.213828\n",
      "0.364667\n",
      "0.486002\n",
      "0.526407\n",
      "1.18499\n",
      "0.385577\n",
      "0.488226\n",
      "0.43815\n",
      "0.746594\n",
      "0.833289\n",
      "0.538012\n",
      "0.537999\n",
      "0.235995\n",
      "0.342533\n",
      "0.396995\n",
      "0.336968\n",
      "0.190998\n",
      "0.571923\n",
      "0.170857\n",
      "0.503487\n",
      "0.484026\n",
      "0.389888\n",
      "0.509187\n",
      "0.663789\n",
      "0.771455\n",
      "0.575047\n",
      "0.98512\n",
      "0.557526\n",
      "0.607607\n",
      "0.707298\n",
      "0.35602\n",
      "0.317722\n",
      "0.313102\n",
      "0.442936\n",
      "0.42571\n",
      "0.775915\n",
      "0.73887\n",
      "0.431675\n",
      "0.375989\n",
      "0.309746\n",
      "0.554903\n",
      "0.592399\n",
      "1.54189\n",
      "6.16648\n",
      "5.74001\n",
      "5.42472\n",
      "1.82926\n",
      "0.608615\n",
      "0.481362\n",
      "0.483519\n",
      "0.367469\n",
      "2.43805\n",
      "1.593\n",
      "1.27561\n",
      "1.2403\n",
      "0.704387\n",
      "0.581367\n",
      "0.302556\n",
      "0.823402\n",
      "0.550268\n",
      "0.372823\n",
      "0.281023\n",
      "0.395288\n",
      "0.777607\n",
      "0.611069\n",
      "0.453434\n",
      "0.375799\n",
      "0.469525\n",
      "0.70998\n",
      "0.470375\n",
      "0.411931\n",
      "0.254796\n",
      "0.756789\n",
      "0.166036\n",
      "1.02415\n",
      "0.286846\n",
      "0.31445\n",
      "0.36333\n",
      "0.538165\n",
      "0.847576\n",
      "0.644459\n",
      "0.471467\n",
      "0.176422\n",
      "0.697276\n",
      "0.328933\n",
      "0.634014\n",
      "0.24171\n",
      "0.307359\n",
      "0.194217\n",
      "0.325921\n",
      "0.374713\n",
      "0.48265\n",
      "0.358419\n",
      "0.436058\n",
      "0.496358\n",
      "0.412204\n",
      "0.509978\n",
      "0.64076\n",
      "0.522547\n",
      "0.378747\n",
      "0.227592\n",
      "0.796828\n",
      "1.64427\n",
      "1.09309\n",
      "0.504765\n",
      "0.621291\n",
      "0.464152\n",
      "0.471029\n",
      "0.187819\n",
      "0.37603\n",
      "0.573837\n",
      "0.303075\n",
      "0.68315\n",
      "0.850949\n",
      "0.761842\n",
      "1.68827\n",
      "0.28058\n",
      "0.48467\n",
      "0.784167\n",
      "0.537649\n",
      "0.612047\n",
      "0.505128\n",
      "0.56307\n",
      "0.651805\n",
      "0.764912\n",
      "0.754536\n",
      "0.535075\n",
      "0.439981\n",
      "0.460244\n",
      "0.366663\n",
      "0.680869\n",
      "0.447685\n",
      "0.335858\n",
      "0.695987\n",
      "2.22592\n",
      "1.39505\n",
      "1.63423\n",
      "1.16967\n",
      "1.02202\n",
      "0.606297\n",
      "0.394353\n",
      "1.01137\n",
      "0.441348\n",
      "0.612344\n",
      "0.636678\n",
      "0.582268\n",
      "0.755706\n",
      "0.800736\n",
      "0.705441\n",
      "0.838175\n",
      "0.385409\n",
      "0.722975\n",
      "0.177944\n",
      "0.231877\n",
      "0.460878\n",
      "0.28821\n",
      "0.6418\n",
      "0.436596\n",
      "0.265987\n",
      "0.200189\n",
      "0.264334\n",
      "0.645479\n",
      "0.845182\n",
      "0.532069\n",
      "0.491119\n",
      "0.666916\n",
      "0.192203\n",
      "0.340685\n",
      "0.223695\n",
      "0.175279\n",
      "0.500835\n",
      "0.354399\n",
      "0.491716\n",
      "0.339895\n",
      "0.279804\n",
      "0.327244\n",
      "0.293981\n",
      "0.397577\n",
      "0.876693\n",
      "0.647533\n",
      "0.248137\n",
      "0.280635\n",
      "0.811032\n",
      "1.08798\n",
      "0.422017\n",
      "0.501177\n",
      "0.258301\n",
      "0.434132\n",
      "0.264104\n",
      "0.580032\n",
      "0.35476\n",
      "0.291728\n",
      "0.240941\n",
      "0.223919\n",
      "0.466937\n",
      "0.355351\n",
      "0.283501\n",
      "0.416716\n",
      "0.618379\n",
      "0.315182\n",
      "0.387266\n",
      "0.203912\n",
      "0.376976\n",
      "0.345172\n",
      "0.313537\n",
      "0.343388\n",
      "0.465676\n",
      "0.464443\n",
      "0.268023\n",
      "0.221643\n",
      "0.536033\n",
      "0.24461\n",
      "0.508009\n",
      "0.728868\n",
      "0.889681\n",
      "0.68943\n",
      "0.308686\n",
      "0.192938\n",
      "0.533864\n",
      "0.559482\n",
      "0.902215\n",
      "0.404918\n",
      "0.238961\n",
      "0.28476\n",
      "0.287872\n",
      "0.315003\n",
      "0.13581\n",
      "0.384674\n",
      "0.41117\n",
      "0.499237\n",
      "0.288391\n",
      "0.376354\n",
      "0.23158\n",
      "0.173161\n",
      "0.502544\n",
      "0.300129\n",
      "0.187867\n",
      "0.288936\n",
      "0.395006\n",
      "0.345017\n",
      "0.58595\n",
      "0.49883\n",
      "0.314959\n",
      "0.678132\n",
      "0.665967\n",
      "0.630571\n",
      "0.316252\n",
      "0.337613\n",
      "1.47516\n",
      "0.524729\n",
      "0.252389\n",
      "1.44744\n",
      "1.32397\n",
      "1.56898\n",
      "1.14526\n",
      "0.876698\n",
      "0.768578\n",
      "0.516298\n",
      "0.364161\n",
      "0.300071\n",
      "1.91276\n",
      "1.09999\n",
      "1.36812\n",
      "1.01299\n",
      "0.413693\n",
      "0.45936\n",
      "0.248593\n",
      "0.885808\n",
      "0.540266\n",
      "0.427142\n",
      "0.236481\n",
      "0.764946\n",
      "0.977342\n",
      "0.35931\n",
      "1.09755\n",
      "0.513285\n",
      "0.440637\n",
      "0.248921\n",
      "0.280624\n",
      "0.46861\n",
      "0.501697\n",
      "0.272236\n",
      "0.209722\n",
      "0.342143\n",
      "0.336022\n",
      "0.56294\n",
      "0.802569\n",
      "0.102185\n",
      "0.578231\n",
      "0.210563\n",
      "0.509801\n",
      "0.237194\n",
      "0.223044\n",
      "0.412961\n",
      "0.367743\n",
      "0.510766\n",
      "0.263189\n",
      "0.392655\n",
      "0.24042\n",
      "0.490723\n",
      "0.406904\n",
      "0.580634\n",
      "0.293216\n",
      "0.264237\n",
      "1.30478\n",
      "1.08895\n",
      "0.439255\n",
      "0.395342\n",
      "0.261795\n",
      "0.275372\n",
      "0.223131\n",
      "0.455525\n",
      "0.227004\n",
      "0.323741\n",
      "0.475677\n",
      "0.440956\n",
      "0.400844\n",
      "0.505249\n",
      "0.0867919\n",
      "0.315067\n",
      "0.380691\n",
      "0.377613\n",
      "0.556484\n",
      "0.494179\n",
      "0.374083\n",
      "0.50628\n",
      "0.4041\n",
      "0.155035\n",
      "0.442705\n",
      "1.22439\n",
      "1.06147\n",
      "0.515949\n",
      "0.191977\n",
      "0.419027\n",
      "0.335085\n",
      "0.63442\n",
      "0.574582\n",
      "1.06819\n",
      "0.558686\n",
      "0.1436\n",
      "0.543961\n",
      "1.39091\n",
      "1.87118\n",
      "0.727738\n",
      "0.468585\n",
      "0.886627\n",
      "0.152262\n",
      "0.344364\n",
      "0.654418\n",
      "0.497713\n",
      "0.354249\n",
      "0.414566\n",
      "0.310163\n",
      "0.435642\n",
      "0.354458\n",
      "1.37859\n",
      "0.373169\n",
      "0.481121\n",
      "0.528785\n",
      "0.231446\n",
      "0.278414\n",
      "0.432636\n",
      "0.898023\n",
      "0.543299\n",
      "1.00885\n",
      "1.00315\n",
      "0.9822\n",
      "1.19235\n",
      "1.13841\n",
      "0.803597\n",
      "0.487355\n",
      "0.285644\n",
      "0.333686\n",
      "0.329344\n",
      "0.371925\n",
      "0.415919\n",
      "0.637935\n",
      "0.284734\n",
      "0.406015\n",
      "0.233719\n",
      "0.409684\n",
      "0.744545\n",
      "0.642781\n",
      "0.676907\n",
      "0.785282\n",
      "0.525283\n",
      "0.435372\n",
      "0.431226\n",
      "0.497828\n",
      "1.54295\n",
      "0.755208\n",
      "0.583981\n",
      "0.187675\n",
      "0.361475\n",
      "1.20678\n",
      "0.980695\n",
      "0.736126\n",
      "0.615702\n",
      "0.853732\n",
      "0.681525\n",
      "0.425403\n",
      "0.297915\n",
      "0.347244\n",
      "0.354117\n",
      "0.866683\n",
      "1.11241\n",
      "0.792322\n",
      "0.841292\n",
      "0.605476\n",
      "0.304186\n",
      "0.853418\n",
      "0.313585\n",
      "0.728601\n",
      "0.358558\n",
      "0.317725\n",
      "0.496828\n",
      "0.521775\n",
      "0.495397\n",
      "0.309103\n",
      "0.560173\n",
      "0.368358\n",
      "0.199625\n",
      "0.585994\n",
      "1.15068\n",
      "0.988055\n",
      "0.946794\n",
      "0.501127\n",
      "0.318119\n",
      "0.535953\n",
      "0.571785\n",
      "0.523929\n",
      "0.274696\n",
      "0.448391\n",
      "0.327664\n",
      "0.435721\n",
      "0.53524\n",
      "0.412313\n",
      "0.236061\n",
      "0.217147\n",
      "0.481934\n",
      "0.516389\n",
      "0.275816\n",
      "0.208493\n",
      "0.4072\n",
      "0.172486\n",
      "0.207068\n",
      "0.160393\n",
      "0.545998\n",
      "0.580731\n",
      "0.199537\n",
      "0.903195\n",
      "0.625371\n",
      "0.236862\n",
      "0.25806\n",
      "0.210366\n",
      "0.4617\n",
      "0.335777\n",
      "1.02173\n",
      "0.472361\n",
      "0.404703\n",
      "0.200501\n",
      "0.317545\n",
      "0.280665\n",
      "0.574131\n",
      "0.379372\n",
      "0.386587\n",
      "0.489094\n",
      "0.54818\n",
      "0.544138\n",
      "1.50429\n",
      "0.616443\n",
      "0.509568\n",
      "0.189822\n",
      "0.714969\n",
      "0.599431\n",
      "0.276853\n",
      "0.311951\n",
      "0.379731\n",
      "0.318247\n",
      "0.8972\n",
      "0.446157\n",
      "0.362823\n",
      "0.614779\n",
      "0.851055\n",
      "0.337358\n",
      "0.204195\n",
      "0.290118\n",
      "1.38871\n",
      "0.828278\n",
      "0.657055\n",
      "0.485693\n",
      "0.430592\n",
      "0.601557\n",
      "0.299848\n",
      "0.367552\n",
      "0.242297\n",
      "0.295865\n",
      "0.413961\n",
      "0.453558\n",
      "0.515183\n",
      "0.313493\n",
      "0.751497\n",
      "0.809392\n",
      "0.565672\n",
      "0.433713\n",
      "0.476503\n",
      "0.268616\n",
      "0.80651\n",
      "0.681834\n",
      "0.447924\n",
      "0.399324\n",
      "0.392939\n",
      "0.322796\n",
      "0.221693\n",
      "0.552981\n",
      "0.281933\n",
      "0.525667\n",
      "0.458862\n",
      "0.461535\n",
      "0.603932\n",
      "0.598459\n",
      "0.332488\n",
      "0.402638\n",
      "0.332283\n",
      "0.310065\n",
      "0.344658\n",
      "0.313876\n",
      "0.326683\n",
      "0.804461\n",
      "1.07814\n",
      "1.30808\n",
      "1.00265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.233328\n",
      "0.816454\n",
      "0.706176\n",
      "0.768293\n",
      "1.13289\n",
      "0.790274\n",
      "0.287567\n",
      "0.659111\n",
      "0.500049\n",
      "0.730333\n",
      "0.269146\n",
      "0.18068\n",
      "0.312641\n",
      "0.834858\n",
      "0.520179\n",
      "0.356684\n",
      "0.699944\n",
      "0.427152\n",
      "0.578119\n",
      "0.528642\n",
      "0.239698\n",
      "0.331184\n",
      "0.265312\n",
      "0.301385\n",
      "0.334427\n",
      "0.403555\n",
      "0.709917\n",
      "0.714905\n",
      "0.975522\n",
      "1.02257\n",
      "0.792921\n",
      "0.680471\n",
      "0.648575\n",
      "0.38818\n",
      "0.522839\n",
      "0.253289\n",
      "0.42296\n",
      "0.405014\n",
      "0.305774\n",
      "0.30579\n",
      "0.484866\n",
      "0.290329\n",
      "0.290756\n",
      "0.1354\n",
      "0.506965\n",
      "0.803101\n",
      "1.57824\n",
      "1.20361\n",
      "0.7485\n",
      "0.379935\n",
      "0.325337\n",
      "0.219224\n",
      "1.04048\n",
      "0.46535\n",
      "0.241411\n",
      "0.403004\n",
      "0.449614\n",
      "0.520923\n",
      "0.508811\n",
      "0.243721\n",
      "0.545151\n",
      "0.366315\n",
      "0.48951\n",
      "0.713328\n",
      "0.344235\n",
      "0.420305\n",
      "0.427736\n",
      "0.241741\n",
      "0.398851\n",
      "0.428247\n",
      "0.527344\n",
      "0.31093\n",
      "0.456865\n",
      "0.224366\n",
      "0.711034\n",
      "0.183169\n",
      "0.292268\n",
      "0.446951\n",
      "0.330775\n",
      "0.284731\n",
      "0.200216\n",
      "0.255582\n",
      "0.223278\n",
      "0.190938\n",
      "0.327155\n",
      "0.812855\n",
      "0.324472\n",
      "0.420475\n",
      "0.559847\n",
      "0.144166\n",
      "0.286329\n",
      "0.280488\n",
      "0.311397\n",
      "0.325787\n",
      "0.431449\n",
      "0.964473\n",
      "0.742091\n",
      "0.525837\n",
      "0.41958\n",
      "1.2656\n",
      "0.663632\n",
      "0.637037\n",
      "0.580734\n",
      "0.329795\n",
      "0.340455\n",
      "1.06588\n",
      "0.256925\n",
      "0.236835\n",
      "0.409175\n",
      "0.471532\n",
      "0.921827\n",
      "0.348454\n",
      "0.602125\n",
      "0.658391\n",
      "0.644044\n",
      "0.771312\n",
      "0.588692\n",
      "0.433054\n",
      "0.327499\n",
      "1.11063\n",
      "0.471649\n",
      "0.406387\n",
      "0.486867\n",
      "0.431545\n",
      "0.617831\n",
      "0.409634\n",
      "0.399456\n",
      "0.339866\n",
      "0.398765\n",
      "0.3025\n",
      "0.374766\n",
      "0.416431\n",
      "0.452631\n",
      "0.663451\n",
      "0.948343\n",
      "0.700679\n",
      "0.544716\n",
      "0.474783\n",
      "0.337933\n",
      "0.265573\n",
      "0.324858\n",
      "0.363907\n",
      "0.392487\n",
      "0.411955\n",
      "0.248429\n",
      "0.411597\n",
      "0.438751\n",
      "0.376816\n",
      "0.469101\n",
      "1.77595\n",
      "1.48814\n",
      "1.07309\n",
      "1.88205\n",
      "2.625\n",
      "1.29854\n",
      "1.07998\n",
      "1.00317\n",
      "0.789599\n",
      "1.19778\n",
      "0.679801\n",
      "0.392214\n",
      "0.620545\n",
      "0.728263\n",
      "0.291701\n",
      "0.449057\n",
      "0.277304\n",
      "0.348403\n",
      "0.669719\n",
      "0.683833\n",
      "0.949651\n",
      "0.816274\n",
      "0.313131\n",
      "0.410635\n",
      "0.639066\n",
      "0.181732\n",
      "0.231365\n",
      "0.406764\n",
      "0.270887\n",
      "0.43814\n",
      "0.619965\n",
      "0.279568\n",
      "0.199199\n",
      "0.279893\n",
      "0.459678\n",
      "0.5652\n",
      "0.457553\n",
      "0.637306\n",
      "0.796696\n",
      "0.491871\n",
      "0.377625\n",
      "0.684803\n",
      "0.882425\n",
      "0.795271\n",
      "0.831434\n",
      "4.2846\n",
      "6.45938\n",
      "3.39568\n",
      "2.41791\n",
      "1.07458\n",
      "0.47985\n",
      "0.318357\n",
      "0.523462\n",
      "0.393862\n",
      "0.60873\n",
      "0.264302\n",
      "0.762246\n",
      "0.459179\n",
      "0.549336\n",
      "0.645515\n",
      "0.376084\n",
      "0.411489\n",
      "0.397994\n",
      "0.285502\n",
      "0.504192\n",
      "0.633268\n",
      "0.401137\n",
      "0.580265\n",
      "0.372915\n",
      "0.373303\n",
      "0.416995\n",
      "0.543249\n",
      "0.515368\n",
      "0.205263\n",
      "0.257619\n",
      "0.575793\n",
      "0.406156\n",
      "0.273117\n",
      "0.528616\n",
      "0.307642\n",
      "0.372894\n",
      "0.313095\n",
      "0.395842\n",
      "0.2148\n",
      "0.340753\n",
      "0.263853\n",
      "0.375381\n",
      "0.412124\n",
      "1.04198\n",
      "0.294006\n",
      "0.29625\n",
      "0.23347\n",
      "0.313706\n",
      "0.263484\n",
      "0.218061\n",
      "0.381727\n",
      "0.655316\n",
      "0.602644\n",
      "0.490966\n",
      "0.634327\n",
      "0.316217\n",
      "0.484894\n",
      "0.325691\n",
      "0.287035\n",
      "2.0163\n",
      "0.683331\n",
      "0.54727\n",
      "0.400852\n",
      "0.756357\n",
      "1.28995\n",
      "1.11626\n",
      "0.842587\n",
      "0.456569\n",
      "0.305395\n",
      "0.335156\n",
      "0.382019\n",
      "0.331063\n",
      "0.501482\n",
      "0.593922\n",
      "0.411279\n",
      "0.568861\n",
      "0.507885\n",
      "0.678042\n",
      "0.175737\n",
      "1.06145\n",
      "0.277428\n",
      "0.267524\n",
      "0.480245\n",
      "0.807471\n",
      "0.361314\n",
      "0.240769\n",
      "0.469175\n",
      "10.2883\n",
      "2.98603\n",
      "1.36176\n",
      "0.562131\n",
      "0.400087\n",
      "0.467645\n",
      "0.378006\n",
      "0.351407\n",
      "0.541916\n",
      "0.473978\n",
      "1.80519\n",
      "0.375804\n",
      "0.297231\n",
      "0.565362\n",
      "0.365929\n",
      "0.326772\n",
      "0.617234\n",
      "0.499407\n",
      "0.211219\n",
      "0.384089\n",
      "0.294091\n",
      "0.671121\n",
      "0.394051\n",
      "0.423734\n",
      "0.346081\n",
      "0.201486\n",
      "0.281636\n",
      "0.320975\n",
      "0.450452\n",
      "0.951228\n",
      "0.838614\n",
      "0.978313\n",
      "0.588995\n",
      "0.268993\n",
      "0.295496\n",
      "0.300985\n",
      "0.257234\n",
      "0.555743\n",
      "0.372215\n",
      "0.371384\n",
      "0.212267\n",
      "0.725411\n",
      "0.632323\n",
      "0.377015\n",
      "0.331432\n",
      "0.688619\n",
      "0.687275\n",
      "0.611284\n",
      "0.263127\n",
      "0.26764\n",
      "0.531101\n",
      "0.331171\n",
      "0.855358\n",
      "0.525113\n",
      "0.193673\n",
      "0.190735\n",
      "0.708045\n",
      "0.538374\n",
      "0.377316\n",
      "0.155415\n",
      "0.264138\n",
      "0.691717\n",
      "0.260322\n",
      "1.16325\n",
      "0.767968\n",
      "2.1634\n",
      "2.44071\n",
      "2.872\n",
      "3.04613\n",
      "0.890413\n",
      "0.367921\n",
      "0.32156\n",
      "0.384005\n",
      "0.45409\n",
      "0.300236\n",
      "0.985729\n",
      "0.577479\n",
      "0.37776\n",
      "0.383867\n",
      "0.356054\n",
      "0.503566\n",
      "0.622911\n",
      "0.332902\n",
      "0.214184\n",
      "0.489295\n",
      "1.76827\n",
      "1.562\n",
      "0.804227\n",
      "0.913128\n",
      "0.799929\n",
      "0.451168\n",
      "1.24033\n",
      "0.901012\n",
      "1.26947\n",
      "0.880352\n",
      "0.435212\n",
      "0.844942\n",
      "1.17592\n",
      "1.45731\n",
      "1.08175\n",
      "1.01643\n",
      "0.787425\n",
      "0.439459\n",
      "0.59671\n",
      "0.514614\n",
      "0.617983\n",
      "0.696454\n",
      "0.402335\n",
      "0.39496\n",
      "0.489746\n",
      "0.47338\n",
      "0.543699\n",
      "0.564331\n",
      "0.270845\n",
      "0.432993\n",
      "0.291263\n",
      "0.304919\n",
      "0.327991\n",
      "0.284847\n",
      "0.322586\n",
      "0.273851\n",
      "0.261123\n",
      "0.361333\n",
      "0.481708\n",
      "0.525727\n",
      "0.457064\n",
      "0.537229\n",
      "0.240113\n",
      "0.628877\n",
      "0.50205\n",
      "0.703903\n",
      "1.50069\n",
      "2.40511\n",
      "2.59868\n",
      "1.63041\n",
      "0.670219\n",
      "0.289001\n",
      "0.337923\n",
      "0.358264\n",
      "0.357392\n",
      "0.485087\n",
      "0.517797\n",
      "0.449501\n",
      "0.549572\n",
      "0.54876\n",
      "0.674242\n",
      "0.253621\n",
      "0.615744\n",
      "2.13888\n",
      "1.35252\n",
      "0.80119\n",
      "0.880061\n",
      "0.192817\n",
      "0.346033\n",
      "0.719298\n",
      "0.428311\n",
      "0.41294\n",
      "0.517585\n",
      "2.17812\n",
      "1.14646\n",
      "0.72024\n",
      "0.469158\n",
      "0.657111\n",
      "0.388189\n",
      "0.484272\n",
      "0.566786\n",
      "0.413544\n",
      "0.675745\n",
      "0.292092\n",
      "0.27781\n",
      "0.37182\n",
      "0.453682\n",
      "0.424689\n",
      "0.627539\n",
      "0.542686\n",
      "0.266163\n",
      "0.188162\n",
      "0.271986\n",
      "0.326648\n",
      "0.352931\n",
      "0.41728\n",
      "0.5053\n",
      "3.84095\n",
      "6.13084\n",
      "2.79233\n",
      "1.63995\n",
      "0.693192\n",
      "0.325338\n",
      "0.618942\n",
      "0.221887\n",
      "0.477132\n",
      "1.08007\n",
      "0.469189\n",
      "0.227122\n",
      "0.163546\n",
      "0.336945\n",
      "0.72978\n",
      "0.376728\n",
      "0.518255\n",
      "0.474765\n",
      "0.373281\n",
      "0.333719\n",
      "0.322856\n",
      "0.586726\n",
      "0.480461\n",
      "0.312759\n",
      "1.16579\n",
      "0.933883\n",
      "0.486013\n",
      "0.451034\n",
      "0.179683\n",
      "0.379709\n",
      "0.515673\n",
      "0.751718\n",
      "0.404216\n",
      "0.515304\n",
      "0.233237\n",
      "0.576523\n",
      "0.376712\n",
      "0.652339\n",
      "0.627707\n",
      "1.77885\n",
      "0.304283\n",
      "1.02787\n",
      "0.964611\n",
      "0.404458\n",
      "0.415043\n",
      "0.275565\n",
      "2.08747\n",
      "0.852387\n",
      "0.917428\n",
      "0.520182\n",
      "0.397518\n",
      "0.780242\n",
      "0.395407\n",
      "0.38854\n",
      "0.309554\n",
      "0.344663\n",
      "0.423833\n",
      "0.387435\n",
      "0.351303\n",
      "0.858391\n",
      "0.75165\n",
      "0.608892\n",
      "0.402148\n",
      "0.375219\n",
      "0.320002\n",
      "0.304576\n",
      "0.714601\n",
      "0.549037\n",
      "0.297348\n",
      "0.350697\n",
      "0.4516\n",
      "0.302272\n",
      "0.344293\n",
      "0.370245\n",
      "0.581968\n",
      "0.535256\n",
      "0.717292\n",
      "0.474926\n",
      "0.322819\n",
      "0.330904\n",
      "0.328742\n",
      "0.640074\n",
      "0.467325\n",
      "0.237654\n",
      "0.503892\n",
      "0.381966\n",
      "0.375379\n",
      "0.327534\n",
      "0.406768\n",
      "0.43452\n",
      "1.04424\n",
      "0.284825\n",
      "0.224438\n",
      "0.366089\n",
      "0.34392\n",
      "0.366697\n",
      "0.410449\n",
      "0.485627\n",
      "2.329\n",
      "1.72341\n",
      "1.0653\n",
      "0.719229\n",
      "0.677236\n",
      "0.635514\n",
      "0.492057\n",
      "0.35576\n",
      "1.01675\n",
      "0.472071\n",
      "0.299586\n",
      "1.0039\n",
      "0.428628\n",
      "0.169624\n",
      "0.435446\n",
      "0.525923\n",
      "0.50546\n",
      "0.328901\n",
      "0.242223\n",
      "0.488586\n",
      "0.941127\n",
      "1.10946\n",
      "1.66351\n",
      "3.12367\n",
      "2.44751\n",
      "3.74031\n",
      "2.28012\n",
      "0.816954\n",
      "0.889754\n",
      "0.407684\n",
      "0.141965\n",
      "0.44176\n",
      "0.188498\n",
      "0.302088\n",
      "0.697971\n",
      "0.694906\n",
      "0.741174\n",
      "1.43501\n",
      "1.27773\n",
      "0.456149\n",
      "0.771221\n",
      "0.359857\n",
      "0.551274\n",
      "1.09053\n",
      "1.02278\n",
      "0.775725\n",
      "0.497414\n",
      "0.320815\n",
      "0.254137\n",
      "0.358515\n",
      "0.26328\n",
      "0.255592\n",
      "0.127852\n",
      "0.224232\n",
      "0.148109\n",
      "0.145363\n",
      "0.215491\n",
      "0.257575\n",
      "0.439472\n",
      "0.399064\n",
      "0.409277\n",
      "0.406528\n",
      "0.273396\n",
      "0.256774\n",
      "0.19524\n",
      "0.166734\n",
      "0.411802\n",
      "0.245369\n",
      "0.270935\n",
      "0.264269\n",
      "0.51004\n",
      "0.18823\n",
      "0.194791\n",
      "0.363975\n",
      "0.230249\n",
      "0.450528\n",
      "0.380243\n",
      "0.505554\n",
      "0.314893\n",
      "0.404833\n",
      "0.392678\n",
      "0.109977\n",
      "0.341637\n",
      "0.19249\n",
      "0.27441\n",
      "4.05187\n",
      "7.2162\n",
      "2.55571\n",
      "1.95605\n",
      "0.853313\n",
      "0.47513\n",
      "0.295155\n",
      "0.277569\n",
      "0.384992\n",
      "0.645931\n",
      "0.55788\n",
      "0.59685\n",
      "0.37782\n",
      "0.475511\n",
      "0.438943\n",
      "0.773197\n",
      "0.703737\n",
      "0.239045\n",
      "0.283053\n",
      "0.82251\n",
      "0.339727\n",
      "0.296978\n",
      "0.2805\n",
      "0.644337\n",
      "0.261022\n",
      "0.297878\n",
      "0.150266\n",
      "0.263422\n",
      "0.229122\n",
      "0.19864\n",
      "0.456793\n",
      "0.256935\n",
      "0.222148\n",
      "0.1943\n",
      "0.747123\n",
      "0.407624\n",
      "0.244733\n",
      "0.482654\n",
      "0.289715\n",
      "0.665928\n",
      "0.518388\n",
      "0.469499\n",
      "0.323775\n",
      "0.516792\n",
      "0.711007\n",
      "0.219698\n",
      "0.393587\n",
      "0.408533\n",
      "0.241666\n",
      "0.301305\n",
      "0.576029\n",
      "0.191044\n",
      "0.348863\n",
      "0.247114\n",
      "2.10886\n",
      "1.1134\n",
      "1.3152\n",
      "0.527328\n",
      "0.302903\n",
      "0.350355\n",
      "0.415608\n",
      "0.428501\n",
      "0.34508\n",
      "0.289738\n",
      "0.456444\n",
      "0.237422\n",
      "0.373427\n",
      "0.324872\n",
      "0.315436\n",
      "0.349097\n",
      "0.460807\n",
      "0.266062\n",
      "0.470603\n",
      "0.286628\n",
      "0.194094\n",
      "0.373347\n",
      "0.359949\n",
      "0.298094\n",
      "0.596546\n",
      "0.710064\n",
      "0.563494\n",
      "0.816007\n",
      "0.573182\n",
      "0.332836\n",
      "0.333357\n",
      "0.20995\n",
      "1.03569\n",
      "0.441477\n",
      "0.277246\n",
      "0.41748\n",
      "0.186798\n",
      "0.247027\n",
      "0.305257\n",
      "0.676586\n",
      "0.179486\n",
      "0.342342\n",
      "0.33197\n",
      "0.674272\n",
      "0.316723\n",
      "0.235879\n",
      "0.329839\n",
      "0.707025\n",
      "0.199692\n",
      "0.458955\n",
      "0.228281\n",
      "0.417401\n",
      "0.34552\n",
      "0.337243\n",
      "0.558144\n",
      "0.696027\n",
      "0.301677\n",
      "1.35522\n",
      "0.287941\n",
      "0.182777\n",
      "0.38793\n",
      "0.430492\n",
      "0.289786\n",
      "0.205343\n",
      "0.455622\n",
      "0.237518\n",
      "0.492661\n",
      "0.614159\n",
      "0.401004\n",
      "0.222184\n",
      "0.740555\n",
      "1.75587\n",
      "0.539581\n",
      "0.407993\n",
      "0.358882\n",
      "0.4034\n",
      "0.291224\n",
      "0.26889\n",
      "0.182977\n",
      "0.385797\n",
      "0.231196\n",
      "0.39168\n",
      "0.233303\n",
      "0.358657\n",
      "0.193056\n",
      "0.958532\n",
      "0.818415\n",
      "0.577714\n",
      "0.441844\n",
      "0.621895\n",
      "0.601738\n",
      "1.51263\n",
      "1.32408\n",
      "0.443745\n",
      "0.22718\n",
      "0.296218\n",
      "0.516783\n",
      "0.250719\n",
      "0.717624\n",
      "0.185528\n",
      "0.355501\n",
      "0.359396\n",
      "0.590202\n",
      "0.4303\n",
      "0.565716\n",
      "0.347781\n",
      "0.349439\n",
      "0.262083\n",
      "0.362747\n",
      "0.518147\n",
      "0.914781\n",
      "0.591992\n",
      "0.866763\n",
      "0.537391\n",
      "0.272513\n",
      "0.287369\n",
      "0.246258\n",
      "0.287588\n",
      "0.566551\n",
      "0.234385\n",
      "0.224721\n",
      "0.24022\n",
      "0.139396\n",
      "0.711423\n",
      "0.302874\n",
      "0.392003\n",
      "0.259777\n",
      "0.218477\n",
      "0.413236\n",
      "0.339969\n",
      "0.478073\n",
      "0.287611\n",
      "0.333502\n",
      "0.796015\n",
      "0.406509\n",
      "0.707434\n",
      "1.34063\n",
      "0.935567\n",
      "0.566384\n",
      "0.30007\n",
      "0.633944\n",
      "1.03059\n",
      "0.251029\n",
      "0.371324\n",
      "1.10367\n",
      "0.726597\n",
      "0.539717\n",
      "0.468474\n",
      "0.668343\n",
      "0.465957\n",
      "0.561514\n",
      "1.02867\n",
      "0.281923\n",
      "0.320724\n",
      "0.436696\n",
      "0.56108\n",
      "0.780312\n",
      "0.587381\n",
      "0.307555\n",
      "0.447899\n",
      "0.787203\n",
      "0.526543\n",
      "0.597343\n",
      "0.480903\n",
      "0.286264\n",
      "0.351052\n",
      "0.49633\n",
      "0.147701\n",
      "0.19911\n",
      "0.402227\n",
      "0.969709\n",
      "0.415643\n",
      "0.290243\n",
      "0.388664\n",
      "0.408291\n",
      "0.428886\n",
      "0.345908\n",
      "0.23454\n",
      "0.328972\n",
      "0.348313\n",
      "1.98247\n",
      "0.774787\n",
      "0.501268\n",
      "0.601684\n",
      "0.555917\n",
      "0.687687\n",
      "0.351746\n",
      "0.252479\n",
      "0.581197\n",
      "0.2818\n",
      "0.248903\n",
      "0.625835\n",
      "0.431915\n",
      "0.299968\n",
      "0.502584\n",
      "0.545303\n",
      "1.44264\n",
      "1.24699\n",
      "1.23545\n",
      "1.09121\n",
      "0.888184\n",
      "0.292642\n",
      "0.304839\n",
      "0.151842\n",
      "0.256716\n",
      "0.390282\n",
      "0.511382\n",
      "0.279248\n",
      "0.237948\n",
      "0.325941\n",
      "0.369541\n",
      "0.316029\n",
      "0.476725\n",
      "0.470376\n",
      "0.194781\n",
      "0.930063\n",
      "0.535229\n",
      "0.600576\n",
      "0.451846\n",
      "0.553853\n",
      "0.333282\n",
      "0.672061\n",
      "0.316618\n",
      "0.670967\n",
      "0.45919\n",
      "0.437711\n",
      "0.616579\n",
      "0.603592\n",
      "0.554008\n",
      "0.465631\n",
      "0.232397\n",
      "0.333871\n",
      "0.404341\n",
      "0.287\n",
      "0.37876\n",
      "0.216768\n",
      "0.502043\n",
      "0.347938\n",
      "0.336442\n",
      "0.564193\n",
      "0.776004\n",
      "0.399517\n",
      "1.35499\n",
      "1.16935\n",
      "0.748994\n",
      "0.399757\n",
      "0.65393\n",
      "0.438771\n",
      "0.316096\n",
      "0.376476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.564914\n",
      "0.281381\n",
      "0.482258\n",
      "0.559559\n",
      "0.320435\n",
      "0.316532\n",
      "0.57189\n",
      "0.282034\n",
      "0.672359\n",
      "0.541025\n",
      "0.167004\n",
      "0.302097\n",
      "0.679546\n",
      "0.641048\n",
      "0.451675\n",
      "0.301642\n",
      "0.351587\n",
      "0.795575\n",
      "0.655007\n",
      "0.158483\n",
      "0.507305\n",
      "0.306885\n",
      "0.435344\n",
      "0.257168\n",
      "0.343408\n",
      "0.512766\n",
      "0.182323\n",
      "0.331249\n",
      "0.179143\n",
      "0.283172\n",
      "0.300093\n",
      "0.436596\n",
      "0.527147\n",
      "1.28005\n",
      "1.47214\n",
      "0.55802\n",
      "0.455587\n",
      "0.479513\n",
      "0.26626\n",
      "0.271118\n",
      "0.182582\n",
      "0.344207\n",
      "0.549403\n",
      "0.871268\n",
      "0.526212\n",
      "0.487766\n",
      "0.508007\n",
      "0.384532\n",
      "0.176369\n",
      "0.192771\n",
      "0.167553\n",
      "0.192116\n",
      "0.755074\n",
      "0.379021\n",
      "0.473\n",
      "0.67472\n",
      "0.587921\n",
      "0.470505\n",
      "0.308129\n",
      "2.36826\n",
      "2.8639\n",
      "2.33683\n",
      "2.36667\n",
      "0.962879\n",
      "0.303825\n",
      "0.211435\n",
      "0.239136\n",
      "0.322374\n",
      "0.767628\n",
      "0.393192\n",
      "0.306963\n",
      "0.364339\n",
      "0.482693\n",
      "0.452518\n",
      "0.564382\n",
      "0.310342\n",
      "0.524383\n",
      "0.305445\n",
      "0.276092\n",
      "0.469395\n",
      "0.715589\n",
      "0.434085\n",
      "0.284086\n",
      "0.366276\n",
      "0.24904\n",
      "0.262036\n",
      "0.332098\n",
      "0.742759\n",
      "0.948005\n",
      "0.887153\n",
      "0.66421\n",
      "0.181876\n",
      "0.367548\n",
      "0.28052\n",
      "0.290709\n",
      "0.4041\n",
      "0.378491\n",
      "0.415284\n",
      "0.253933\n",
      "0.2769\n",
      "0.291353\n",
      "0.383528\n",
      "0.782321\n",
      "0.527077\n",
      "1.5788\n",
      "0.760144\n",
      "0.490336\n",
      "0.198795\n",
      "0.34386\n",
      "0.164055\n",
      "0.527431\n",
      "0.508205\n",
      "1.06846\n",
      "0.349672\n",
      "0.526853\n",
      "4.95055\n",
      "1.46287\n",
      "1.06315\n",
      "0.470467\n",
      "1.69472\n",
      "0.777662\n",
      "0.442411\n",
      "0.449804\n",
      "0.727915\n",
      "0.66302\n",
      "0.603149\n",
      "3.54339\n",
      "1.11573\n",
      "0.567407\n",
      "0.495654\n",
      "0.333693\n",
      "0.642495\n",
      "0.334604\n",
      "0.449289\n",
      "0.52713\n",
      "0.355044\n",
      "0.732626\n",
      "0.305409\n",
      "0.85279\n",
      "0.467379\n",
      "0.328152\n",
      "0.740465\n",
      "0.37301\n",
      "0.485533\n",
      "0.458883\n",
      "0.387204\n",
      "0.366329\n",
      "0.34752\n",
      "0.685179\n",
      "0.322283\n",
      "0.287534\n",
      "0.547253\n",
      "0.417971\n",
      "0.900876\n",
      "0.81037\n",
      "0.758639\n",
      "0.35302\n",
      "0.531616\n",
      "0.240755\n",
      "0.653477\n",
      "0.488091\n",
      "0.318648\n",
      "0.159164\n",
      "0.320468\n",
      "0.539445\n",
      "1.23319\n",
      "0.518023\n",
      "0.4304\n",
      "0.297584\n",
      "0.385854\n",
      "0.482165\n",
      "0.327131\n",
      "0.445213\n",
      "0.262876\n",
      "0.15251\n",
      "0.240104\n",
      "0.295327\n",
      "0.68047\n",
      "0.588933\n",
      "0.514001\n",
      "0.310232\n",
      "0.377032\n",
      "0.427634\n",
      "0.291304\n",
      "1.85997\n",
      "0.825182\n",
      "0.602526\n",
      "0.270773\n",
      "0.384625\n",
      "0.295841\n",
      "0.332784\n",
      "0.375326\n",
      "0.403777\n",
      "0.270521\n",
      "0.489798\n",
      "0.541711\n",
      "0.291973\n",
      "0.307665\n",
      "0.534378\n",
      "0.417527\n",
      "0.203321\n",
      "0.164505\n",
      "0.270608\n",
      "0.55502\n",
      "3.52266\n",
      "0.940353\n",
      "0.698377\n",
      "0.459008\n",
      "0.780693\n",
      "0.318668\n",
      "0.492983\n",
      "0.424705\n",
      "0.451779\n",
      "0.437544\n",
      "0.403778\n",
      "0.310247\n",
      "0.233822\n",
      "0.442015\n",
      "0.329774\n",
      "0.290583\n",
      "0.542937\n",
      "0.382311\n",
      "0.247474\n",
      "0.192937\n",
      "0.368647\n",
      "0.670893\n",
      "0.291624\n",
      "0.188042\n",
      "0.315959\n",
      "0.359138\n",
      "0.590122\n",
      "0.633595\n",
      "0.662803\n",
      "0.479615\n",
      "0.626044\n",
      "0.29428\n",
      "0.288515\n",
      "0.235179\n",
      "0.238554\n",
      "0.587408\n",
      "0.768388\n",
      "0.55884\n",
      "0.741321\n",
      "0.28665\n",
      "0.209016\n",
      "0.656951\n",
      "0.350855\n",
      "0.316641\n",
      "0.590541\n",
      "0.526258\n",
      "1.49276\n",
      "1.6158\n",
      "1.23061\n",
      "0.944445\n",
      "0.545753\n",
      "0.525566\n",
      "0.445414\n",
      "0.602422\n",
      "0.570543\n",
      "0.263734\n",
      "0.152291\n",
      "0.285742\n",
      "0.38687\n",
      "0.417534\n",
      "0.362543\n",
      "0.308966\n",
      "0.257775\n",
      "0.159539\n",
      "0.502743\n",
      "0.439465\n",
      "0.341057\n",
      "0.271734\n",
      "0.520498\n",
      "0.931146\n",
      "0.892447\n",
      "1.33459\n",
      "0.289105\n",
      "0.478595\n",
      "0.522595\n",
      "0.317111\n",
      "0.271112\n",
      "0.240974\n",
      "0.239798\n",
      "0.295653\n",
      "0.459372\n",
      "0.591645\n",
      "1.40592\n",
      "0.705767\n",
      "0.430665\n",
      "0.167974\n",
      "0.472095\n",
      "0.186338\n",
      "0.505258\n",
      "0.456595\n",
      "0.558121\n",
      "0.43857\n",
      "0.193638\n",
      "0.238895\n",
      "0.309428\n",
      "0.341428\n",
      "0.213512\n",
      "0.295329\n",
      "0.468293\n",
      "0.195804\n",
      "0.585768\n",
      "0.450523\n",
      "0.257948\n",
      "0.259417\n",
      "0.357764\n",
      "0.339016\n",
      "0.217553\n",
      "1.30864\n",
      "2.10441\n",
      "1.59099\n",
      "1.06127\n",
      "0.911573\n",
      "0.641034\n",
      "0.724081\n",
      "0.897842\n",
      "0.748552\n",
      "0.803142\n",
      "0.400295\n",
      "0.293632\n",
      "0.482518\n",
      "0.319119\n",
      "0.853948\n",
      "1.27227\n",
      "0.851076\n",
      "0.683394\n",
      "0.513987\n",
      "0.330027\n",
      "0.36513\n",
      "0.438911\n",
      "0.526539\n",
      "0.249463\n",
      "0.286088\n",
      "0.232465\n",
      "0.306963\n",
      "0.546541\n",
      "1.18861\n",
      "0.745398\n",
      "0.735641\n",
      "0.401877\n",
      "0.49139\n",
      "0.472788\n",
      "0.296606\n",
      "0.471385\n",
      "1.00001\n",
      "0.704853\n",
      "0.249337\n",
      "0.330072\n",
      "0.233143\n",
      "0.325441\n",
      "0.259995\n",
      "0.233432\n",
      "0.155957\n",
      "0.228542\n",
      "0.43566\n",
      "0.33321\n",
      "0.447193\n",
      "0.261664\n",
      "0.538556\n",
      "0.443982\n",
      "0.207958\n",
      "0.65917\n",
      "0.538909\n",
      "0.489771\n",
      "0.215159\n",
      "0.262963\n",
      "0.596854\n",
      "0.456047\n",
      "0.190199\n",
      "0.581097\n",
      "0.617017\n",
      "0.59971\n",
      "0.985554\n",
      "1.16025\n",
      "0.657454\n",
      "0.802717\n",
      "0.389106\n",
      "0.449099\n",
      "0.229277\n",
      "0.49097\n",
      "0.534016\n",
      "0.942258\n",
      "0.731013\n",
      "1.64936\n",
      "1.3091\n",
      "0.378872\n",
      "0.476504\n",
      "0.47534\n",
      "0.195494\n",
      "0.207736\n",
      "0.169536\n",
      "0.378819\n",
      "0.295864\n",
      "0.73378\n",
      "0.246545\n",
      "0.322549\n",
      "0.294289\n",
      "0.238229\n",
      "0.217431\n",
      "0.472992\n",
      "0.743794\n",
      "0.608012\n",
      "2.693\n",
      "0.940379\n",
      "1.43114\n",
      "0.778815\n",
      "0.510005\n",
      "0.43655\n",
      "0.379515\n",
      "0.323084\n",
      "0.302297\n",
      "0.455432\n",
      "0.281937\n",
      "0.346569\n",
      "0.359002\n",
      "0.52428\n",
      "0.45836\n",
      "0.358869\n",
      "0.903385\n",
      "0.366167\n",
      "0.193192\n",
      "0.250993\n",
      "0.132168\n",
      "0.411245\n",
      "0.162822\n",
      "0.310567\n",
      "0.206473\n",
      "0.279428\n",
      "0.254088\n",
      "0.256567\n",
      "0.116133\n",
      "0.507272\n",
      "0.403586\n",
      "0.325581\n",
      "0.462257\n",
      "0.555085\n",
      "0.303554\n",
      "0.27235\n",
      "0.462244\n",
      "0.518138\n",
      "0.288063\n",
      "0.722032\n",
      "0.792586\n",
      "0.373106\n",
      "0.138204\n",
      "0.235779\n",
      "0.33122\n",
      "0.24573\n",
      "0.192475\n",
      "0.200474\n",
      "0.152624\n",
      "0.275732\n",
      "0.234105\n",
      "0.237686\n",
      "0.363426\n",
      "0.301031\n",
      "0.46135\n",
      "0.704651\n",
      "0.839462\n",
      "0.392662\n",
      "0.224468\n",
      "0.531287\n",
      "0.307021\n",
      "0.42966\n",
      "0.339412\n",
      "0.262947\n",
      "0.307306\n",
      "0.438589\n",
      "0.357839\n",
      "0.488861\n",
      "1.15184\n",
      "0.74962\n",
      "0.441475\n",
      "0.380862\n",
      "0.691588\n",
      "0.483253\n",
      "0.242523\n",
      "0.359264\n",
      "0.257858\n",
      "0.287418\n",
      "0.406067\n",
      "0.435368\n",
      "1.06127\n",
      "0.598847\n",
      "0.294004\n",
      "0.306255\n",
      "0.2857\n",
      "0.50913\n",
      "0.595591\n",
      "0.330422\n",
      "0.811027\n",
      "0.248579\n",
      "0.40929\n",
      "0.232458\n",
      "0.663381\n",
      "0.347165\n",
      "0.496776\n",
      "0.369825\n",
      "0.249574\n",
      "0.429353\n",
      "0.706031\n",
      "1.16228\n",
      "0.742637\n",
      "0.58636\n",
      "0.531536\n",
      "0.38782\n",
      "0.192576\n",
      "0.217674\n",
      "0.303457\n",
      "0.482841\n",
      "0.427962\n",
      "0.332488\n",
      "0.285746\n",
      "0.472763\n",
      "0.303673\n",
      "0.347034\n",
      "0.387297\n",
      "0.252148\n",
      "0.190471\n",
      "0.198625\n",
      "0.491371\n",
      "0.509696\n",
      "0.320399\n",
      "0.156698\n",
      "0.39122\n",
      "0.536426\n",
      "0.327274\n",
      "0.308419\n",
      "0.32158\n",
      "0.507578\n",
      "0.181188\n",
      "0.758244\n",
      "0.200941\n",
      "0.576289\n",
      "0.268157\n",
      "0.401135\n",
      "0.349158\n",
      "0.419802\n",
      "0.201433\n",
      "0.368297\n",
      "0.183151\n",
      "0.270902\n",
      "0.422839\n",
      "0.549168\n",
      "0.278306\n",
      "0.401039\n",
      "0.185202\n",
      "0.272303\n",
      "0.538444\n",
      "0.292135\n",
      "0.592907\n",
      "0.857911\n",
      "1.36078\n",
      "0.650213\n",
      "0.338924\n",
      "0.273041\n",
      "0.856125\n",
      "0.581197\n",
      "0.294486\n",
      "0.423796\n",
      "0.7763\n",
      "0.397074\n",
      "0.296546\n",
      "0.411233\n",
      "0.355169\n",
      "0.226981\n",
      "0.28021\n",
      "0.1919\n",
      "0.30017\n",
      "0.308352\n",
      "0.488025\n",
      "0.467428\n",
      "0.383452\n",
      "0.310562\n",
      "0.294307\n",
      "0.362822\n",
      "0.303313\n",
      "1.11515\n",
      "0.488276\n",
      "0.31189\n",
      "0.293936\n",
      "0.24635\n",
      "0.187227\n",
      "0.130483\n",
      "0.192139\n",
      "0.593422\n",
      "0.368041\n",
      "0.461333\n",
      "0.570766\n",
      "0.493565\n",
      "0.355575\n",
      "0.581829\n",
      "0.313862\n",
      "0.403684\n",
      "0.339406\n",
      "0.406861\n",
      "0.700207\n",
      "0.339296\n",
      "0.146094\n",
      "0.147054\n",
      "0.475325\n",
      "0.294131\n",
      "0.376657\n",
      "0.143692\n",
      "0.210444\n",
      "0.18679\n",
      "0.49468\n",
      "0.338256\n",
      "0.387337\n",
      "0.331459\n",
      "0.494131\n",
      "0.498537\n",
      "0.289702\n",
      "0.409265\n",
      "0.166563\n",
      "0.384847\n",
      "0.115127\n",
      "0.350004\n",
      "0.298617\n",
      "0.369722\n",
      "0.221539\n",
      "0.445428\n",
      "0.460709\n",
      "0.308495\n",
      "0.336828\n",
      "0.643409\n",
      "0.414026\n",
      "0.328212\n",
      "0.381555\n",
      "0.338039\n",
      "0.59554\n",
      "0.404689\n",
      "1.05935\n",
      "0.207544\n",
      "0.60361\n",
      "0.257504\n",
      "0.767403\n",
      "0.499278\n",
      "0.480277\n",
      "0.254885\n",
      "0.434027\n",
      "0.279041\n",
      "0.802808\n",
      "0.587968\n",
      "0.411513\n",
      "0.470107\n",
      "0.911158\n",
      "0.683705\n",
      "0.672307\n",
      "0.533564\n",
      "1.35092\n",
      "1.14815\n",
      "0.490894\n",
      "0.159115\n",
      "0.240334\n",
      "0.362261\n",
      "0.291915\n",
      "0.245355\n",
      "0.404422\n",
      "0.602935\n",
      "0.264145\n",
      "0.232737\n",
      "0.159788\n",
      "0.446161\n",
      "0.277489\n",
      "0.681999\n",
      "0.486012\n",
      "0.266603\n",
      "0.428111\n",
      "0.454784\n",
      "0.271756\n",
      "0.349849\n",
      "0.38489\n",
      "0.448542\n",
      "0.169681\n",
      "0.251595\n",
      "0.324744\n",
      "0.40887\n",
      "0.428628\n",
      "0.282585\n",
      "0.214862\n",
      "0.317505\n",
      "0.413384\n",
      "0.481088\n",
      "0.418813\n",
      "1.13559\n",
      "1.68181\n",
      "0.89988\n",
      "0.408791\n",
      "0.706565\n",
      "0.22318\n",
      "0.36933\n",
      "0.249885\n",
      "0.625181\n",
      "0.23731\n",
      "0.235002\n",
      "0.517901\n",
      "0.668841\n",
      "0.475217\n",
      "0.592157\n",
      "0.366977\n",
      "0.311022\n",
      "0.12317\n",
      "0.291959\n",
      "0.289081\n",
      "0.35698\n",
      "1.093\n",
      "0.754174\n",
      "0.557359\n",
      "0.710928\n",
      "1.04124\n",
      "0.472441\n",
      "0.668701\n",
      "0.402914\n",
      "0.305009\n",
      "0.183211\n",
      "0.357368\n",
      "0.273631\n",
      "0.215268\n",
      "0.29163\n",
      "0.337397\n",
      "0.444699\n",
      "0.65943\n",
      "0.375526\n",
      "0.426888\n",
      "0.443588\n",
      "0.495814\n",
      "0.318423\n",
      "0.565525\n",
      "0.238428\n",
      "0.282466\n",
      "0.411869\n",
      "0.6825\n",
      "0.380438\n",
      "0.53936\n",
      "0.23528\n",
      "0.234761\n",
      "0.269045\n",
      "0.37183\n",
      "0.214203\n",
      "0.267854\n",
      "0.430378\n",
      "0.76597\n",
      "0.648961\n",
      "0.492764\n",
      "9.71172\n",
      "0.993085\n",
      "0.53576\n",
      "0.392783\n",
      "0.275802\n",
      "0.482092\n",
      "0.383504\n",
      "0.478946\n",
      "0.275912\n",
      "0.309802\n",
      "0.242307\n",
      "0.435486\n",
      "0.236422\n",
      "0.347526\n",
      "0.407736\n",
      "0.422153\n",
      "0.241963\n",
      "0.410853\n",
      "0.34794\n",
      "0.288649\n",
      "0.114032\n",
      "0.24676\n",
      "0.332988\n",
      "0.386111\n",
      "0.571143\n",
      "0.880282\n",
      "0.30865\n",
      "0.286984\n",
      "0.52009\n",
      "0.276614\n",
      "0.321614\n",
      "0.384563\n",
      "0.422126\n",
      "0.468847\n",
      "0.156365\n",
      "0.267982\n",
      "0.254936\n",
      "0.586019\n",
      "2.17579\n",
      "1.747\n",
      "1.10294\n",
      "0.454356\n",
      "0.254448\n",
      "0.278344\n",
      "0.367155\n",
      "0.922091\n",
      "0.505228\n",
      "0.359479\n",
      "0.213057\n",
      "0.609351\n",
      "0.306868\n",
      "0.171822\n",
      "0.192623\n",
      "0.103665\n",
      "0.26059\n",
      "0.835982\n",
      "0.937293\n",
      "0.532129\n",
      "0.31678\n",
      "0.510942\n",
      "0.336251\n",
      "0.293757\n",
      "0.433743\n",
      "0.161005\n",
      "0.272403\n",
      "0.425751\n",
      "0.327132\n",
      "0.241563\n",
      "0.563548\n",
      "0.183805\n",
      "0.275763\n",
      "0.854311\n",
      "0.221381\n",
      "0.255708\n",
      "0.194087\n",
      "0.859055\n",
      "0.283066\n",
      "0.383814\n",
      "0.635459\n",
      "0.336434\n",
      "0.257711\n",
      "0.376848\n",
      "0.245121\n",
      "0.203139\n",
      "0.578944\n",
      "0.534194\n",
      "0.648407\n",
      "0.448336\n",
      "0.436236\n",
      "0.327613\n",
      "0.163563\n",
      "0.352997\n",
      "1.11828\n",
      "0.347451\n",
      "0.29652\n",
      "0.479437\n",
      "1.49743\n",
      "0.376109\n",
      "0.310814\n",
      "0.180838\n",
      "0.155006\n",
      "0.173874\n",
      "0.301064\n",
      "0.141658\n",
      "0.175616\n",
      "0.256616\n",
      "0.377175\n",
      "0.17418\n",
      "0.134455\n",
      "0.22538\n",
      "0.180215\n",
      "0.302194\n",
      "0.148161\n",
      "0.302366\n",
      "0.194633\n",
      "0.187159\n",
      "0.508964\n",
      "0.47291\n",
      "1.01308\n",
      "1.00467\n",
      "0.487916\n",
      "0.553778\n",
      "0.208437\n",
      "0.31817\n",
      "0.314721\n",
      "0.237607\n",
      "0.278816\n",
      "0.359215\n",
      "0.524517\n",
      "0.912907\n",
      "0.985852\n",
      "0.330322\n",
      "0.377964\n",
      "0.168724\n",
      "0.285629\n",
      "0.370826\n",
      "0.568127\n",
      "0.405963\n",
      "0.636866\n",
      "0.452428\n",
      "0.304737\n",
      "0.833017\n",
      "0.37236\n",
      "0.299073\n",
      "0.328318\n",
      "0.274051\n",
      "0.576058\n",
      "0.416087\n",
      "0.388644\n",
      "0.298348\n",
      "0.346053\n",
      "0.233641\n",
      "0.530348\n",
      "0.208657\n",
      "0.203935\n",
      "0.183157\n",
      "0.239847\n",
      "0.218711\n",
      "0.202805\n",
      "0.169784\n",
      "0.180544\n",
      "0.242817\n",
      "0.228193\n",
      "0.246244\n",
      "0.457884\n",
      "0.296679\n",
      "0.317954\n",
      "1.43174\n",
      "0.555865\n",
      "0.452721\n",
      "0.564397\n",
      "0.567537\n",
      "0.520688\n",
      "0.48229\n",
      "0.358428\n",
      "0.345401\n",
      "0.266292\n",
      "0.19871\n",
      "0.46765\n",
      "0.49292\n",
      "0.818057\n",
      "2.08455\n",
      "1.1454\n",
      "0.448243\n",
      "0.356324\n",
      "0.466911\n",
      "0.634859\n",
      "0.381119\n",
      "0.545472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.185684\n",
      "0.221553\n",
      "0.390174\n",
      "0.250264\n",
      "0.24132\n",
      "0.239826\n",
      "0.444675\n",
      "0.338124\n",
      "0.385345\n",
      "0.196361\n",
      "0.188897\n",
      "0.274519\n",
      "0.183647\n",
      "0.541704\n",
      "0.500251\n",
      "0.20984\n",
      "0.243874\n",
      "0.505169\n",
      "0.424668\n",
      "0.271525\n",
      "0.209636\n",
      "0.259435\n",
      "0.3176\n",
      "0.130282\n",
      "0.367991\n",
      "0.155806\n",
      "0.291556\n",
      "0.279253\n",
      "0.532035\n",
      "0.911199\n",
      "0.393047\n",
      "0.309009\n",
      "0.47445\n",
      "0.257116\n",
      "0.280738\n",
      "0.585359\n",
      "0.29714\n",
      "0.419749\n",
      "0.27902\n",
      "0.538483\n",
      "0.917038\n",
      "1.39239\n",
      "0.631224\n",
      "0.519917\n",
      "0.908818\n",
      "0.212027\n",
      "0.658836\n",
      "0.296828\n",
      "0.456707\n",
      "0.715347\n",
      "0.718296\n",
      "0.352569\n",
      "0.190004\n",
      "0.386902\n",
      "0.429036\n",
      "0.211784\n",
      "0.246902\n",
      "0.28504\n",
      "0.372236\n",
      "0.386332\n",
      "1.07152\n",
      "0.835619\n",
      "0.100988\n",
      "0.284647\n",
      "0.187664\n",
      "0.208617\n",
      "0.256722\n",
      "0.455736\n",
      "1.24107\n",
      "1.07311\n",
      "0.919635\n",
      "0.908704\n",
      "0.373562\n",
      "0.457792\n",
      "0.481856\n",
      "0.289857\n",
      "0.24641\n",
      "0.340319\n",
      "0.51514\n",
      "0.638552\n",
      "0.437371\n",
      "0.438269\n",
      "0.268945\n",
      "0.375259\n",
      "0.705913\n",
      "1.58064\n",
      "1.47848\n",
      "0.906052\n",
      "0.837502\n",
      "0.400988\n",
      "0.306938\n",
      "0.381546\n",
      "0.286643\n",
      "0.503659\n",
      "0.317596\n",
      "0.121491\n",
      "0.310064\n",
      "0.659065\n",
      "0.281824\n",
      "0.479154\n",
      "0.244004\n",
      "0.379178\n",
      "0.265522\n",
      "0.309592\n",
      "0.300904\n",
      "0.32763\n",
      "0.782961\n",
      "0.67964\n",
      "0.945625\n",
      "1.28021\n",
      "0.75165\n",
      "0.458085\n",
      "0.629093\n",
      "0.380637\n",
      "0.396911\n",
      "0.44096\n",
      "0.327317\n",
      "0.354582\n",
      "0.883438\n",
      "0.601856\n",
      "0.399099\n",
      "0.31642\n",
      "0.513385\n",
      "0.800029\n",
      "0.79745\n",
      "0.219605\n",
      "0.139425\n",
      "0.306304\n",
      "0.650019\n",
      "0.595792\n",
      "0.255188\n",
      "0.536678\n",
      "0.37372\n",
      "0.484147\n",
      "0.381372\n",
      "0.274665\n",
      "0.188171\n",
      "0.318558\n",
      "0.209595\n",
      "0.459406\n",
      "0.373717\n",
      "0.360192\n",
      "0.306857\n",
      "0.322497\n",
      "0.273545\n",
      "0.274577\n",
      "0.40447\n",
      "0.347716\n",
      "0.298946\n",
      "0.375989\n",
      "0.709202\n",
      "0.280361\n",
      "0.258693\n",
      "0.465293\n",
      "0.233518\n",
      "0.340734\n",
      "0.284308\n",
      "0.896642\n",
      "0.463388\n",
      "0.419386\n",
      "0.29178\n",
      "0.752666\n",
      "1.39103\n",
      "0.303002\n",
      "0.24902\n",
      "0.151202\n",
      "0.339952\n",
      "0.24249\n",
      "0.852739\n",
      "0.619515\n",
      "0.724021\n",
      "0.409497\n",
      "0.688378\n",
      "0.379272\n",
      "0.538432\n",
      "1.83222\n",
      "0.445785\n",
      "0.249002\n",
      "0.421299\n",
      "0.302759\n",
      "0.212405\n",
      "0.401366\n",
      "0.356802\n",
      "0.389547\n",
      "0.558613\n",
      "0.84247\n",
      "0.43628\n",
      "0.240278\n",
      "0.159349\n",
      "0.577524\n",
      "0.248359\n",
      "0.172261\n",
      "0.222459\n",
      "0.539548\n",
      "0.230166\n",
      "0.286799\n",
      "0.141306\n",
      "0.479879\n",
      "0.263915\n",
      "0.210341\n",
      "0.270016\n",
      "0.613094\n",
      "0.571163\n",
      "0.760457\n",
      "0.671717\n",
      "0.32871\n",
      "0.585591\n",
      "0.248456\n",
      "0.359034\n",
      "0.318238\n",
      "0.525323\n",
      "0.312488\n",
      "0.249419\n",
      "0.971334\n",
      "0.4627\n",
      "0.46876\n",
      "0.600007\n",
      "0.355716\n",
      "0.717811\n",
      "0.494699\n",
      "0.540261\n",
      "0.410797\n",
      "0.494521\n",
      "0.224437\n",
      "0.497898\n",
      "0.392499\n",
      "0.431661\n",
      "0.194109\n",
      "0.262216\n",
      "0.469166\n",
      "0.264554\n",
      "0.354473\n",
      "0.285368\n",
      "0.307642\n",
      "0.208262\n",
      "0.271969\n",
      "0.254131\n",
      "0.760293\n",
      "0.835506\n",
      "0.239714\n",
      "0.376694\n",
      "0.474614\n",
      "0.505711\n",
      "0.610992\n",
      "0.27086\n",
      "0.194983\n",
      "0.813699\n",
      "0.147175\n",
      "0.562836\n",
      "0.209514\n",
      "0.399981\n",
      "0.251959\n",
      "0.579036\n",
      "0.270205\n",
      "0.19669\n",
      "0.181027\n",
      "0.209516\n",
      "0.256591\n",
      "0.308213\n",
      "0.182963\n",
      "0.16589\n",
      "0.253603\n",
      "0.371628\n",
      "0.241813\n",
      "0.213435\n",
      "0.36967\n",
      "0.231385\n",
      "0.210143\n",
      "0.245847\n",
      "0.363566\n",
      "0.228563\n",
      "0.302056\n",
      "0.404976\n",
      "0.655408\n",
      "0.376668\n",
      "0.283621\n",
      "0.56539\n",
      "0.277424\n",
      "0.211024\n",
      "0.218224\n",
      "0.391377\n",
      "0.313369\n",
      "0.293169\n",
      "0.359078\n",
      "0.217065\n",
      "0.468588\n",
      "0.547259\n",
      "0.331062\n",
      "0.588825\n",
      "1.06253\n",
      "1.22466\n",
      "0.871888\n",
      "0.906873\n",
      "0.269044\n",
      "0.181436\n",
      "1.08973\n",
      "0.180295\n",
      "0.319058\n",
      "0.8985\n",
      "1.46366\n",
      "1.60173\n",
      "0.955283\n",
      "0.554806\n",
      "0.71937\n",
      "0.233288\n",
      "9.44678\n",
      "2.48463\n",
      "1.07434\n",
      "0.392603\n",
      "0.323986\n",
      "0.343998\n",
      "0.714553\n",
      "0.164611\n",
      "0.548112\n",
      "0.403206\n",
      "0.153627\n",
      "0.168644\n",
      "0.127674\n",
      "0.212273\n",
      "0.371935\n",
      "0.307477\n",
      "0.33709\n",
      "0.369757\n",
      "0.232978\n",
      "0.313592\n",
      "0.272717\n",
      "0.282811\n",
      "0.242904\n",
      "0.475378\n",
      "0.404856\n",
      "0.279867\n",
      "0.257132\n",
      "0.49595\n",
      "0.639667\n",
      "0.559072\n",
      "0.542238\n",
      "0.645138\n",
      "1.36081\n",
      "0.437325\n",
      "0.213981\n",
      "0.588638\n",
      "0.396412\n",
      "0.305368\n",
      "0.665978\n",
      "0.144737\n",
      "0.224114\n",
      "0.650739\n",
      "0.41535\n",
      "0.221716\n",
      "0.357134\n",
      "0.234389\n",
      "0.156179\n",
      "0.23169\n",
      "0.5389\n",
      "0.70591\n",
      "0.155759\n",
      "0.269659\n",
      "0.376907\n",
      "0.165069\n",
      "0.367575\n",
      "0.475039\n",
      "0.37809\n",
      "0.290012\n",
      "0.229637\n",
      "0.214999\n",
      "0.220243\n",
      "0.35987\n",
      "1.25262\n",
      "0.480676\n",
      "0.300483\n",
      "0.191688\n",
      "0.159215\n",
      "0.248568\n",
      "0.202802\n",
      "0.215987\n",
      "0.260661\n",
      "0.191812\n",
      "0.136921\n",
      "0.464904\n",
      "0.59257\n",
      "0.265484\n",
      "0.311683\n",
      "0.950128\n",
      "0.372257\n",
      "0.572919\n",
      "0.650263\n",
      "0.190032\n",
      "0.26466\n",
      "0.168014\n",
      "0.34398\n",
      "0.294624\n",
      "0.354483\n",
      "0.439399\n",
      "0.187558\n",
      "0.474208\n",
      "0.221549\n",
      "0.191661\n",
      "0.271939\n",
      "0.495265\n",
      "0.300029\n",
      "0.467629\n",
      "0.393771\n",
      "0.299959\n",
      "0.197579\n",
      "0.236979\n",
      "0.236886\n",
      "0.256917\n",
      "0.227335\n",
      "0.200125\n",
      "0.167896\n",
      "0.461296\n",
      "0.551949\n",
      "0.404794\n",
      "0.227589\n",
      "0.163031\n",
      "0.224316\n",
      "0.145613\n",
      "0.386393\n",
      "0.328101\n",
      "0.359167\n",
      "1.18702\n",
      "0.19797\n",
      "0.622482\n",
      "0.67182\n",
      "0.354879\n",
      "0.44753\n",
      "0.571876\n",
      "1.58293\n",
      "0.949994\n",
      "0.607817\n",
      "0.362888\n",
      "0.337292\n",
      "0.497678\n",
      "0.281743\n",
      "0.888445\n",
      "0.639549\n",
      "0.496584\n",
      "0.843255\n",
      "0.194054\n",
      "3.73358\n",
      "2.22912\n",
      "1.83291\n",
      "1.05328\n",
      "0.697889\n",
      "0.38251\n",
      "0.296654\n",
      "0.497812\n",
      "0.681219\n",
      "0.393695\n",
      "0.235707\n",
      "0.455838\n",
      "0.793206\n",
      "0.329125\n",
      "0.219838\n",
      "0.37134\n",
      "1.13101\n",
      "0.724455\n",
      "0.543332\n",
      "0.391232\n",
      "0.157851\n",
      "0.39584\n",
      "0.495532\n",
      "0.730524\n",
      "0.59814\n",
      "0.466145\n",
      "0.286805\n",
      "0.389638\n",
      "0.882444\n",
      "0.805318\n",
      "0.584492\n",
      "0.450909\n",
      "1.36217\n",
      "0.854485\n",
      "0.904568\n",
      "0.363847\n",
      "0.459006\n",
      "0.274681\n",
      "0.172202\n",
      "0.286854\n",
      "0.179759\n",
      "0.31696\n",
      "0.383895\n",
      "0.593007\n",
      "0.321019\n",
      "0.612122\n",
      "0.596219\n",
      "0.488991\n",
      "0.343067\n",
      "0.480428\n",
      "0.345256\n",
      "0.37931\n",
      "0.257276\n",
      "0.347782\n",
      "0.622315\n",
      "0.246899\n",
      "0.209787\n",
      "0.552245\n",
      "0.367247\n",
      "0.699245\n",
      "0.178954\n",
      "0.357181\n",
      "0.458716\n",
      "0.510908\n",
      "0.549506\n",
      "0.144085\n",
      "0.160028\n",
      "0.271816\n",
      "0.569603\n",
      "0.40074\n",
      "0.269263\n",
      "0.395277\n",
      "0.127726\n",
      "0.777353\n",
      "0.106428\n",
      "0.271729\n",
      "0.559446\n",
      "0.699268\n",
      "0.559711\n",
      "0.37909\n",
      "0.158106\n",
      "0.271764\n",
      "0.382911\n",
      "0.276245\n",
      "0.297245\n",
      "0.239728\n",
      "0.138539\n",
      "0.282505\n",
      "0.235418\n",
      "0.536006\n",
      "0.427251\n",
      "0.746506\n",
      "0.597676\n",
      "0.468925\n",
      "0.658745\n",
      "0.29468\n",
      "0.20815\n",
      "0.209784\n",
      "1.08511\n",
      "1.02177\n",
      "1.53215\n",
      "1.4033\n",
      "1.35335\n",
      "1.22909\n",
      "1.52217\n",
      "1.5744\n",
      "0.530539\n",
      "0.500728\n",
      "0.46125\n",
      "0.474647\n",
      "0.238838\n",
      "0.334951\n",
      "0.331652\n",
      "0.309285\n",
      "0.533186\n",
      "0.244227\n",
      "0.25648\n",
      "0.242488\n",
      "0.189144\n",
      "0.257224\n",
      "0.197701\n",
      "0.277019\n",
      "0.241585\n",
      "0.265797\n",
      "0.100442\n",
      "0.480826\n",
      "0.652518\n",
      "0.187754\n",
      "0.147778\n",
      "0.396264\n",
      "0.408767\n",
      "1.14425\n",
      "0.886177\n",
      "0.553603\n",
      "0.571393\n",
      "0.615016\n",
      "1.02729\n",
      "0.701418\n",
      "0.453141\n",
      "0.417522\n",
      "1.00878\n",
      "0.766172\n",
      "0.545907\n",
      "0.532503\n",
      "0.497878\n",
      "0.322814\n",
      "0.74314\n",
      "0.248538\n",
      "0.42182\n",
      "0.253087\n",
      "0.144091\n",
      "0.195852\n",
      "0.24659\n",
      "0.405559\n",
      "0.369801\n",
      "0.270681\n",
      "0.290427\n",
      "0.249915\n",
      "0.285731\n",
      "0.31261\n",
      "0.243193\n",
      "0.20438\n",
      "0.498287\n",
      "0.364391\n",
      "0.182888\n",
      "0.191233\n",
      "0.268773\n",
      "0.229346\n",
      "0.383509\n",
      "0.494939\n",
      "0.397733\n",
      "0.43002\n",
      "0.564011\n",
      "0.305616\n",
      "0.277028\n",
      "0.535543\n",
      "0.675614\n",
      "0.37668\n",
      "0.202107\n",
      "0.26405\n",
      "0.170807\n",
      "0.269301\n",
      "0.391125\n",
      "0.158036\n",
      "0.263072\n",
      "0.313561\n",
      "0.418738\n",
      "0.663039\n",
      "0.293821\n",
      "0.50635\n",
      "0.908652\n",
      "1.46209\n",
      "0.618004\n",
      "0.54198\n",
      "0.444343\n",
      "0.52301\n",
      "1.11045\n",
      "1.11918\n",
      "0.874004\n",
      "0.564641\n",
      "0.78263\n",
      "0.690304\n",
      "0.721196\n",
      "0.537113\n",
      "0.815419\n",
      "0.778487\n",
      "0.645671\n",
      "0.208334\n",
      "0.221521\n",
      "0.196736\n",
      "0.142691\n",
      "0.233454\n",
      "0.388287\n",
      "0.347126\n",
      "0.586571\n",
      "0.216886\n",
      "0.135511\n",
      "0.163971\n",
      "0.221157\n",
      "0.316039\n",
      "1.42066\n",
      "0.777828\n",
      "0.585981\n",
      "0.370106\n",
      "0.304161\n",
      "0.245509\n",
      "0.207911\n",
      "0.255232\n",
      "0.312206\n",
      "0.198466\n",
      "0.285483\n",
      "0.205836\n",
      "0.18315\n",
      "0.425774\n",
      "0.189498\n",
      "0.185809\n",
      "0.342874\n",
      "0.172324\n",
      "0.351379\n",
      "0.65219\n",
      "1.04133\n",
      "0.349053\n",
      "0.745349\n",
      "0.588954\n",
      "0.16511\n",
      "0.245382\n",
      "0.203521\n",
      "0.202111\n",
      "0.541591\n",
      "0.640143\n",
      "0.793058\n",
      "0.644024\n",
      "0.578532\n",
      "0.57635\n",
      "0.278211\n",
      "0.391608\n",
      "0.384559\n",
      "0.422039\n",
      "0.10937\n",
      "0.23406\n",
      "0.406832\n",
      "0.339954\n",
      "0.339403\n",
      "0.434831\n",
      "0.151946\n",
      "0.302644\n",
      "0.252789\n",
      "0.507794\n",
      "0.199814\n",
      "0.178502\n",
      "0.516721\n",
      "0.369655\n",
      "0.162381\n",
      "0.216418\n",
      "0.325853\n",
      "0.406799\n",
      "0.316579\n",
      "0.301515\n",
      "0.24652\n",
      "0.540457\n",
      "0.60356\n",
      "0.287112\n",
      "0.264772\n",
      "0.581394\n",
      "0.590181\n",
      "0.997663\n",
      "0.264184\n",
      "0.359138\n",
      "0.646943\n",
      "0.374986\n",
      "0.539714\n",
      "0.703804\n",
      "0.561053\n",
      "0.369057\n",
      "0.170405\n",
      "0.202665\n",
      "0.208121\n",
      "0.253841\n",
      "0.23933\n",
      "0.602466\n",
      "0.301289\n",
      "0.376088\n",
      "0.518151\n",
      "0.418754\n",
      "0.390946\n",
      "0.272659\n",
      "0.408419\n",
      "0.242617\n",
      "0.204754\n",
      "0.630894\n",
      "0.312353\n",
      "0.693383\n",
      "0.354862\n",
      "0.230018\n",
      "0.407598\n",
      "0.310167\n",
      "0.408749\n",
      "0.236826\n",
      "0.789566\n",
      "0.195536\n",
      "0.0989477\n",
      "0.235717\n",
      "0.279301\n",
      "0.234581\n",
      "0.594387\n",
      "0.522968\n",
      "0.418916\n",
      "0.298631\n",
      "0.29289\n",
      "0.662294\n",
      "0.533084\n",
      "0.654108\n",
      "0.298152\n",
      "0.321017\n",
      "0.732916\n",
      "0.182079\n",
      "0.227443\n",
      "0.288869\n",
      "0.28764\n",
      "0.4055\n",
      "0.333222\n",
      "0.519169\n",
      "0.410781\n",
      "0.327989\n",
      "0.266463\n",
      "0.17593\n",
      "0.417438\n",
      "0.384254\n",
      "0.189934\n",
      "0.163982\n",
      "0.398428\n",
      "0.101161\n",
      "0.46473\n",
      "0.315026\n",
      "0.191616\n",
      "0.477515\n",
      "0.376952\n",
      "0.200862\n",
      "0.350521\n",
      "0.269314\n",
      "0.448817\n",
      "2.12947\n",
      "1.59579\n",
      "0.88228\n",
      "1.15718\n",
      "1.03569\n",
      "0.648054\n",
      "0.346316\n",
      "0.104376\n",
      "0.242714\n",
      "0.439716\n",
      "0.167209\n",
      "0.535507\n",
      "0.456958\n",
      "0.355345\n",
      "0.41179\n",
      "3.64945\n",
      "1.3702\n",
      "0.729557\n",
      "0.377917\n",
      "0.194119\n",
      "0.397566\n",
      "0.459045\n",
      "0.461021\n",
      "0.424995\n",
      "0.752589\n",
      "0.539758\n",
      "0.478503\n",
      "0.297625\n",
      "0.182531\n",
      "0.431258\n",
      "0.161821\n",
      "0.589334\n",
      "0.67263\n",
      "0.486997\n",
      "0.449363\n",
      "0.321416\n",
      "0.220963\n",
      "0.367928\n",
      "0.203755\n",
      "0.45444\n",
      "0.8658\n",
      "0.477715\n",
      "0.491289\n",
      "0.178984\n",
      "0.329748\n",
      "0.55214\n",
      "0.168739\n",
      "0.189331\n",
      "0.342407\n",
      "0.334829\n",
      "0.419833\n",
      "0.43114\n",
      "0.314389\n",
      "0.206167\n",
      "0.221633\n",
      "0.107724\n",
      "0.419736\n",
      "0.517153\n",
      "0.428914\n",
      "0.3709\n",
      "0.729431\n",
      "0.241404\n",
      "0.267324\n",
      "0.768774\n",
      "0.592842\n",
      "0.297822\n",
      "0.265533\n",
      "0.190132\n",
      "0.525092\n",
      "0.367457\n",
      "0.308098\n",
      "0.590909\n",
      "0.221198\n",
      "0.150163\n",
      "0.282251\n",
      "0.293798\n",
      "0.118607\n",
      "0.708459\n",
      "0.31452\n",
      "0.265112\n",
      "0.573203\n",
      "0.25109\n",
      "0.239324\n",
      "0.138591\n",
      "0.299855\n",
      "0.270853\n",
      "0.170411\n",
      "0.171674\n",
      "0.426465\n",
      "0.249573\n",
      "0.553966\n",
      "0.401634\n",
      "0.833808\n",
      "0.62292\n",
      "0.685686\n",
      "0.685846\n",
      "0.369809\n",
      "0.192848\n",
      "0.271656\n",
      "0.0706097\n",
      "0.224669\n",
      "0.232427\n",
      "0.313201\n",
      "0.330195\n",
      "0.328461\n",
      "0.353229\n",
      "0.70623\n",
      "0.388637\n",
      "0.196625\n",
      "0.319552\n",
      "0.247579\n",
      "0.802285\n",
      "0.351618\n",
      "0.196047\n",
      "0.31134\n",
      "0.465664\n",
      "0.355448\n",
      "0.423541\n",
      "0.420173\n",
      "0.364432\n",
      "0.221633\n",
      "0.217738\n",
      "0.826733\n",
      "0.883085\n",
      "0.482727\n",
      "0.836394\n",
      "0.549342\n",
      "0.543821\n",
      "0.871666\n",
      "1.26761\n",
      "0.822592\n",
      "1.12861\n",
      "0.952444\n",
      "0.562329\n",
      "0.295794\n",
      "0.353345\n",
      "0.296079\n",
      "0.190844\n",
      "0.305054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.383346\n",
      "1.26052\n",
      "1.21918\n",
      "1.65239\n",
      "1.96905\n",
      "1.21905\n",
      "0.692263\n",
      "0.648336\n",
      "0.583709\n",
      "0.487303\n",
      "0.365086\n",
      "0.268434\n",
      "0.381203\n",
      "0.19074\n",
      "0.270575\n",
      "0.22866\n",
      "0.260582\n",
      "0.21415\n",
      "0.46682\n",
      "0.213275\n",
      "0.242425\n",
      "0.222149\n",
      "0.323104\n",
      "0.412365\n",
      "0.318738\n",
      "0.511704\n",
      "0.37769\n",
      "0.328758\n",
      "0.605685\n",
      "0.270896\n",
      "0.368469\n",
      "0.872077\n",
      "0.500848\n",
      "0.333277\n",
      "0.355157\n",
      "0.236641\n",
      "0.396475\n",
      "0.270975\n",
      "0.557038\n",
      "1.20087\n",
      "0.716262\n",
      "0.590922\n",
      "0.369809\n",
      "0.443444\n",
      "0.210228\n",
      "0.142136\n",
      "0.38845\n",
      "0.268226\n",
      "0.572403\n",
      "0.213127\n",
      "0.420915\n",
      "0.321199\n",
      "0.331774\n",
      "0.468275\n",
      "0.360097\n",
      "0.36893\n",
      "0.48859\n",
      "0.781123\n",
      "0.825365\n",
      "0.609127\n",
      "0.691824\n",
      "0.336673\n",
      "0.23955\n",
      "0.253616\n",
      "0.285282\n",
      "0.319148\n",
      "0.171339\n",
      "0.176282\n",
      "2.98845\n",
      "4.63749\n",
      "3.25062\n",
      "2.1657\n",
      "0.700113\n",
      "0.35932\n",
      "0.352643\n",
      "0.258468\n",
      "0.289241\n",
      "0.189397\n",
      "0.259286\n",
      "0.280421\n",
      "0.425424\n",
      "0.293707\n",
      "0.243392\n",
      "0.264045\n",
      "0.561145\n",
      "0.397524\n",
      "0.244595\n",
      "0.410076\n",
      "0.113019\n",
      "0.273081\n",
      "0.490155\n",
      "0.757023\n",
      "0.462545\n",
      "0.38576\n",
      "0.272659\n",
      "0.462994\n",
      "0.509254\n",
      "0.454478\n",
      "0.195248\n",
      "0.405864\n",
      "0.753527\n",
      "0.680638\n",
      "0.561549\n",
      "0.249702\n",
      "0.436762\n",
      "0.247753\n",
      "0.157776\n",
      "0.310788\n",
      "0.406358\n",
      "0.342423\n",
      "0.225324\n",
      "0.167841\n",
      "0.261674\n",
      "0.0926355\n",
      "0.228521\n",
      "0.174302\n",
      "0.288419\n",
      "0.14669\n",
      "0.195837\n",
      "0.300378\n",
      "0.181102\n",
      "0.252618\n",
      "0.341056\n",
      "0.338735\n",
      "0.309465\n",
      "0.28751\n",
      "0.298232\n",
      "0.110521\n",
      "0.26525\n",
      "0.197958\n",
      "0.25317\n",
      "0.338629\n",
      "0.561689\n",
      "0.962069\n",
      "0.444713\n",
      "0.150128\n",
      "0.179408\n",
      "0.249851\n",
      "0.241527\n",
      "0.437709\n",
      "0.369956\n",
      "0.4255\n",
      "0.176403\n",
      "1.48215\n",
      "0.561254\n",
      "0.486704\n",
      "0.243978\n",
      "0.251854\n",
      "0.540699\n",
      "0.271425\n",
      "0.229615\n",
      "0.418\n",
      "0.267712\n",
      "0.26459\n",
      "0.233714\n",
      "0.131266\n",
      "0.43692\n",
      "0.168978\n",
      "0.164554\n",
      "0.3275\n",
      "0.152336\n",
      "0.449315\n",
      "0.229627\n",
      "0.124296\n",
      "0.168364\n",
      "0.25329\n",
      "0.595592\n",
      "0.51545\n",
      "0.304508\n",
      "0.293903\n",
      "0.193283\n",
      "0.270827\n",
      "0.168028\n",
      "0.171694\n",
      "0.242406\n",
      "0.216718\n",
      "0.156251\n",
      "0.612028\n",
      "0.231744\n",
      "0.363342\n",
      "0.501055\n",
      "0.133636\n",
      "0.115131\n",
      "0.139099\n",
      "0.344544\n",
      "3.44674\n",
      "0.750152\n",
      "0.583821\n",
      "0.540961\n",
      "3.3636\n",
      "1.73501\n",
      "0.605353\n",
      "0.840599\n",
      "0.341491\n",
      "0.411144\n",
      "0.260642\n",
      "0.343088\n",
      "0.537633\n",
      "0.362928\n",
      "0.33533\n",
      "0.196529\n",
      "0.268576\n",
      "0.286296\n",
      "0.185771\n",
      "0.297247\n",
      "0.283811\n",
      "0.442883\n",
      "0.459001\n",
      "0.718894\n",
      "0.908028\n",
      "0.886211\n",
      "1.06217\n",
      "0.347874\n",
      "0.322917\n",
      "0.332047\n",
      "0.234749\n",
      "0.42085\n",
      "0.320248\n",
      "0.289724\n",
      "0.741981\n",
      "0.539068\n",
      "0.231819\n",
      "0.323507\n",
      "0.263474\n",
      "0.547168\n",
      "0.367725\n",
      "0.268792\n",
      "0.279806\n",
      "0.304718\n",
      "3.23055\n",
      "1.32124\n",
      "1.06653\n",
      "0.618788\n",
      "0.574893\n",
      "0.370263\n",
      "1.03402\n",
      "0.755523\n",
      "0.670485\n",
      "1.1137\n",
      "0.859838\n",
      "0.563765\n",
      "0.541583\n",
      "0.627961\n",
      "0.736809\n",
      "0.452365\n",
      "0.463813\n",
      "0.555379\n",
      "0.31727\n",
      "0.345251\n",
      "0.531731\n",
      "0.365024\n",
      "0.499127\n",
      "0.577063\n",
      "0.459365\n",
      "0.553403\n",
      "0.213314\n",
      "0.120642\n",
      "0.248253\n",
      "0.542599\n",
      "0.28895\n",
      "0.145437\n",
      "0.262915\n",
      "0.44147\n",
      "0.235496\n",
      "0.231627\n",
      "0.289304\n",
      "0.232807\n",
      "0.267611\n",
      "0.263645\n",
      "0.442997\n",
      "0.289375\n",
      "0.373517\n",
      "0.246964\n",
      "0.430127\n",
      "0.476467\n",
      "0.269688\n",
      "0.224657\n",
      "0.678016\n",
      "0.36547\n",
      "0.161866\n",
      "0.36917\n",
      "0.259858\n",
      "0.241265\n",
      "0.201236\n",
      "0.387257\n",
      "0.508396\n",
      "0.729914\n",
      "0.382478\n",
      "0.650128\n",
      "0.643255\n",
      "0.637444\n",
      "0.710619\n",
      "0.358029\n",
      "0.416504\n",
      "0.254915\n",
      "0.192999\n",
      "0.292279\n",
      "0.605641\n",
      "0.412476\n",
      "0.45496\n",
      "0.300071\n",
      "0.927442\n",
      "0.330866\n",
      "0.23181\n",
      "0.494882\n",
      "0.288065\n",
      "0.37922\n",
      "0.156729\n",
      "0.217781\n",
      "0.161597\n",
      "0.156379\n",
      "0.179429\n",
      "0.215644\n",
      "0.28191\n",
      "0.265266\n",
      "0.193671\n",
      "0.453886\n",
      "0.400447\n",
      "0.322189\n",
      "0.262727\n",
      "0.324625\n",
      "0.17374\n",
      "0.229\n",
      "0.158395\n",
      "0.118109\n",
      "0.226351\n",
      "0.277242\n",
      "0.204549\n",
      "0.425439\n",
      "0.329339\n",
      "0.530991\n",
      "0.232425\n",
      "0.128621\n",
      "0.384024\n",
      "0.122321\n",
      "0.232793\n",
      "0.296727\n",
      "0.276336\n",
      "0.40291\n",
      "0.314865\n",
      "0.139088\n",
      "0.413513\n",
      "0.212657\n",
      "0.610857\n",
      "0.643239\n",
      "0.365927\n",
      "0.359534\n",
      "0.520571\n",
      "0.759516\n",
      "0.527247\n",
      "0.582239\n",
      "0.374555\n",
      "0.381044\n",
      "0.295835\n",
      "0.241384\n",
      "0.320792\n",
      "0.323503\n",
      "0.311327\n",
      "0.198474\n",
      "0.34843\n",
      "0.207869\n",
      "0.141279\n",
      "0.171018\n",
      "0.220015\n",
      "0.150944\n",
      "0.541381\n",
      "0.504289\n",
      "0.561932\n",
      "0.557858\n",
      "0.354244\n",
      "0.216517\n",
      "0.197496\n",
      "0.294881\n",
      "0.304917\n",
      "0.211316\n",
      "0.155113\n",
      "0.140828\n",
      "0.258856\n",
      "0.280355\n",
      "0.244293\n",
      "0.30777\n",
      "0.199167\n",
      "0.091142\n",
      "0.231811\n",
      "0.591389\n",
      "0.650429\n",
      "1.07341\n",
      "0.300153\n",
      "0.396264\n",
      "0.268822\n",
      "0.129552\n",
      "0.97551\n",
      "0.237461\n",
      "0.632288\n",
      "1.60049\n",
      "0.736594\n",
      "0.512543\n",
      "0.29716\n",
      "1.41692\n",
      "1.05115\n",
      "0.906692\n",
      "0.862487\n",
      "0.480558\n",
      "0.446386\n",
      "0.342117\n",
      "0.402785\n",
      "0.466679\n",
      "0.315037\n",
      "0.535114\n",
      "0.292121\n",
      "0.189103\n",
      "0.429965\n",
      "0.173122\n",
      "0.311341\n",
      "0.300395\n",
      "0.385457\n",
      "0.332929\n",
      "0.176202\n",
      "0.218877\n",
      "0.417194\n",
      "0.131723\n",
      "0.254452\n",
      "0.289533\n",
      "0.335916\n",
      "0.323502\n",
      "0.245886\n",
      "0.27053\n",
      "0.254185\n",
      "0.278761\n",
      "0.32285\n",
      "0.215296\n",
      "0.193468\n",
      "0.445318\n",
      "0.2923\n",
      "0.331369\n",
      "0.12735\n",
      "0.28609\n",
      "0.358787\n",
      "0.266705\n",
      "0.539555\n",
      "0.391459\n",
      "0.350735\n",
      "0.124796\n",
      "0.21879\n",
      "0.410566\n",
      "0.581174\n",
      "0.229956\n",
      "0.256276\n",
      "0.194438\n",
      "1.20482\n",
      "0.470053\n",
      "0.184066\n",
      "0.190815\n",
      "0.299736\n",
      "0.356934\n",
      "0.25378\n",
      "0.4367\n",
      "0.388793\n",
      "0.217451\n",
      "0.375278\n",
      "3.46591\n",
      "2.12732\n",
      "1.11376\n",
      "0.836008\n",
      "0.351488\n",
      "0.279623\n",
      "0.696957\n",
      "0.435341\n",
      "0.194349\n",
      "0.26581\n",
      "0.597293\n",
      "0.620799\n",
      "0.455472\n",
      "0.226893\n",
      "0.182736\n",
      "0.392596\n",
      "0.235883\n",
      "0.453703\n",
      "0.221454\n",
      "0.336278\n",
      "0.239552\n",
      "0.183965\n",
      "0.316974\n",
      "0.193137\n",
      "0.51665\n",
      "0.201452\n",
      "0.233867\n",
      "0.195788\n",
      "0.258359\n",
      "0.462955\n",
      "0.241231\n",
      "0.319539\n",
      "0.256915\n",
      "0.27075\n",
      "0.387041\n",
      "0.344857\n",
      "0.696112\n",
      "0.237277\n",
      "0.168764\n",
      "0.292424\n",
      "0.300704\n",
      "0.327521\n",
      "0.216938\n",
      "0.471133\n",
      "0.729768\n",
      "0.985718\n",
      "0.714976\n",
      "0.281285\n",
      "0.257477\n",
      "0.435\n",
      "0.504699\n",
      "1.2825\n",
      "0.728351\n",
      "0.476778\n",
      "0.284127\n",
      "0.148029\n",
      "0.300383\n",
      "0.264443\n",
      "0.146723\n",
      "0.493526\n",
      "0.40009\n",
      "0.261295\n",
      "0.29128\n",
      "0.228703\n",
      "1.08015\n",
      "2.04844\n",
      "1.63028\n",
      "0.518486\n",
      "0.634332\n",
      "0.548391\n",
      "0.404725\n",
      "0.186958\n",
      "0.251544\n",
      "0.717792\n",
      "0.203204\n",
      "0.302355\n",
      "0.786956\n",
      "0.237481\n",
      "0.585128\n",
      "0.359326\n",
      "0.454451\n",
      "0.242424\n",
      "0.201686\n",
      "0.168754\n",
      "0.182934\n",
      "0.196162\n",
      "0.165715\n",
      "0.224684\n",
      "0.227234\n",
      "0.396387\n",
      "0.421842\n",
      "0.215076\n",
      "0.200552\n",
      "0.132621\n",
      "0.501562\n",
      "0.389734\n",
      "0.439431\n",
      "0.268079\n",
      "0.168459\n",
      "0.471642\n",
      "0.449696\n",
      "0.350896\n",
      "0.212256\n",
      "0.39987\n",
      "0.548135\n",
      "0.336395\n",
      "0.226187\n",
      "0.268535\n",
      "0.231049\n",
      "0.266132\n",
      "0.734959\n",
      "0.323085\n",
      "0.337043\n",
      "0.428646\n",
      "0.614241\n",
      "0.329597\n",
      "0.234728\n",
      "0.509297\n",
      "0.908498\n",
      "0.687364\n",
      "0.742545\n",
      "0.259742\n",
      "0.16867\n",
      "0.232331\n",
      "0.496643\n",
      "0.385261\n",
      "0.341795\n",
      "0.694226\n",
      "0.176649\n",
      "0.338866\n",
      "0.343846\n",
      "0.231475\n",
      "0.295042\n",
      "0.559816\n",
      "0.462495\n",
      "0.153005\n",
      "0.300508\n",
      "0.124944\n",
      "0.464344\n",
      "0.33438\n",
      "0.170476\n",
      "0.092709\n",
      "0.25899\n",
      "0.250189\n",
      "0.248486\n",
      "0.284801\n",
      "0.242844\n",
      "0.185685\n",
      "0.39299\n",
      "0.30367\n",
      "0.747748\n",
      "0.986432\n",
      "0.536658\n",
      "0.24984\n",
      "0.170251\n",
      "0.15487\n",
      "0.170551\n",
      "0.640194\n",
      "0.401394\n",
      "0.282838\n",
      "0.233942\n",
      "0.18655\n",
      "0.296088\n",
      "0.206699\n",
      "0.261781\n",
      "0.505718\n",
      "0.124963\n",
      "0.215518\n",
      "0.30297\n",
      "0.17221\n",
      "0.267732\n",
      "0.153948\n",
      "0.241457\n",
      "1.57668\n",
      "0.435521\n",
      "0.236883\n",
      "0.339046\n",
      "0.215283\n",
      "0.182124\n",
      "0.26608\n",
      "0.570671\n",
      "0.950196\n",
      "0.211057\n",
      "0.178142\n",
      "0.352214\n",
      "0.335693\n",
      "0.501974\n",
      "0.377023\n",
      "0.387652\n",
      "0.655324\n",
      "0.199372\n",
      "0.311438\n",
      "0.195474\n",
      "0.276327\n",
      "0.275397\n",
      "0.193798\n",
      "0.111083\n",
      "0.20282\n",
      "1.67806\n",
      "0.843092\n",
      "0.691875\n",
      "0.812516\n",
      "0.816716\n",
      "1.27591\n",
      "1.21301\n",
      "0.774725\n",
      "0.213071\n",
      "0.174292\n",
      "0.14914\n",
      "0.312661\n",
      "0.288654\n",
      "0.375864\n",
      "0.133048\n",
      "0.179773\n",
      "0.249684\n",
      "0.141193\n",
      "0.235195\n",
      "0.304438\n",
      "0.360868\n",
      "0.326787\n",
      "0.137233\n",
      "0.53233\n",
      "0.521614\n",
      "0.334926\n",
      "0.506162\n",
      "0.260624\n",
      "0.301166\n",
      "0.294316\n",
      "0.291214\n",
      "0.182258\n",
      "0.138787\n",
      "0.293011\n",
      "0.164897\n",
      "0.150219\n",
      "0.310241\n",
      "0.704895\n",
      "0.285949\n",
      "0.252427\n",
      "0.448932\n",
      "0.178865\n",
      "0.218177\n",
      "0.415499\n",
      "0.328281\n",
      "0.460917\n",
      "0.29142\n",
      "0.0900271\n",
      "0.59588\n",
      "0.30375\n",
      "0.276876\n",
      "0.138614\n",
      "0.166155\n",
      "0.555139\n",
      "0.206712\n",
      "0.262742\n",
      "0.268445\n",
      "0.428895\n",
      "0.379193\n",
      "0.666558\n",
      "0.602148\n",
      "0.520527\n",
      "0.447626\n",
      "0.441355\n",
      "0.679575\n",
      "0.707694\n",
      "0.427103\n",
      "0.393054\n",
      "0.396111\n",
      "0.327578\n",
      "0.63705\n",
      "0.219097\n",
      "0.246292\n",
      "0.1613\n",
      "0.28928\n",
      "0.169343\n",
      "0.148357\n",
      "0.298149\n",
      "0.327093\n",
      "0.296785\n",
      "0.409855\n",
      "0.313338\n",
      "0.136359\n",
      "0.113787\n",
      "0.264257\n",
      "0.250403\n",
      "0.139832\n",
      "0.29456\n",
      "0.313195\n",
      "0.237393\n",
      "0.186559\n",
      "0.34963\n",
      "0.708083\n",
      "0.502545\n",
      "0.324859\n",
      "0.395295\n",
      "0.833999\n",
      "1.39562\n",
      "1.36133\n",
      "0.511108\n",
      "0.414959\n",
      "0.770952\n",
      "0.348451\n",
      "0.202746\n",
      "0.229846\n",
      "0.306569\n",
      "0.31576\n",
      "0.189423\n",
      "0.176101\n",
      "0.3495\n",
      "0.287544\n",
      "0.316126\n",
      "0.449806\n",
      "0.197203\n",
      "0.427275\n",
      "0.153711\n",
      "0.142858\n",
      "0.208712\n",
      "1.60468\n",
      "0.651933\n",
      "0.289122\n",
      "0.474369\n",
      "0.770412\n",
      "0.246402\n",
      "0.433605\n",
      "0.261408\n",
      "0.326488\n",
      "0.353978\n",
      "0.16704\n",
      "0.198409\n",
      "0.385755\n",
      "0.220867\n",
      "0.415828\n",
      "0.219412\n",
      "0.288782\n",
      "0.587504\n",
      "0.857326\n",
      "1.04248\n",
      "1.64515\n",
      "1.34273\n",
      "1.19844\n",
      "0.373503\n",
      "0.335989\n",
      "0.181682\n",
      "0.26103\n",
      "0.393277\n",
      "0.305934\n",
      "0.221081\n",
      "0.443332\n",
      "0.274973\n",
      "0.293397\n",
      "0.244471\n",
      "0.618133\n",
      "0.200946\n",
      "0.189496\n",
      "0.172556\n",
      "0.173716\n",
      "0.437532\n",
      "0.354602\n",
      "0.275697\n",
      "0.173073\n",
      "0.442826\n",
      "0.184046\n",
      "0.435762\n",
      "0.16598\n",
      "0.15082\n",
      "0.526843\n",
      "0.18162\n",
      "0.184194\n",
      "0.183951\n",
      "0.675696\n",
      "0.451158\n",
      "0.153762\n",
      "0.257021\n",
      "0.176681\n",
      "0.131678\n",
      "0.249833\n",
      "0.147811\n",
      "0.115654\n",
      "0.385768\n",
      "0.155234\n",
      "0.200831\n",
      "0.237418\n",
      "0.169221\n",
      "0.298857\n",
      "0.198814\n",
      "0.267867\n",
      "0.308886\n",
      "0.432714\n",
      "0.29712\n",
      "0.14869\n",
      "0.315606\n",
      "0.432107\n",
      "0.833392\n",
      "0.512448\n",
      "0.507933\n",
      "0.186509\n",
      "0.362048\n",
      "0.15985\n",
      "0.195568\n",
      "0.23932\n",
      "0.278765\n",
      "0.240195\n",
      "0.147599\n",
      "0.257562\n",
      "0.339033\n",
      "0.476227\n",
      "0.270393\n",
      "0.213154\n",
      "0.230895\n",
      "0.227276\n",
      "0.280733\n",
      "0.150719\n",
      "0.218018\n",
      "0.256864\n",
      "0.120275\n",
      "0.199845\n",
      "0.388632\n",
      "0.423707\n",
      "0.262589\n",
      "0.399982\n",
      "0.484522\n",
      "0.661918\n",
      "0.147374\n",
      "0.151448\n",
      "0.137277\n",
      "0.271174\n",
      "0.407862\n",
      "0.470968\n",
      "0.388043\n",
      "0.184104\n",
      "0.413588\n",
      "0.124523\n",
      "0.20083\n",
      "0.298852\n",
      "0.245407\n",
      "0.369404\n",
      "0.20292\n",
      "0.556232\n",
      "0.280157\n",
      "0.300596\n",
      "0.104063\n",
      "0.160393\n",
      "0.2754\n",
      "0.281838\n",
      "0.394264\n",
      "0.272215\n",
      "0.384163\n",
      "0.403164\n",
      "0.255809\n",
      "0.53703\n",
      "0.431429\n",
      "0.199916\n",
      "0.647391\n",
      "0.439264\n",
      "0.422914\n",
      "0.29157\n",
      "0.363444\n",
      "0.3064\n",
      "0.225935\n",
      "0.204216\n",
      "0.221412\n",
      "0.121707\n",
      "0.548224\n",
      "0.471377\n",
      "0.462776\n",
      "0.202772\n",
      "0.214081\n",
      "0.319156\n",
      "0.169525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.199939\n",
      "0.170252\n",
      "0.227164\n",
      "0.153433\n",
      "0.548862\n",
      "9.42942\n",
      "1.0432\n",
      "1.03654\n",
      "1.13837\n",
      "1.13726\n",
      "1.43116\n",
      "0.77047\n",
      "0.493743\n",
      "0.48204\n",
      "0.243265\n",
      "0.365702\n",
      "0.387168\n",
      "0.547051\n",
      "1.12575\n",
      "0.638115\n",
      "0.439154\n",
      "0.409517\n",
      "0.337402\n",
      "0.373818\n",
      "0.281498\n",
      "0.333026\n",
      "0.281396\n",
      "0.475506\n",
      "0.347478\n",
      "0.457629\n",
      "0.489882\n",
      "1.29444\n",
      "1.17679\n",
      "0.453526\n",
      "0.29159\n",
      "0.351687\n",
      "0.258986\n",
      "0.490974\n",
      "0.122014\n",
      "0.288422\n",
      "0.251195\n",
      "0.618686\n",
      "0.242928\n",
      "0.221606\n",
      "0.117969\n",
      "0.149856\n",
      "0.179523\n",
      "0.251726\n",
      "0.336547\n",
      "0.2726\n",
      "0.13485\n",
      "0.486945\n",
      "0.847346\n",
      "0.336493\n",
      "0.393213\n",
      "0.756355\n",
      "0.221378\n",
      "0.154362\n",
      "0.266428\n",
      "0.197897\n",
      "0.481668\n",
      "0.339431\n",
      "0.190574\n",
      "0.186058\n",
      "0.228465\n",
      "0.121858\n",
      "0.319237\n",
      "0.165617\n",
      "0.13943\n",
      "0.249642\n",
      "0.361347\n",
      "1.52513\n",
      "0.797875\n",
      "0.533576\n",
      "0.31575\n",
      "0.361499\n",
      "0.451655\n",
      "0.267056\n",
      "0.258115\n",
      "0.159571\n",
      "0.471537\n",
      "0.331413\n",
      "0.119224\n",
      "0.301768\n",
      "0.740293\n",
      "0.504128\n",
      "0.402745\n",
      "0.14593\n",
      "0.298245\n",
      "1.10001\n",
      "0.398125\n",
      "0.207029\n",
      "0.292512\n",
      "0.363696\n",
      "0.621904\n",
      "0.163302\n",
      "0.186364\n",
      "0.243333\n",
      "0.31096\n",
      "0.4168\n",
      "0.324468\n",
      "0.316603\n",
      "0.304086\n",
      "0.177353\n",
      "0.243307\n",
      "0.571058\n",
      "0.269509\n",
      "0.269879\n",
      "0.613813\n",
      "0.388977\n",
      "0.576367\n",
      "0.215343\n",
      "0.407661\n",
      "0.297195\n",
      "0.152875\n",
      "0.152833\n",
      "0.453036\n",
      "0.19145\n",
      "0.234322\n",
      "0.229409\n",
      "0.322242\n",
      "0.261837\n",
      "0.184511\n",
      "0.214143\n",
      "0.283073\n",
      "0.495652\n",
      "0.354403\n",
      "0.21279\n",
      "0.183104\n",
      "0.215338\n",
      "0.290024\n",
      "0.229483\n",
      "0.200483\n",
      "0.281951\n",
      "0.355053\n",
      "0.222546\n",
      "0.334537\n",
      "0.411727\n",
      "0.56601\n",
      "0.1462\n",
      "0.215015\n",
      "0.22273\n",
      "0.112472\n",
      "0.218643\n",
      "0.273476\n",
      "0.394794\n",
      "0.426657\n",
      "0.291213\n",
      "0.262086\n",
      "0.151326\n",
      "0.177475\n",
      "0.277355\n",
      "0.359357\n",
      "0.399904\n",
      "0.150399\n",
      "0.127738\n",
      "0.261656\n",
      "0.277005\n",
      "0.145372\n",
      "0.247074\n",
      "0.198751\n",
      "0.116676\n",
      "0.299897\n",
      "0.358721\n",
      "0.301466\n",
      "0.233343\n",
      "0.229768\n",
      "0.225299\n",
      "0.212724\n",
      "0.654141\n",
      "0.305669\n",
      "0.191301\n",
      "0.198138\n",
      "0.113538\n",
      "0.0859287\n",
      "0.126013\n",
      "0.347924\n",
      "0.233136\n",
      "0.188984\n",
      "0.329381\n",
      "0.368993\n",
      "0.165777\n",
      "0.196798\n",
      "0.307384\n",
      "0.37492\n",
      "0.592487\n",
      "0.455852\n",
      "0.157564\n",
      "0.258434\n",
      "0.178296\n",
      "0.210974\n",
      "0.324455\n",
      "0.388828\n",
      "0.334706\n",
      "0.290084\n",
      "0.322946\n",
      "0.205002\n",
      "0.24081\n",
      "0.171602\n",
      "0.162706\n",
      "0.234718\n",
      "0.19161\n",
      "0.280902\n",
      "0.379211\n",
      "0.172276\n",
      "0.586388\n",
      "0.200088\n",
      "0.288757\n",
      "0.307957\n",
      "0.275247\n",
      "0.412937\n",
      "0.24292\n",
      "0.143299\n",
      "1.05172\n",
      "0.300162\n",
      "0.842956\n",
      "0.338622\n",
      "0.249888\n",
      "0.37127\n",
      "0.27856\n",
      "0.591408\n",
      "0.535281\n",
      "0.238769\n",
      "0.275598\n",
      "0.382989\n",
      "0.136509\n",
      "0.239332\n",
      "0.278771\n",
      "0.261093\n",
      "0.16405\n",
      "0.151647\n",
      "0.271509\n",
      "0.168127\n",
      "0.217266\n",
      "0.388688\n",
      "0.181497\n",
      "0.272537\n",
      "0.290826\n",
      "0.287261\n",
      "0.201062\n",
      "0.194331\n",
      "0.209952\n",
      "0.42178\n",
      "0.232055\n",
      "0.361693\n",
      "0.192311\n",
      "0.285306\n",
      "0.247534\n",
      "1.07074\n",
      "0.366988\n",
      "0.448538\n",
      "0.327928\n",
      "0.261792\n",
      "0.433865\n",
      "0.336972\n",
      "0.202155\n",
      "0.292463\n",
      "0.412737\n",
      "0.385803\n",
      "0.165028\n",
      "0.340201\n",
      "0.312717\n",
      "0.287689\n",
      "0.251426\n",
      "0.188797\n",
      "0.626657\n",
      "0.362666\n",
      "0.348416\n",
      "0.617845\n",
      "0.293339\n",
      "0.226378\n",
      "0.246497\n",
      "0.215839\n",
      "0.28472\n",
      "0.36708\n",
      "0.200111\n",
      "0.458256\n",
      "0.252116\n",
      "0.256919\n",
      "0.677637\n",
      "0.465947\n",
      "0.940291\n",
      "0.552548\n",
      "0.491907\n",
      "0.726027\n",
      "0.265115\n",
      "0.213042\n",
      "0.200466\n",
      "0.535237\n",
      "0.644002\n",
      "0.284652\n",
      "0.139147\n",
      "0.166497\n",
      "0.308206\n",
      "0.322347\n",
      "0.341328\n",
      "0.428862\n",
      "0.308052\n",
      "0.294213\n",
      "0.955395\n",
      "0.519199\n",
      "0.185524\n",
      "0.214314\n",
      "0.1381\n",
      "0.259313\n",
      "0.190121\n",
      "0.212916\n",
      "0.661482\n",
      "0.444458\n",
      "0.270408\n",
      "0.184849\n",
      "0.200634\n",
      "0.250614\n",
      "0.113136\n",
      "0.102623\n",
      "0.343717\n",
      "0.167067\n",
      "0.582041\n",
      "0.528204\n",
      "0.705787\n",
      "0.708045\n",
      "0.474293\n",
      "0.432003\n",
      "0.332922\n",
      "0.551818\n",
      "0.358763\n",
      "0.291384\n",
      "0.311992\n",
      "0.746884\n",
      "0.425201\n",
      "0.47026\n",
      "0.566825\n",
      "0.301561\n",
      "0.275766\n",
      "0.183372\n",
      "0.15284\n",
      "0.167422\n",
      "0.142617\n",
      "0.23318\n",
      "0.244997\n",
      "0.359335\n",
      "0.315309\n",
      "0.634569\n",
      "2.16466\n",
      "1.48086\n",
      "0.555183\n",
      "0.159997\n",
      "0.3686\n",
      "0.241218\n",
      "0.292367\n",
      "0.177323\n",
      "0.318856\n",
      "0.247406\n",
      "0.209323\n",
      "0.219707\n",
      "0.237707\n",
      "0.175519\n",
      "0.396233\n",
      "0.207545\n",
      "0.211028\n",
      "0.283023\n",
      "0.323012\n",
      "0.594836\n",
      "0.498846\n",
      "0.982039\n",
      "5.97813\n",
      "13.3756\n",
      "5.42287\n",
      "3.59348\n",
      "1.22972\n",
      "0.690633\n",
      "0.93967\n",
      "0.830576\n",
      "0.863221\n",
      "1.16353\n",
      "0.772642\n",
      "0.774801\n",
      "0.371648\n",
      "0.561968\n",
      "1.33657\n",
      "0.240566\n",
      "0.363901\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)\n",
    "\n",
    "zipped = list(zip(datasetX, datasetY))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "loss_history = list()\n",
    "for _ in range(10000):\n",
    "    datax = list()\n",
    "    datay = list()\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        samp = random.choice(zipped)\n",
    "        datax.append(samp[0])\n",
    "        datay.append([samp[1]])\n",
    "    _, l = sess.run([optimizer,loss], feed_dict={input_pl: datax, output_pl: datay})\n",
    "    print(l)\n",
    "    loss_history.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 4: standardizing data\n",
    "\n",
    "\n",
    "\n",
    "Last section we made our first multi-layer neural network. Let's first try to see how well our network performs on the test dataset we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.466203\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIf4/Wk9fwdq1uaoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f585d951ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "0.461872\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f585841fda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def reset_and_train_network(dataX, dataY, batch_size, train_x_rounds, verbose=True):\n",
    "    init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "    sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "    sess.run(init)\n",
    "\n",
    "    zipped = list(zip(dataX, dataY))\n",
    "\n",
    "\n",
    "    loss_history = list()\n",
    "    for _ in range(train_x_rounds):\n",
    "        datax = list()\n",
    "        datay = list()\n",
    "        for _ in range(batch_size):\n",
    "            samp = random.choice(zipped)\n",
    "            datax.append(samp[0])\n",
    "            datay.append([samp[1]])\n",
    "        _, l = sess.run([optimizer,loss], feed_dict={input_pl: datax, output_pl: datay})\n",
    "        if verbose:\n",
    "            print(l)\n",
    "        loss_history.append(l)\n",
    "    return loss_history\n",
    "\n",
    "## Do the train set\n",
    "def evaluate_network(dataX, dataY):\n",
    "    datasetX = dataX.values\n",
    "    datasetY = dataY.values\n",
    "    datasetY = [[x] for x in datasetY]\n",
    "    predicted_values, loss_dataset = sess.run([outputnetwork,loss], feed_dict={input_pl: datasetX, output_pl: datasetY})\n",
    "    print(loss_dataset)\n",
    "    plt.scatter(datasetY, predicted_values)\n",
    "    plt.show()\n",
    "\n",
    "evaluate_network(X_train, Y_train)\n",
    "print(\"-\"*10)\n",
    "evaluate_network(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why standardize data\n",
    "Scikit preprocessing library: http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "tf.Transform library: https://github.com/tensorflow/transform\n",
    "\n",
    "At the moment we are feedign the raw properties of atoms to the network. Especially in classification you often see that you can already kind of draw a separation between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0          1          2          3          4         5  \\\n",
      "1720  73.516695  20.815717  18.769616  18.214832  17.604923  14.30713   \n",
      "\n",
      "              6          7          8          9  ...   1265  1266  1267  \\\n",
      "1720  13.091671  12.793909  12.678041  12.574113  ...    0.0   0.0   0.0   \n",
      "\n",
      "      1268  1269  1270  1271  1272  1273  1274  \n",
      "1720   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[1 rows x 1275 columns]\n",
      "[-0.36737947 -0.18380304 -0.21721598 ..., -0.07035703 -0.05191918\n",
      " -0.05197147]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIV6RrMx37wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58580d0320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = reset_and_train_network(X_train.values,Y_train.values, 32, 1000, verbose=False)\n",
    "plt.plot(losses)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "res = scaler.transform(X_train)\n",
    "print(X_train.head(1))\n",
    "print(res[0])\n",
    "\n",
    "losses_norm = reset_and_train_network(res,Y_train.values, 32, 1000, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f585dafdcc0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4HNXV/z93i7TqXbKau9x7BdsYG2NqEiCUBBJaQggJ\naZCe35v+prwJCQnpdEICoYbejQHbGIx7712Wrd7blvn9cWe2SLuSbK1Wlnw+z6NH0u7s7Ozu7HfO\n/d5zzlWGYSAIgiAMfGz9fQCCIAhCdBBBFwRBGCSIoAuCIAwSRNAFQRAGCSLogiAIgwQRdEEQhEGC\nCLogCMIgQQRdEARhkCCCLgiCMEhwxPLJsrOzjeHDh8fyKQVBEAY869atqzQMI6e77WIq6MOHD2ft\n2rWxfEpBEIQBj1LqUE+2E8tFEARhkCCCLgiCMEgQQRcEQRgkiKALgiAMEkTQBUEQBgki6IIgCIME\nEXRBEIRBggi6IPQDPp/Bkx8dodXt7e9DEQYRIuiC0A+s3l/Fd57ZzKq9lf19KMIgQgRdEPqBjUdq\nAWjz+Pr5SITBhAi6IPQDlqC7vSLoQvQQQReEGGMYhl/QPV6jn49GGEyIoAtCjCmra6WioQ0Aj08i\ndCF6iKALQozZZEbnAO0SoQtRRARdEGLMxqMBQfeIhy5EkW4FXSlVrJRarpTarpTappT6unl7plLq\nTaXUHvN3Rt8friAMfDYdqWV0bjIgHroQXXoSoXuAbxqGMQE4C7hdKTUB+B6wzDCMEmCZ+b8gCF3g\n9RlsOVrHrGE6/nGLhy5EkW4F3TCMMsMw1pt/NwA7gELgMuARc7NHgMv76iAFYbCwt7yRpnYvM01B\nlwhdiCYn5aErpYYD04EPgTzDMMrMu44DeVE9MkEYhFgTojP8gi4RuhA9eizoSqlk4BngG4Zh1Aff\nZxiGAYQNNZRStyql1iql1lZUVPTqYAVhoLPxaC2pLgcjspJw2hVun0ToQvTokaArpZxoMf+3YRjP\nmjefUErlm/fnA+XhHmsYxr2GYcwyDGNWTk63i1YLwqBm4+FaphanY7MpHDabROhCVOlJlosCHgB2\nGIbx+6C7XgBuNP++EXg++ocnCIOHlnYvu040MK04HQCHXeEWD12IIo4ebDMfuB7YopTaaN72A+DX\nwJNKqc8Dh4Br+uYQBWFwsL2sHq/PYEqRFnSn3SaVokJU6VbQDcNYCagIdy+J7uEIwuClvtUNQFZy\nHAAOm5IsFyGqSKWoIMQIS7ydNv21c9ptYrkIUUUEXRBihNe0V+w2PeB12JVYLkJUEUEXhBhhReNO\nuynoYrkIUUYEXRBihNfMObcidG25SIQuRA8RdEGIEZZ4O+36a6ctF4nQheghgi4IMaJjhO6wSYQu\nRBcRdEGIEVaZv8NuWS7ioQvRRQRdEGKE14zGHWbaosMmhUVCdBFBF4QY4ekQoUvpvxBtRNAFIUb4\nBT0oy0UidCGaiKALQozwdLJcxEMXoosIuiDEiHARumS5CNFEBF0QYoTHa2BTYAsp/ZcIXYgeIuiC\nECM8PsNvt4CZ5SKWixBFRNAFIUZ4vD5/hgvoPHSxXIRoIoIuCDHC4zP8VaIglosQfUTQBSFGeHw+\nfx8XkNJ/IfqIoAtCjPB2iNDjHOKhC9FFBF0QYoTba+AMtlxs4qEL0UUEXRBihNdn4Ai2XOw2PD4D\nw5AoXYgOIuiCECPcXp+/qAjwR+syMSpECxF0QYgROkIPznLRXz/x0YVoIYIuCDHC7TWwBxUWWWuL\nuqVBlxAlRNAFIUZ4fT6/iEOgp4tE6EK0EEEXhBjRubDIslwkQheigwi6IMQIj9fAGdZykQhdiA4i\n6IIQIzw+X2iEbpMIXYguIuiCECM8nbJczAhdPHQhSoigC0KM8HiN0Dx0y0OXLBchSoigC0KM8HSs\nFJUsFyHKiKALQozwdKwUNcVd+rkI0UIEXRBiROdeLlL6L0QXEXRBiBFuX2iEbmW5SIQuRAsRdEGI\nEd5Ok6LioQvRRQRdEGKEO1JzLslyEaKECLogxAivz/DbLBDIcpE8dCFaiKALQoxwe0MrRZ3SPleI\nMiLoghAjvD4jtNuiP8tFLBchOoigC0KM8HTsh+7PcpEIXYgO3Qq6UupBpVS5Umpr0G0/UUqVKqU2\nmj+X9O1hCsLAx92xH7o/y0UidCE69CRCfxi4KMztdxuGMc38eSW6hyUIgwufz8Aw6NAPXdrnCtGl\nW0E3DOM9oDoGxyIIgxZrmTmnvbPlIhG6EC1646F/RSm12bRkMiJtpJS6VSm1Vim1tqKiohdPJwgD\nF68ZhYeL0CXLRYgWpyrofwNGAdOAMuB3kTY0DONewzBmGYYxKycn5xSfThAGNtbEZ9jmXJLlIkSJ\nUxJ0wzBOGIbhNQzDB9wHzInuYQnC4MKK0MP2Q5cIXYgSpyToSqn8oH+vALZG2lYQhIBPHtxt0W5T\nKCUeuhA9HN1toJR6HFgEZCuljgI/BhYppaYBBnAQ+GIfHqMgDHg8YSJ00BOjkuUiRItuBd0wjGvD\n3PxAHxyLIAxaLFslOELX/yvcHonQhegglaKCEAOs8v6OEbrDpmSBCyFqiKALQgzwWy72DpaL3SYL\nXAhRQwRdEGKAJ0zaImiBlywXIVqIoAtCDAhYLh08dJtN8tCFqCGCLggxwLJc7J0sF4nQheghgi4I\nMcASbWfHCN1uk37oQtQQQReEGOC3XOyds1ykH7oQLUTQBSEGRJoUddptUikqRA0RdEGIAf5eLmEK\niyQPXYgWIuiCEAOsXPOwpf8SoQtRQgRdEGKAN0JhkeShC9FEBF0QYoA7QnMuh12acwnRQwRdEGKA\nN0JhkdOmZFJUiBoi6IIQA6zURLuU/gt9iAi6IMQAy0N3dspykdJ/IXqIoAtCDLBslY4RurZcJEIX\nooMIuiDEAI8/Qu88KSoeuhAtRNAFIQZ4InjoTruSLBchaoigC0IM8ETy0G0SoQvRQwRdEGJAJA9d\nslyEaCKCLggxwBOhsMgpWS5CFBFBF4QY4PH5sNsUSoVZJFoidCFKiKALQgzw+IxO0TmY7XN9BoYh\noi70HhF0QYgBHm8kQde3SQtdIRqIoAtCDPD6jE690CHQH11sFyEaiKALQgxwe31hI3TrNpkYFaKB\nCLogxAAdoYf30EEidCE6iKALQgxwe41OrXMhsOCFFBcJ0UAEXRBigNfnCx+hmyLfLoIuRAERdEGI\nAW6f0alKFIIjdLFchN4jgi4IMcDrNfzReDD+LBeZFBWigAi6IMQAq1K0I04ry0UidCEKiKALQgzw\n+IxOvdBB8tCF6CKCLggxwOPt2kOXPHQhGoigC0IM8Ph8YStFLV9dInQhGoigC0IMiNTLRfLQhWgi\ngi4IMcAToZeL02+5SIQu9B4RdEGIAR5fpF4uluUiEbrQe7oVdKXUg0qpcqXU1qDbMpVSbyql9pi/\nM/r2MAVhYNOd5SJpi0I06EmE/jBwUYfbvgcsMwyjBFhm/i8IQgQ83TXnkiwXIQp0K+iGYbwHVHe4\n+TLgEfPvR4DLo3xcgjCo8PoiNOeySem/ED1O1UPPMwyjzPz7OJAXpeMRhEGJ2xuhOZcZobvFQxei\nQK8nRQ29GGLE8EIpdatSaq1Sam1FRUVvn04QBiTeCGuKOmQJOiGKnKqgn1BK5QOYv8sjbWgYxr2G\nYcwyDGNWTk7OKT6dIAxs3N4IS9BJlosQRU5V0F8AbjT/vhF4PjqHIwiDE2+EtEWnZLkIUaQnaYuP\nA6uBsUqpo0qpzwO/BpYqpfYA55v/C4IQAU/EFYsky0WIHo7uNjAM49oIdy2J8rEIwqAlUtqiQ9rn\nClFEKkUFIQZEqhSVRaKFaCKCLggxwBMhy8VuUygllosQHUTQBaGP8foMDIOwWS6go3SxXIRoIIIu\nCH2MVTQUboEL0MvQSdqiEA1E0AWhj/GaRUPhlqADHblLYZEQDUTQBaGPsSY87WHSFkELvZT+C9FA\nBF0Q+hhrwjNihG6zSZaLEBVE0AWhj7HslEgeusOuZJFoISqIoAtCH2MJujOi5SIRuhAdRNAFoY/x\ndJPl4rApyUMXooIIuiD0MVaEHq70X99uo90jEbrQe0TQhajzyb+u4r8bjvb3YZw2WHZKuOZcoCdL\nJUIXooEIuhBV3F4f6w/XsvFwbX8fymmDJdYRI3SbEg9diAoi6EJUaW7zAlDb4u7nIzl9CETokS0X\nyUMXooEIuhBVmto9ANSJoPsJeOhdWS4SoQu9RwRdiCpNbVrQa5tF0C2sLJeIEbrNJr1chKgggi5E\nlaZ2bbnUS4Tux+rlEknQdem/ROhC7xFBF6KKP0IXQffj7i5t0WaTLBchKoigC1HFEvS6FjeGIVEn\n6AWiIXLaosMuWS5CdBBBH+A8v7GU9Ydr+vsw/DSblovXZ9BoivuZjtvbdS8Xp90mvVyEqCCCPsD5\nxcs7eHjVwf4+DD/BIi4To5pAP/QIEbrkoQtRQgR9gFPb4j6tIuHm9sCxSOqiprsVixyDZAm6z9z/\nAY9+cKi/D+OMRgR9ANPq9tLu8Z1Wgt5oFhaBCLpFdysWDZbS/48O1LD+0Olj/52JiKAPYCzBbGw9\nfQS9WSyXTni68dAHwwIXrW4v7V4flY1t/X0oZzQi6AMYS9Cb2k8fQW9q96JM3aptae/fgzlN8HTj\noQ+GJeisUWJVo3zm/YkI+gDmdIzQm9o85KW4ALFcLCw7pasViwZ66X+DeQ5WNUmE3p+IoA9g6kxL\n43Ty0JvbPWQmxRHvsPmP70zHslMirVjksNnw+owBnbdvBRVVje0D+nUMdETQBzBWBNzm8Z02Q/bG\nNg/J8Q7SEpzioZv4I/QuJkWBAZ3p0tCqP2uPz6C+5fQJMM40RNAHMMGWRtNpEqU3t3tJjLeTnugU\ny8XE000vF6sL40DOdGkIOv8qxXbpN0TQBzD1rQHBbDhNfPSmNg9J8Q7SE+JkUtTEb7lEnBTVtw/s\nCD1w/snEaP8hgj6ACY6ATxcfvanNS1KcnVSxXPxYEXqEAN1vuQzkFroNQcFFlaQu9hsi6AOY09Fy\naWr3kBjnID3RKS10TTxeH067QqnIeejAgM50Cc60qmySCL2/cPT3AQinTrBgNpwGgm4YBk3mpKjD\npqSFronXZ0RMWYRAW93TZWL7VGho8xBnt9Hu9UmE3o9IhD6AqWtxk50cB5weEXqbx4fPgMR4O2kJ\nTprbdWuCMx2314iYsgjBlsvAjdAbWj2kJjjJSHSKh96PiKAPYOpa3BSkJwCnR3GRdVFJjteWC0hx\nEeh+6JFSFiHYchm4F7+GVjepLgdZyfFSXNSPiKAPYOpa3BSkmYJ+GkToTWZjrsQ4B2mJeuRQJ5ku\nuH1GxMUtYHDkoTe2eUh2OchKiqNSIvR+QwR9AFPX4iY/XZfZnxaCbvaUSYrTlgtIgy4Ar9eImIMO\nQRH6ABb0hlYPKS4H2cnx4qH3IyLoA5Q2j5dWt4/MxDgSnPbTwkO3jkHnoYvlYuH2+SKuJwpBk6ID\n2HJpbNWT4dnJcVRJlku/0assF6XUQaAB8AIewzBmReOghO6xhDIt0Umyy3GaROjackkyK0VBInTQ\nWS5dRehWYdHAjtDdpLicZCXHU9vsxu31RSykEvqOaKQtLjYMozIK+xlQvL7tOHXNbq6ZXdwvz2+l\nLKYlOEmOd4QsLNFfNAdF6GkSofvxeA1/eX84LLEf6GmLyfEOssysq5qmdnJTXf18VGcecgk9RR5a\ndYA/Ld/Tb89vCWWqJeit/S+c1ighKc5BisuJUkguOjp7pUsP3V/6PzAF3WcuCJ7qcpCVFA8gE6P9\nRG8F3QDeUEqtU0rdGo0DGigcr2vleF0rvn6q7rM62qUlOEmKt/szTPqT5nYry8WO3aZIdTmpa5Yv\nto7Qu7JcBnYeerPbi2FAssvhr4uQlYv6h94K+gLDMGYAFwO3K6UWdtxAKXWrUmqtUmptRUVFL5+u\nlzRXw11j4ND7vdqNYRiU1bXi9hr9duLWhVguztOiUrQxyHIBfWyD2XKpbGxjy9G6brfz+AzsXaQt\nDvQ8dKuPi+Whgyx00V/0StANwyg1f5cD/wXmhNnmXsMwZhmGMSsnJ6c3T9d7qg9A4wk4sqZXu6lp\ndtNmVkAeq2uNxpGdNKGCfnpkuTS3e7DbFPEOfVqlJzoHteXyp2V7uO7+D7pd0MHj8+HsclJ0YOeh\nW0VtwR66VIv2D6cs6EqpJKVUivU3cAGwNVoH1ic0levf9aW92k1ZXYv/72O1LV1s2XeECPrpkuVi\ndlq0mlAN9kUuDlQ109Dqoaab1+jxdtfLZWBH6PWmoKe4HKTEO4iz28RD7yd6E6HnASuVUpuANcDL\nhmG8Fp3D6iOaTMunrpeCXhuIyvtT0BPj7DjtNpLiTxdB9/jtFtCCPpg7Lh6tbgbCnANeD9Qc8v/r\n8RldpvDFmSOa02Ee5FSwzr0UlwOlFFnJcVJc1E+csqAbhrHfMIyp5s9EwzB+Ec0D6xMazQi97kiv\ndlNWrwVdKThW23+Wi5UamBLvoN3j6/dGWM3tXhLj7P7/O1ouP3lhGw+vOtAfhxZ1fD6DozVayK3f\nfrY8BX+eDS01gOWhR47Q81NdZCfHseZAdZ8db18S7KEDWtCluKhfOLPSFpvMdPleWi7H61pw2BQj\nspL6NUK3BN2KivvbR7fWE7WwJkUNQ08e/3P1QZ5Z37v3/nShorGNdjPNsLTjOVC9H7xt/pGg1Q89\nEjabYuGYHN7bU4G3t1lTrfXQVNW7fZwkwR46QFaSlP/3F2eYoJsRenMVuE9diMtqW8lLdVGYkRDi\np8eSuhY3qaagW1+k/rZdms3FLSzSE+LwmjnKb2w7gc+AXScaBmy+dTBHTLsFoLRjhN5sBg6NJ4Du\n+6EDLBqbS22zm01Ha3t3YK9+Bx67pnf7OEkagjx00BG6eOj9w5kl6JblAr3y0cvqWslPc1GYnkBp\nP1ku9UER+uki6E1t3lAPPaj8/9WtZQC0e3zsr2jql+OLJkdqtKDHO2ydR2nWXI15vrm9vi4rRQEW\nlmRjU/DOrp6l9n7pX+u46/Vdne+o3A1Ve3u0D4CWdi9vbj/R7Xat7sj+fkObB6V0QRmgG3Q1tXWb\n/SNEnzNL0JsqISFD/11/9JR3U1bXQn56AvlpCVQ2ttHmif1kVrDlkuw6TQS93UNSfMBDt47vUFUz\nq/dVcf74XAC2Hes+d/t052i1FvFpxemdLRfL8giK0LuqFAVIT4xjWnE67+4q73I7gIqGNl7bdpzl\n4batL4PW2h6PQJ9Zf5Qv/HMtu080RNxma2kdU3/6Biv3hO/w0dDqJjnOgc18jVlJcbS6ff5Csz6h\ntQ5e/ia0RT7uM5EzTNDLoWC6/rvu1ATdKirKT3NRYLauPd4PuejhPPR+F/Q2bwfLRR/fM+uP4vEZ\nfHnxaOIdNrYfq++vQ4waR2qayU2JZ2ROchhB7xihd90P3WLR2Fw2l9Z16z8v31WOYcC+isbQSmWf\n138R8f/uhr3ljYAW7Uj8+tWdtHl8bImwTUOrxx9UAIHior60XQ6sgI/uh0Or++45BiBnjqB7PbpS\nNH+q/v8ULZdas6hoSKrLv1pQpy90H+P26ugn1RXIcoH+X7VIrycanOWii0xe2VJGYXoC04vTGTck\nhe1lg0DQq1soykigMN1FdVM7LcHRaBgPvbsIHWDR2BwMA97b07XtsmyH3m+r28ex4DmcxnIwzONo\nON6j17GvQgv6tggX2fd2V7Byr349ls3UkUazF7qFVVxU2ZfVovXH9O+Gsr57jgHImSPozVWAAamF\nkJR7ypaL9QUqSA8IelmMffRAp0X9JTodsly8PoMWd2iEbo0g2jw+Lpo0BKUUEwrS2F5WP+D91aO1\nzRRnJlKY0eGi7nX70xUtQfd00w/dYlJBGtnJcV366K1uLyv2VDI2LwUIRNgANBwL+rtnQmfNZ4Sz\nwbw+g1+9upPizATGDUkJmQgOpqHNHZLdlJ0UgwjdylTr4UjkTOHMEXQrwyU5F9KKTtlyseyVIWkJ\n5KdpyyVaqYst7V7ufGJjt/sL7oUOp4eH3twemroG+HuiA1w8aQgAEwpSqW12U9ZPLROigcfr41ht\nK8UZiRSmJwJBgt4clDJoWi6eHkboNptiYUkO7+2OnL744YFqmtu93LpwJNBR0IOi8oaeTXQeq2tB\nKdh+rPNF9rkNpewoq+c7F45jVE5y53x7Ex2hBz7rQPm/ROix5gwSdDPqScqBtMJTtlwsIcpPc+Fy\n2slKiotaP5f1h2t4dkNpt5kOwWX/EMgu6F9BNzstBlkuLqedeIeN3JR4ZgzVk9ET8lMBBrSPXlbX\nitdnaMvFjND9F2Gr1iEpNxChd9MPPZhzx+ZQ0+xmc4T0xWU7TpDgtHPplHwyEp3sC84Yqj+5CP1A\nZROGAXNHZFLf6gkR7Fa3l9+9sYspRWlcOjmfoswESmtawnYX7eihZyaZgt6XxUXW6+uhtXSmcOYI\neqMl6LmQVqwj9FMY9peZRUXZ5sRPQXpC1CJ0y8+M5FVadBR0u02RGGeP6KE3tXmojtKXK5JV0hTU\nCz2YsUNSuHpWkT8DYtyQFB0RDmAf3RK+4sxE8lLisdtUIBfdChzyJuhsE09bt/3QgzmnRDewW72/\nc3GQYRgs21HOgpJsXE47o3OT2RcSoZeBskNKQY+sCMtu+fjUAiD0M1m2o5xjda1884Kx2GyK4oxE\n2r0+TjR0Dl4azF7oFi6nnZR4R992IrUsFxH0EM4cQfdH6NnaR3c36S/cSVJWp4uKrEKR/DRX1IqL\nrOFzJK/SoqOgg/bRrUWaO/LjF7Zx3X0f9Pr4jlQ3M+nHr7P+cE2n+6w+JMF56ADPfXk+31w6NuQ4\nR2QlDegI3brgFmUk4LDbGJLq6my55E3Sv5squu2HHkxmUhwFaS72nGjsdN/O4w2U1rawZJxO/xyV\nk+wPAgCdspgyBFILehShW4+9eFI+NhU6MbpsxwnSE53MH5UFwNBMbS0drup8bja0hnroAPnpLjYc\nru2buRLDCLJcRNCDOYMEvRzsceBK05YLnLztsvlJhpW9zpC0wNJaBel6KBr2xG2tC5tWVd3Uzq9e\n3dGpWMP6gkXyKi3qg1YrskiJd/gr9jry0cFqdp1o8Pvcp8qW0jqa2r28vrXzl8i6mCQF9XIB7Qvb\nOkSn4wtSB3aEXt2MTeGfFNcFZh0i9NwJABgNJ7rth96R0Xkp7CnvnF/99k7tyZ9nCvro3GSqmtqp\nsUZfDccgJV+Leg889P0VjRSkuchMimNkTjLbzYlRr89g+a5yFo/N9VtFxaagH+lwbrq9PlrdvhAP\nHeCmeSPYeKSWN3pQtHTStNSApxXikvVIxHd6NDXbdbyB2x5dR30/rh52Bgl6pfbPldKWC5zcxKhh\nwJs/5pK6x/2ToaCzXZravf4WoiGsuRceujjU2wRe2FjKP97dz4cdmjHtK9dD4KMnabmAGaGH8dDr\nWtwcqmrGMGB3mKjvZDhYpY/PSmMLpqnD4hZdMSE/lcPVzf164veGozUt5Kcl+DsoFqS7giyXSm17\n5OhRic+MILvqh96Rktxk9pY3dvKrl+04wdSiNP9anaNykwHYa0Xp9WWQmq9FvQcR+v7KJv8+JuSn\n+kdN6w/XUNPsZolZCGa9RqU6jx479nGxuGZWEaNykvjNazvxRLvVg/V9Kpiu0zSbY9u7JhL3rdjP\na9uO88iqg/12DGeOoDeWa0EHbbnAyaUu1h6ChmMUeo+Rnxrnv9mfuhjOdqnaBxiw/52Qmzcc0VZP\ncDFHQ6ub4/WtpLocVDZ2yGvuQF2LG5fTRrwjEA0nR2ihG5yOtrOXUfGhymZzn/WdPPmmdstysXd6\nXEcmFOiJ0R0D1HY5UtPsnwwFKMxI4Hh9qxaupgpIzNJRMuAzI2V7Dy0X0ILe6vZ1mqTcfLSO+aOz\n/beNztFi7PfRG8q0f56S1221qGEY7K9oYmR2EgATC1I5VtdKTVM7b+04gcNsGGYR77AzJNXVaX4n\nuHVuMA67je9cNI59FU08ufbUq7LDEizocFpkujS1eXhlSxlKwf0rD/g7UMaaM0fQm8p1yiJAch7Y\nnCcXoZvWSaJqY5QrMBy2BD3sxKjVE3vf2yE3bzisBT04k8GaoDrH/BJ1FaUHV4laJEWwXLaVatGM\ns9vYebx3ZdIHq5r8RUzv7wuN0pvNL3ZiXPcR+kQr02WA2i5Hqlsozkj0/1+YnojXZ1De0KajxaQc\nf/Dga9A2ifMkLJcSM8c82HbZUVaPx2cwtTg96HkTiHfY9NxLWyO01QcidOhyYrSioY3GNg8jzYuC\ndZHdXlbPsh3lzB2Z6S9csyjOSPS3PLCo97fO7fy5XzAhj5nDMrj7rd29tvtCsPLtC2eY//e/j/7q\n1uM0t3v54aUTqGtx88/VgX745Q2t3PzQmm5H3tHgDBL0ykCEbrPpE/9kPPTDgXVIR6pARFCQZgl6\nmNTFmoP69/53wFyNprKxjcOmB7u1NCBo1oToIlPQu8p0CSfoKa7wk6JbSusoSHMxoSCVXb0U9ENV\nzSwZn0uKy9Gpr0fH9US7IjfVRXZy/ICcGG3zeDnR0EpRhwgdzFz0pgpIygJHvO4bZIpqd90Wgxlt\n2iB7gjJYrLL7KUVp/ttsNsXInGRtuVhRakoBJOvRQVdCZ6U7jsyxInS931e2lLG3vJEl4/I6PaYo\nM6FzhO7vtOjstL1Sih9cMo6KhjYeXBnFPvj1x0DZAlXfp4GgP73uCMOyErl5/nDOG5fLfSv209jm\nobyhlWvv/YAP9lfHpPZiwAq62+vjwZUHuOOJjd23YzUM84sWtKZpWvHJ9UU//AENGXqiK98TiOxz\nUuJx2FTnCN3dqiOJjBH6uU/o1fk2mtH5eePyKK1t8VsX+yoacdiUf0jd1cRo+Ag9fNri1tI6Jham\nMW5ICjuPn3qFZku7l+P1rYzKSebskVms2FMZsi8rD73jpGgkJhWmsvFIL1vF9hCvz+CPb+2JSoR0\nrLYVwwhMEgIUmj19SmtaQgOH5Dy/oHfVD70jaQlO8lLjQxpmbTpSR3ZyPENSXSHbjs41M10sQU/N\n99s9XQkS4dIdAAAgAElEQVTd/kp9sbAi9MykOPLTXDxl2iPnj+8s6MUZiRyvbw1pRtcQwUO3mDks\nkwWjs6PbB7++VL+3qUXmQfSNoK85UM3Lm8PYOXVHYcdL/n+PVDfzwf5qrppRhFKKry0pobbZzd1v\n7ubaez+grK6Vh2+ezezhmX1ynMEMSEFfsaeCi/+4gp+9tJ3/bijtXhha68DbHiroqYU9X7moqRIq\nd7M3ZymNhovMlsBwym5T5KW6Ol99aw/r3zNv1L/3Lwdgw5EaHDbFdXP1xKwVee2raGR4dhL5aS7i\nHbYuUxfrWjydBD053tlpCbOGVjf7K5uYXJjG2CEp1DS7qWg4tdzgw+bxDM9OYkFJNqW1Lf7bQHuI\n8Q5bjwtoZg/PZE95Y9Ty47tiw+Ea7n5rN/e9t7/X+7I+l+KgCD2kp09TJSSaPndyLsqsFu3p+2JR\nkpsSUgW6pbSWKUVp/vVaLUabFZztNaZgpgRZLl0JekUTLqeN/KALxIT8VNq9PkpykxmaldjpMUMz\nEzGM0P7vkTz0YJaMz+VAZVPYlMdTot7M5nHE6fmKxr4R9J+/tJ07ntzYOZ9+5d3wxGf8tS3Pri9F\nKfjkTH2BmVaczrljcnhg5QFTzOcwd2RWnxxjRwaUoFc1tvGVx9Zz/QNrcHt93P2pqdgUEdt6+rFS\nyZIDs/akFeqsgJ6kPB3W/vkO5yQOGvkkNoQOH0PS1ixqTdEfejbkjPf76BsO1zI+P5VZ5tV6i+mj\n7y1vZFROEkopCjMSOFIdOUKvD1rcwiI53k671xcSPVmWxuTCNMYN0R7pqfroByr1EH14VpJ/FBGc\n7aJb55pf6q3PQHXXQ+yzRurX/9HBvl92bZmZ7vfq1uNhKx1PBmvkVBQUoSfGOchMiuN4dR201QUC\nh6RcbM36uU/GcgEoyQtkujS1edhb3sjkwrRO243KTcIwoOb4QX1DSj4kZuo5oi4mC/dXNDIiOzkk\npXSi6aMvCROdQ/jURWvyL7kLQT/XtBHf7arpWNU+2P585PuDqS/TufZgZvREX9ArGtrYUlpHu8fH\nvz44FHpn2Sb9+8C7+HwGT68/wrxRWRSmBy7y375wLJML03j45jnMGdH3kbnFgBB0wzB4cdMxlt79\nHm9sO8GdS8fwxh0LuWJ6EZMK0zpN0HXCWtgiKZAhQFoR+Nyhi15E4tBqsMez0TuCY44iVIcFBArS\nXZ0jass/zxgOoxbDodV425rZfLSO6UPTSXU5GZ6VyJbSOtxeH4eqmv3eaXFGIkdrQ/fX6vbywqZj\n3PDgGkprW8hKigu5P9nfoCsg6FtNQZ9YmMq4IXqi7VR99ENmyuLQrERGmiOJVUGC3txmridaexie\n/hy8/v+63N/kwnTiHTY+3N/3gr58Zzkup43yhjbWHupcFHUyHKlpxmFTnayPgnQXDVXmJGSSGY0l\n52FrKgeMk7JcQEfoze1eSmtb2F5Wj8+AqcWdBd06Z5oqj0J8KsQn69TclCG0VB/jT8v2cOcTG7ni\nr6u4+aE1/pTX/ZVNfv/cYsYw3Z7hIrPvTkeKM7VgBZ/rDWaE3nECNZgR2UkUZSTwblctLVbeDU/e\nCBW7I29jUX8skKmWMqTHWS4ns+buu7v1sY7ITuJfHxwKBEo+LxzX9in73+Gjg9UcqW7hKjM6t5hU\nmMaLX10QUzGHASLoP31xO199fAPFGQm89LUFfG1JiT9lb/7obDYcrg3NwTYMva6jRVNQ2b+F5b/1\nxEc//D4UzeJQnYfqhKFatNwBi2VqcTplda2hHm3NQXC4tNc36jzwtnFs89s0tnmYPlRnKkwuSmdr\naT2Hj1fg8RmMMv3M4swEjlQ1w8bHoEafTBf94T2+9vgG9pU38rXzRnPbuaNCDjEpTAvdraV15KXG\nk5viIiMpjtyU+B5F6OX1rfz57T0h+cMHq5rJTIojLcGJUtrrf39flb+JlH890W3/1Q/Y/WqXk85x\nDhszhmaw5mDf5hAfrWlm5/EGbjt3FHEOG69sOfUUN6/P4L3dFQzLSuwUcRemJ9BaZ0aKfg89F5un\nhSRaT6qwCHSEDnrktvmotuUmhYnQh2clYVPgrSsNWC2AkTKEXXt387s3d7N6fxXxDhsr91bypX+t\no7HNw5HqZkZlhwr6uWNyePfbi5gWlEkTTF6Kizi7LWRitKHVg8OmiHdEfn1KKc4dk8PqfZWRRbVi\nF2DA+3+MuB/AzOap03MFoCeAe1BE1e7xsfiud/ja4xtCRmker4+fvritUxS+fFc5uSnx/OyyiVQ2\ntvPCRjOzpnIPeFr0d3v/O7y98wRxdhsXTgx/EYw1A0LQl07I43sXj+OZL81jjJnSZTF/VDYenxG6\nYvraB+Ge6XDIzEwJbsxlkWYKenepi22NULaZtsK5bDhcS3zuGCD0gjFvlI78V+8LEqeagzo6VwqG\nzQN7HI3b3wRgerGOhCYXppJTt4UR909ghtrtj7aKMhIZ1bYdnvsS3LeYPR+9ycGqZn522URWfGcx\nd14w1r+IgEVKmI6LW0vrmFQQEIGx5sRoVxiGwbef3sxdb+wO6SdyqKqJYUG+6jkl2dQ2u/157s3t\nZoS+9Vn9ug0D1v+zy+eaMyKT7cfq+7TAaLlpt3x8agGLxuTw6tayU7ZdHn7/INuO1fON88d0uq8w\nPRFPg3me+T10bV3kqNpAYVFjz5aYK/FnujSw+Wgt+WkuclNcnbZzOe0UZybiaDweEDmgknQS2ir5\n5RWTWf39Jfzn1rP59Sen8P6+Km555CN8RmBC1EIpxbCspI5P4cdm03ZgcOqi1Qu9o7ffkYVjcmhq\n97Iu3AjJMPTSecoOm57oOvvMP/kbFKH3oFp0zYFqSmtbeGHTMX7+8nYMw8DrM7jjyU08tOogv3pl\nh3/04vH6eG93BYvG5rBgdDZj81J4YOUBnQRwfLPe4bTroO4Ipfu2M6kwtUfpurFgQAj6/NHZ3Hbu\nqLATS7OGZxDnsAWG/w0n4K2f6r83P4nb62PPgf0YKD2BYmEJek036VRHPwLDy3pjLO1eH2Mnmbmv\nVXv8m4zJSyYrKa6DoB+C9GH677gkKJ5LWtkKMhKdfmGcVJjG5faV2AwPV9nf83/BijMSucT+IT5b\nHCRkMP6Nz3CN4z2umF7YqYzeIjleD3ktQW9u97CvojEkqhufn8qe8sYuK/ee21jqH26uCJqbOFTV\nzPCgL/v80dk47Yr/fHTE/7wjbCegbCPMvgVGnw/rH9ELi0Rg7ohMfAbhv+ToysjvP7u508Spx+vr\ntt+Nxds7yxlu2kSXTsnnRH1b2F403VFa28Lv3tjForE5fGxKfqf7x+WnkOwxJ+eDInSAHOp0RH/g\nPbhrtL7odUN6Yhw5KfHsOdHIlqN1Yf1zi3mjsnG1nqDSps9vwzD4sCKefFstV88KWAFXziziG+eX\n8IFpc3W0XHpCUUZCyGR4Q6s7bMpi52PMwmFT4RfvaKrQhVBnfQkMH6z+S+QdWSNqv4c+RFeLNgXZ\nrjte7LQ03Vs7TuBy2vjM3KE8tOog9763n28/vYkXNx3j07OLaWr38sRHOpFh/eFaGlo9LB6bi1KK\nzy0Yzs7jDfr7XbYJ7PEw51YAMsvfZ6ZpVZ0ODAhB7wqX086sYRmsssT09R/oIVHxXNjxAve+s4s1\nW3bRaE/DsAWl1CWka8E9tqHTPv+5+mBgsu7walA2njpRQE5KPOMmmtVplQFBV0px1qgsVu+v0ldx\nwwhE6BajFlPQuo8lBe3+aGZSQQoX29cAcKljDckOHTkWpbu42L6Gyrz5cMtbbHFM4jeOv5Oy7q8R\n3werQtOynnaYvmuwEIzNS6Hd4+OgmW1wsLKJR1cf1EUfXg/VpXv56YvbmTE0nTkjMv2CbvXNDo7Q\ns5Pj+fTsoTz50RGOVDfT3O7hnPaV+s6JV8Csz+loavdrEY95+tAMHDYVOroC8HnZdqyO2x9bz+Nr\njnDJH1ew1vw8Vu2t5JJ7VnDOb5bz/MZSnd9/dG3Y/Te3e1i1r4rF4/QX87xxuabtcnKTaIZh8KPn\ntmIY8PPLJoWNRhePzSVLmaOfIA8dzAjdbtOTxQAv39kjm6AkN5n1h2vYX9kUkn/eke9fVEKOquXl\nAzql9Y3tJ9jRmEgqjTh9oRfDry8p4coZRSQ47Z0i9J4wNDMxxHLxW23dkOJyMmNYRngfvcJc7HrU\neTD5alj3sF5dLBxWlWiwoEMg06VsEzzxWXj7f/0PMQyDN7efYMHoHH5+2SQunZzPr17dybPrS7lz\n6Rh+feUUzh6ZxcOrDuL2+nhnV7lOIS7RI63LphWSlRTHAysP6P3nTYSccbQnFXAWW5g5LLY+eVcM\neEEHHS3uKKunbuvrsPVpWHAnzPsqNFexdvnzFDgbOeZO5sm1HdIUC2dC6fqQm07Ut/LjF7bx3ac3\na3/40Pv4cifx6p5mLpyYh82Voos3qvaFPO7skVmU1bVqsWyuhvaGEEFvKLkcn6G4Ri3z35Zavp48\nVctz3nmk0ehvETC8bSeFqoodGedRayRxdeOd7MpeCm/+KGJ0Z1ku1iTVljC+69igidHKxjY+c/+H\n/PD5bSz67Tuse+a3pN83m5nt6/i/K6dw7pgcdpTVU96g5wYMQ08QBXP74tHYbIp7lu2hqc3LnKZ3\noPgsPfopuUAPi9c+GPFzS4izM6UoLSDojRXw8Mfw/HU+tz2yhvSEOB6+eTbxThufuvcDPvWP1Xzm\n/g9pcXuZUpTGt5/azL53/wX3L4E9b3Xa//t7q2j3+PxFMikuJwtLTt52eWXLcZbtLOfOpWNC8s+D\nyUmJZ3xqGx7s4DI9aFPQs1UddmXAzlegcJYuyX/pG922by7JTfYXAE0pCu9rA6R6a3HgY19bKj/4\n7xbufnM3PvO5/ULn80JjOUop7rp6Ciu/u7hHQtyR4sxEapvd/uyW+g7Lz3XFuWNy2G6eUyFUmoKe\nMxYWfEN3Ql1zb/id1AelZwb/tjJdrKrsdQ/7Ex52lOkuldcOOYKtpZrfXTOVy6YV8N2LxvG1JSUA\n3HLOCI7VtfLKljKW76pg5rAM/0Svy2nnU7OLWb7rBL6yzZA/BZTiQOps5tm2MbM4tUevPxYMGkGP\npx37q9+CzFGw4A58o86nWSXwCccHzM/34UnI5kfPbwv1kAtn6lz0oGjp1S1lGIbOAnhlw0E4+hFH\nUqfR4vZy0UTz5MkeHWK5AJxtthldva8qNMPFZH19Kst8M5hW/hx4zLzW7c/jVk5+6r6BFnuKP4JL\n3f8ybsPOasds3t9Xhdtw0HTxn3QK5H9vg8MfdnoPOi5Dt7lUF6LkpQa89tG5ydhtis1Ha/nio+uo\nbGzjN1dOoSgjgcYtr2DDx1/i/0KJs4KFZl/uVXsrOWj2cOnorw5Jc/HZucN4dkMpqQ37KWjbB5M+\nqe+0O2DGjbBvWZcpjHNHZrH5aC2tRzbCfYsxDq3CUbmDyc2r+cf1M1k0NpcXv7qAiyYOYfPROr51\nwRjevONcHrl5DkUZCWx+17zAffj3Tvt+e1c5SXH2kEyDS6cMoayu1d9Ppzve31vJQ089yzW5pdw8\nf3iX245PbafKSKHCWnotMRND2clRdaTXbNLtJ+beBkt+BLtegU2Pd7m/0UHzRZML0/R5Vb2/84XA\n9JXPmjqJlzeXsfN4A+fMmGzeZwrdq9+FP0yGss0opTrNwfQUq+WBlVYb13yczLiedTu00hdX7O6Q\nlVaxW3dOTC2E3PEw9hL9eXrC1CjUl0FCJjjNFEF/EZXpre9brkXe2w7v/wnQdssC2xaWrL4Jnvkc\nLqedP356Ol9aFEgsWDw2l5E5SfzhrT3sKKtn8bigBArgksn5FFCBra3OX6G6wjuRdNVETuPOrl+4\n16OPpb2p6+2iwMAT9LUPwT8W6rUbTSYXpnGd632Smw7DpXeB08Vj68t5zTOTS53riGs6zqjhI0hN\ncPLlf68PTBwWzdK/jwWi9Je3lDEmL5kxecksf/tV8LSyrHUsaQlO5pq502SN1pZL0BdrZHYSeanx\neiKx9qC+MUN76FWNbfzo+a0867yEuLZq2Pactgp2vEBp1jxqSKV0yBJdfeZuQe14ng3OaextcLJi\nTyUp8Q6mDM+DTz+mo9/HPx2axUMgbbGx1cPRmmZe3lzGwpLsEHvA5bQzPCuR+1ceYN2hGn53zVSu\nmV3MM1+YwYK43exKX0h8nAP+8xkmZtvISNTPb3VZHN6x2GTf23xlso84u+JCtUrPU0y4LHD/jOv1\nRNeqyJkLc0ZkMt/YgPPhi/D5vPxfwZ8oNbL4Sd5Kf9+SVJeTv0w/yraRf+IrC4fhctrJSIrjoZtm\nMZetOire+yZUBtJJDcNg+c5yzinJIS4oA2PJ+DwSnHZ+9PxW6pq7nox9b3cFX3h4NX91/J5f+e7u\ndpGK4vgmqow0/0QsNjtuVxY51JJ99C2wOaBkKcz9Egybr0XWKkALgzUxWpyZQIajDf5xrp7sv3sS\nPPtFOKLtOuq1mF141nQWjc1hSlEac6foqmYajuuRz/p/6pazT90U6i8fWaNTTD09KzizUhcPVzdB\nczX31t3Gr0tv0CMxb9fv54T8VL1m6u4OtkvlLqoThvG5R9bqltLTP6tb5B4J08M/OGUR/KOgw4cO\n4G5tgsMfwMRPwqQr4aMHoLmaD7ft5h7XP8CRoEfBe5d12q3Npvj8ghH+eovFY0MFfWJBKuemmHbP\nkKkYhsGTVeYFwWq+d2wjrLqn84Xow7/BG//TqadTXzCwBL2tEd7+ufaxDq7032y3Ka5M2MARlc9r\nzeO4+83d/N+rO9mfu5Q4dz3UHcGVPoQ/XTudg5VN/M9/t2ive8gULTil6wC9XuhHB2v42JQCbl88\nmqLadRgoHjqSz5Lxuf52qWSV6EmcoLadSinOHpnF6n1VGNUH9Y3pw2hp9/L5R9ZyvK6VL9x4s37s\nmnv1c9aXoiZcrredfJW2ad67C2oPsy19EUdrmlmxp4KzRmXpCeHETPjMU3ri6OVvhrw1wcvQ/eLl\nHSgF37pwLB0Zl5+K12fwjfNL+NgU7UOqI2uwe1sZe/GXUVc9CBU7sb3wFeaPymKlKehpCU7SE4Ny\n3w+sgEevIPPh+byfeCeftb9FadqMQMQE2uecexusewi2PB32I505NJ0fOh7luC2XS1p+xv0HMjk6\n6jpyKz+E8h16o+ZqeOkObIdWwoF3/Y8dZquggAoe5HLaDTtbn7sLr89g3aEarr1Pl1xfMDG0SCbV\n5eSvn53BnhONXP/gh/7Mho4s31nOLY+s5fq0TeQaldgbj8GJbWG3tUjx1tJoT2PZzsCIr82VTa6q\nJfPwGzBioZ67sdngsr8ACh6/Vp/XYbAyuqYUpmsLobUWzv0uFM+GPW/APy/XImI2q7KnFfDQTbN5\n5kvzsFsec8Nxfb552+Hjf9RJAC+ads+m/8DDl8LqP+vItgcMz04iOd7Bt57azFtP/4NEWml2ZsJL\nd8Bf5sB/PgP3nQe/nwgr/xDyWJtNceHEIby+9XhIIZ6vYher6rJ4e2c5f3p7D4w4VxdG7XmDY7Ut\n/Oa1nXz/2S187fENlB7Zhzsp6DO1O2mNy2DF+i289OKz4G3TdR/nfBPcTTS+ew83VfyONKMBbnoZ\n0ofCWz/291YK5pPTi8hIdFKQ5mJMXucMoIuzyvEYNupSSzhc3czupgRqUkp0QdRTN8G958KbP9Ti\nbVG9H97+hR51jPtYj97j3jCwBP2j+7WI2pyw44XA7a11jG/dwMvumdz27w3c8/YeijITueZTN+kF\nLQCSsjlrZBbfOH8Mz208xlPrjkJcol4qzJxUe9nMUb50Sj4fm1LAYtcudjKMI60uLgrOM83Wvhsd\nCozmjcqmsrGN+rI9kJSD15nE1/+zgU1Ha/njp6czY1gWzPkClK6FZT8Fm5Nh867kwx8sYfTsi3W6\n28q7weagvGApe8obOVrTwjklQQVRWaP0ybrv7ZDFM2w2RVKcnWU7T/Dq1uPcvmi0vyQ9mJvnDedb\nF4zh66Z3COi2BDYHDF+gJ6aW/Ai2P8en0nZQ3tDG2zvKO0fnH/xNZw1d+juSiqcQp3zUjP9s589s\n6U+1r/7CVwMCHURqzTZG2cr4c/P5qJQhPP+V+cy98g6dSWD5qMt+piM2ZyJsfy7w4APvAXDlTXew\nPnkRw448x0X/9wpX/u199pY38dNPTOTyaYWdnnPx2Fz+9tkZ7Cir54YH13RKmzxQ2cTtj61nTF4S\n30p+I9A/v4sJXgDVXElcWh4r9lT6C1HaXNnMsO3BVX8Axl0a2DhzBFz9EJRvh/9+MazAZCbFcfXM\nIq6elgur/wrDz4HFP4CrH4Yvr9YX+MeugSMf6cAkWU/+Oq2Lv82pBeWj+7SgzLxJP37r0/CvT+rn\nLZ4LcSnaAuoBqS4n//3yPBaOySZt77Ps8hXx6JRH4bon9flQtVcXONkdsPaBTvbQlxePBuCet0zL\nsrUeW0MZO9z5zBmRyd/f3c/WSi8Mn49n1xtcd98H3Pveft7cfoLNR2uJbz7OR9WB87qqsY2D7ank\nqRpqt76OYY/TacK542H8J0hccw9L7euoPvsHUDQTzvsRHN8CW57q9NoS4rQV86srp4Sd+J5sP8Re\no5C399X7M7OMEefqzK7db8DC7+gMrzX/0AGMYeiLp80Bl9ylU5j7mIEj6G2N8P49MGqJ/mLseDGQ\ne7r7DeyGhylLruOFr8xn+08v4tWvn8PQ3HQY93G9jZlCdvvi0cwblcWPnt/KnhMN2kc/th58Pl7e\nfIzx+amMyknG7m1jirGLVZ7xJDjtIb2hydInZXCmCwR89OYT+2hPGcrnHv6IN7af4EcfmxCovpt6\nrfYLD67Q4ulKIy/Vpb8AEy/XKVgjFpKVk+cv2lkQ1AMb0CdNch4s/0XIzUnxDraW1lOcmcAXzFXh\nOzIruZqvFB8MPWH3LYei2RBverZnfwVSi5hT9igAx+paQ/3z6gNaAGbeDLNvIe6zT5D6k1ImX/S5\nzk9od8I1j+h9P/FZ3VcnmM1P4bM5Gb/kBp6/fb7u+peUpUcsm/6jh8frHtaR/riPwc6XA0P7A+9B\n8hCyhk1i7qe/T4pq4WrnCr594Vje+84ibpw3PGKa55LxefzluhlsK63jhgcCFZRtHi9ffXw9TruN\nh89z4zixCc65U/fe3vNG2H35aaokK7eA5navPzWwNT6bdGV6p2MvCd1+9BK48Few8yU98gzDb6+e\nyiL3ezoKn/+NwB0pQ/Rozd0Km/9jtoQOyuIyq0XZ8C99MZz/NX37gm/CyMU6IJh5E1z/X30cu18L\ne1EJR0leCn+9OJPZtt3sz7+Uiybnw5gL4Za34PYP4YbnYP7XtZ1UGVr5WZiewGfOGsrT64+yr6KR\npjJ9kU8smMB9188iKymObz21iaahi3FU7cLRcJQnbzubtf9zPu/cMY9sVc/qinid4QT8+tWdHPdl\nMDu7nbnGZg4nTtJpwgALv43N8LLGPo3s87+ub5t0pfbA3/7fkOJAi4Vjcjh3eKIW/W3PhWTBpdZu\nZ59jFK9uOc7aQzWkxDtIP+8OOP8n8PWNcN7/g4t+ree6XvgqvPUTPaJc+pPAKml9zMARdCs6X/Q9\nmPAJnbt62PTYdr4IyXnMO/diphSlkxDc8W/yVfq3GWXZbYo/fGoayfEObn9sPW1506G1jhOHtrP+\ncG0gx7h0LXZfOwdTpnPRpCG4nEH7TB+ql7PrMDFanJlIUUYCRs0h3ixzseZANT+/bCI3zx8R2MiV\nClM/rf8O9ptBp2wBTLyCInPyqTA9oVN2CXGJOpPn4Ap/lAqBfho/vHRC6PFalO+AB5bqqC7Yzijb\npL/kFnYnzPsK8aUf8PFM7fEODz6GNfdq8Zh9S+fnCEfKEB1VVh8ItYp8Xtj6NLYxF3LDedNCvG7m\n3AruZm1JpAwJfO4tNdpuMwz92kcsBKVQxbOhYAa3xr/F7eeO7FGhxwUTh/C3z85k27E6PnP/B9Q0\ntfOb13axtbSe3141hezN9+moc+q1UHKh9pubIlS2ulugvZEhBcW4nDbe3nECt9dHmVePEFtypwdS\n7YKZ+0V9YVz5e9jw7873+3zal82dqIU3mNzx8Ol/6Ug83L5ThuiMkaI5MPQsfZvNBp96FG58CT72\nB/1Zj7tUF+ccW995H5EwI9yLr/ta+Ayckgv0792vd7rry4tGE++wcfebu1n9gR5lLj13AWmJTn5x\nxWR2Hm/gs+/q9+1vc6uZMdTM8zYnPuMzi/if57by4iY90s7IKyat6SATbId4praE42ajvLdq8rjC\n87+8PeV3KOtiZ7PB0p9B3WF48euBBAbDgF2vwX1L4JcF8PcF8NSN+v8N/4KG46jGE9jyp/Du7gre\n31vJ9GEZ2NKLYMEdgT5Rdqc+1+OSYdUftLjPDBPo9BEDQ9CDo/PiOfpkscdr28XdqlPWxl6iP6yO\njFoMt74LIxf5b8pNdXH3p6axp7yRW815im0f6T/8gn5gBaD4/m238OsrJ4fu02aHzJEhk3AWC0am\nkeuroD11KK9/YyHXnz288zHN/wZMv14LVDBDz4JblsG0z/gnnxaMzg5fhTfzJp0++fYv/MPa0TnJ\nLJ2Qx9KxmbDnTX2CWtFs9X7tudqd+mR76yfm63wXMPT7FMyMGyAhg9uduk2o33Jpa9An+MQrQioT\nu2XYPFj4bS0E1iTSgXe1kFgXsmAKpmk7wNsGF/5SXwhHnw/OJO1ZVuzUWSMjFgYeM/eL+iIbbMuA\nfg9W3RN2AnLphDzuvWEWu0808vE/r+SBlQe48exhXJDXoNsXzL5FZ1SMuVC/T3s7pEdaloJZ2OJM\n0dWF/91Qytm/epuX9+tRpIrknyoFl/xWn58vfq3zxNneN6Fih454w50HIxbC9c/Chb/ofJ+VumhF\n5xbxKTDinMD+Rp+vLZse2i4YBmx+QltAaUXht0kr0hehMKOanJR4Pjd/BC9tLuPgzg14sDN2vM4c\nWWazfIQAAA3HSURBVDohj09MLWBTaw7NiYWU1AdNjJo56FefNwevz+Crj28gP83F+JIx0K7nIVYa\nk/njsj38ZflevvDoWjx50/n8eR2+vyMXwVlf1tbTH6fpoOHeRfD4p/Q5tegHcNVD8IXlMPJceP52\nPUcAFE+cR5tZyzErUkFRyhC45p/6QvqJP4XXpT5iYAh6cHQO+oQcfb62XfYv11FIVxMOBdM6fRnO\nKcnh0c/NpSJ+GE1GPEe2rGRyYVrAWji4EvKnkJSWFbLUm5/8qfq5yzaH3PytuUk4lI/LF88L24IU\ngPRiuOzPAYsjmKJZYLMzKieZOcMzQyr9QnC6YOE3dSbAirtgy9P8Y+ZR/pHzNOruCfDvq/QJ+vsJ\n8MYP4ZHL9MTYDc/rXN/dr8HBVdpuiU+Fghmh+49LgjlfZFzdSkrUUcabqwyx8TG9Ms7cL4U/rq5Y\n8A2dyvnKt3UmwOan9HOPuSj89hf+Cs77ob54gBbWkqXaorAm8YIFfdJVkD9N7z84kn77f/Vk1VM3\nha1cXTw2lwdvnM2oxvX8OP01fhj3mPaX7fEw+wt6o/xpuhfQnqCI8/X/B3+eDbVHoNlMxUvK4bJp\nhbS4vcwcls7Hz1+CYY/HNfWKyO+L3akFIGccPHGDHu6DPreW/0L3HbLSQcMxYmEgAg9m2DwdIXa0\nejqSmKm33dlDQS9dr73yKdd0vV3JUl2Y19q53cQXFo4k1eVgqO8InvQR+j0wuevqqbz9zcUkTrxY\nX/z9ab563ixv6Dh+8omJ2BT8+OMTicswRyeudKbMWsjjaw7z29d38YmpBTx129nkpIRJ0bzoV/CN\nLXo+6siHesL5E3+Gr66HRd/V73fhDLj2CZ01Y17sxk+f52+M12WF6LCz4ZY3A/NtMeL0aEDQHdaw\nt3hO4LYJn4BdL+sTPj419IvdQxaUZPPi1xdR9efJzK49QMLZZqm+uwWOrvGX94Zl6c90FP+f6/SV\nPFl77NluPSxUGSMiP7YHuJx2nrzt7K43mn4DrLnPXxWnAGWP0wI59Vp9EVv/qC6ljkuCG1/Qw/T0\nYbDmfl2o1FSuIy17mFNhzq3w/j08P2Etibmf1yOSD/+uI4+imSf/opwJcPFv4bGrYcXv9AV5wmX6\n4hSOopmdn2fCZToCf/9P+uJgpoYC+jVc/led3vfqd+CqB/RIZdUf9AWrdJ0e6Z1zZ+g+fT4WHPkH\nC+y/gVZgXYIWuSU/9H+u2Gx6ZLjzRX1R2PKUzg5BwaNXaGEASMzm4+MKuHRyvunfz4IFFwbypiPh\nStOe+P3nw7+u0u0DTmzR1t5lfw0RvB5z9u36pyeMvQRe/74eyWWGn3/xs/kJfbHraBl2ZMyF+r3f\nv7zTtmkJTn5++SRmvVaBy1p5yCTOYdMWX8kFOpg7tEpfFD78mz4ns0ZxTRZcOGEIaYlO2GmOFEee\ny5eXjGXNoXoum1bArQtHdt1jJrVAf8bn/U/kCUtHHFz5gA7CGk5gT0jjgolDeHrdkZDlAE8XBoag\nz7he/wQz5iLtHR7foiMzR1z4x3aD3abIHTeP3A//zoRp5hD16Ec6mh1+TuQHpgyBax+DBy/Wk303\nvqCXHbP6oAcLTV/hiNN2UsMxbSt42vSJlxAUOYy9WFfMedsDw+O4RFj8fT1xAzDva533DXpycsYN\nJK65F375nN4H6IvZqTLmAj2aevfX+v8pYeyWrii5QHe6azimbaGO5E3U1s47v9RR0vJf6qH/za/o\noqx3fqXPnTwzT7utUUfjO1+CaZ+Fi38dfuQEWqA2/kvnXL/5Ixi2QI8a/321f0hutWgOmYztTswt\nUgu0qD/8MS3gl9ylJ/ESY1BaPvZiLei7XoOzvxx5u5YaXQA39uJABlkkiuZAfJq2XSxB3/SEHuHN\n+QKXTcqB50shO0KkP/wcM9vpPh08Fc6CCwLWUlqieZGz8tJHnUduiotXvt7F9zYc3WWfWL67yXcv\nGsvVs4pOqdK2rzn9jqinJKRrL2zvmzC+l/mdRbPg/XYdERXO1CePsmlB6IqC6XD5X3T/73sXa7+3\n5pCOqlJjM6uN09V9RJWc2/m2qdfB+3/WZdcjF3e+32LBnfpLnDJEWwJDpsCQSb075ot+pbNXXGld\nXzTDEZ+s7badL+l85XCcc6eO/l/+pvbcr35Yi+qlv9NW2nO3wZIfaz98x0tQf1RnJ8y9resv96jF\nOoh49du6betVD0JKnrZL/nOt3iYpO/Lje0LeRPjO/pikuIWQOQJyJ2hrIZKgN1XBo5drQT6rC9G3\nsDtg9Hl6lGQYujL2OdOq83m1P214dcl/OOISdSrtrld0deg1j4QP3PKnas87BnneoBunzRh6agFk\nXzNwBR1g1s16lnr0+b3bT6E5rH/l23p1oYMr9EnSXQQCOoJqLNfD0KxR+qQaviA0hex0xO7QxS27\nXtbHHYmUPPhkhL4ap0r6UC2CNtupvU/Tr9fe7MhF4e+3O7X18u+r9WRhjtnuNikbPvZ7ePIGnYdt\nj4fh8+GyP0XeVzDxKXr7gyu1uKSYI7oxF2hB2f2atv96S6zF3GLsxboYqO5o58nOxnL452Xakvn0\n4zB0bs/2WXKh7pH/7v/Bu7/R73NcMrz2Pf3dAcju3I7Yz7hLtWVz5f2RJ2CV6nqO4QxCneqiwafC\nrFmzjLVrw3fG61cMQ89kH9uoJ0da67Ul0VP/UTg9MYzw4rjlaVOcz9FR4MlQtU+nzw1fEJ1jPJ2o\n2A33LdZzVjc8Fxj5HdsIz9yiG2Nd+x8dWfeUxgrdMhh04HTDC7rQ5tEr9MIxAD84Fsgd74jPq6td\nY5THfbqilFpnGMasbrcTQRcEwU/pOj0pa3PoUcj253XdQWKWHlUNm3fy+3zkEzrCv/mVwHxASw08\ndImel/nquui+hkFITARdKXUR8EfADtxvGMavu9peBF0QBgAVu7VXXl8KKJj9eZ0+mnCKWR3uFj2v\n1NFea2vQP+GKooQQeirop+yhK6XswF+ApcBR4COl1AuGYWw/1X0KgnAakDMGPv+G7is07brAHNOp\nEinLJz4lckaRcEr0ZlJ0DrDXMIz9AEqp/wCXASLogjDQSSvSWUHCgKI3laKFQPASQEfN2wRBEIR+\noM9L/5VStyql1iql1lZU9GzFc0EQBOHk6Y2glwLFQf8XmbeFYBjGvYZhzDIMY1ZOTk7HuwVBEIQo\n0RtB/wgoUUqNUErFAZ8GXujmMYIgCEIfccqTooZheJRSXwFeR6ctPmgYRtdrdAmCIAh9Rq9K/w3D\neAXoYc9NQRAEoS8ZGP3QBUEQhG4RQRcEQRgkxLSXi1KqAjh0ig/PBiqjeDgDhTPxdZ+JrxnOzNd9\nJr5mOPnXPcwwjG7TBGMq6L1BKbW2J70MBhtn4us+E18znJmv+0x8zdB3r1ssF0EQhEGCCLogCMIg\nYSAJepSXzRkwnImv+0x8zXBmvu4z8TVDH73uAeOhC4IgCF0zkCJ0QRAEoQsGhKArpS5SSu1SSu1V\nSn2vv4+nL1BKFSulliultiultimlvm7enqmUelMptcf8ndHfxxptlFJ2pdQGpdRL5v8jlFIfmp/3\nE2avoEGFUipdKfW0UmqnUmqHUurswf5ZK6XuMM/trUqpx5VSrsH4WSulHlRKlSultgbdFvazVZp7\nzNe/WSk1ozfPfdoLetDKSBcDE4BrlVIT+veo+gQP8E3DMCYAZwG3m6/ze8AywzBKgGXm/4ON/9/e\n/YRMVYVxHP8cMEQNUltI6kKjqEVQhgshiTAXZVEtWgRBLoSW1SqIVi2DyFq5UdQiWqRS0qJFf6BV\nRkZUZFRSlPGaQmnRRqOnxTkDwxsDZl7HOe/zhcvcc+6B+zz8Zn7MeebMPU/i2Fj7eeyMiBvwG3ZM\nJapheRnvRMTNuFXNv1utSylr8AQ2RsQt6vOfHtGn1vtwz7y+Sdreixvb8Th2/Z8bX/GGbmxnpIg4\nh9HOSF0REXMR8Wk7/0P9gK9Rc93fhu3HQ9OJcBhKKWtxH3a3dsEWHGhDesz5GtyJPRAR5yLijM61\nVp8dtaSUsghLMadDrSPiQ/w6r3uStg/ilah8hOWllOsu9t6zYOgLbmekUso6bMARrIqIuXbpJFZN\nKayheAlP4+/WvhZnIuKv1u5R7/U4jb2t1LS7lLJMx1pHxM94AT+qRn4WR/Wv9YhJ2l5Sf5sFQ19Q\nlFKuxkE8FRG/j1+LuiSpm2VJpZT7cSoijk47lsvMItyOXRGxAX+aV17pUOsV6rfR9ViNZf5dllgQ\nDKntLBj6Be2M1AOllKtUM38tIg617l9GU7D2empa8Q3AHXiglPKDWkrbotaWl7dpOX3qfQInIuJI\nax9QDb5nrbfi+4g4HRHncUjVv3etR0zS9pL62ywY+oLYGanVjvfgWES8OHbpMLa38+1463LHNhQR\n8UxErI2Idaqu70fEo/gAD7dhXeUMEXESP5VSbmpdd+MrHWutllo2lVKWtvf6KOeutR5jkraH8Vhb\n7bIJZ8dKM/+diLjiD2zDNziOZ6cdz0A5blanYZ/js3ZsU2vK7+FbvIuV0451oPzvwtvt/Hp8jO/w\nBhZPO74B8r0NnzS938SK3rXGc/gaX+JVLO5Ra7yu/k5wXp2N7ZikLYq6iu84vlBXAV30vfOfokmS\nJJ0wCyWXJEmS5AJIQ0+SJOmENPQkSZJOSENPkiTphDT0JEmSTkhDT5Ik6YQ09CRJkk5IQ0+SJOmE\nfwDLUDVHy5M5DAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f585dafd278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[-100:])\n",
    "plt.plot(losses_norm[-100:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Annealing the learning rate\n",
    "Right now we keep the learning rate the same the whole time. It's generally a good idea to reduce your learning rate slowly during training. There are several options for this with Tensorflow: \n",
    "- Define a placeholder for the learning rate and pass this to the network as parameter\n",
    "- Use the `tf.train.exponential_decay` function. \n",
    "We are going use the second approach!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.exponential_decay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "When training a model, it is often recommended to lower the learning rate as\n",
    "the training progresses.  This function applies an exponential decay function\n",
    "to a provided initial learning rate.  It requires a `global_step` value to\n",
    "compute the decayed learning rate.  You can just pass a TensorFlow variable\n",
    "that you increment at each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = LEARNING_RATE\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           10000, 0.96, staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 5: improving the network by understanding the activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 6: the importance of hyper parameters, grid-search optimization -> wide vs deep? -  Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting train, test, and the validation set\n",
    "During our search for the best hyper parameters we did something terrible: by optimising our network on our test data, and taking the best parameters for our test data, we \"leaked knowledge\" about our test set into our algorithms. \n",
    "\n",
    "Right now I'm not going to fix this, but you have to be aware of this \"smell\" in your own train data... it probably affects your results when you take new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"datasets/celebrities/list_attr_celeba.txt\"])\n",
    "textlinereader = tf.TextLineReader(skip_header_lines=2) #https://www.tensorflow.org/api_docs/python/tf/TextLineReader\n",
    "key, val = textlinereader.read(filename_queue)\n",
    "sess = tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "#record_defaults = [[\"000001.jpg\"] + [0]*40]\n",
    "record_defaults = [[0] for _ in range(40)]\n",
    "content = tf.decode_csv(val, record_defaults=record_defaults)\n",
    "\n",
    "\n",
    "# Evaluate the tensor `c`.\n",
    "print(sess.run([key,val]))\n",
    "print(sess.run([key,val]))\n",
    "print(sess.run([key,val]))\n",
    "print(sess.run([key,val]))\n",
    "#print(sess.run([content]))\n",
    "\n",
    "# https://www.tensorflow.org/programmers_guide/reading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.decode_csv?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(zipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col1, col2, col3, col4, col5 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([col1, col2, col3, col4])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Start populating the filename queue.\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "  for i in range(1200):\n",
    "    # Retrieve a single instance:\n",
    "    example, label = sess.run([features, col5])\n",
    "\n",
    "  coord.request_stop()\n",
    "  coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = tf.estimatorDNNRegressor(\n",
    "    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n",
    "    hidden_units=[1024, 512, 256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.DNNRegressor(feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n",
    "    hidden_units=[1024, 512, 256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cool Keras thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=1275, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=4, batch_size=5, verbose=1)\n",
    "seed=10\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, datasetX, datasetY, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "estimator.fit(datasetX, datasetY)\n",
    "prediction = estimator.predict(datasetXtest)\n",
    "plt.scatter(datasetYtest,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# To getter a better understanding of interaction of the dimensions\n",
    "# plot the first three PCA dimensions\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,\n",
    "           cmap=plt.cm.Paired)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some of the clouds overlap you can see that we are probably able to classify any new species based on these three properties of the flowers. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you have your own dataset with values that look similar to those of the Iris dataset, give the network you just made a go! If the network does not converge yet, there are many tips and tricks you have to know about neural networks that you need to learn, and will learn during this course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(iris['feature_names'])\n",
    "print(len(iris['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cool snail-age tutorial: https://www.tensorflow.org/versions/r0.12/tutorials/estimators/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single layer network\n",
    "This is where we show \n",
    "\n",
    "$Y = \\sum{weight*input} + bias$\n",
    "\n",
    "This means that for one neuron we multiply the input with the weight, do this for each input-weight combination, and then add a bias. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What activation functions are out there?\n",
    "\n",
    "Story about how deep learning advances the last couple of years are party due to new activation functions: RELU and ELU. \n",
    "\n",
    "By only multiplying you get a 'linear' seperation. Why/how? \n",
    "\n",
    "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "\n",
    "That's why it's common to add a 'nonregularity' after each layer. This turns the linear activation into a nonlinear activation. \n",
    "\n",
    "[Available activation functions](https://www.tensorflow.org/api_guides/python/nn#Activation_Functions)\n",
    "Let's take a look at the nonregularities Tensorflow offers out of the box with some sample graphs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputdata = np.arange(-5.0, 5.0, 0.1)\n",
    "SHAPE_INPUT = inputdata.shape\n",
    "print(SHAPE_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydatainput = tf.placeholder(dtype=tf.float32, shape=SHAPE_INPUT)\n",
    "reluoutput = tf.nn.relu(mydatainput) # https://www.tensorflow.org/api_docs/python/tf/nn/relu\n",
    "\n",
    "relu6output = tf.nn.relu6(mydatainput)\n",
    "creluoutput = tf.nn.crelu(mydatainput) # Concatenated relu\n",
    "eluoutput = tf.nn.elu(mydatainput)\n",
    "sigmoidoutput = tf.sigmoid(mydatainput)\n",
    "tanhoutput = tf.tanh(mydatainput)\n",
    "\n",
    "# tf.nn.softplus\n",
    "# tf.nn.softsign\n",
    "# tf.nn.dropout\n",
    "# tf.nn.bias_add\n",
    "\n",
    "print(reluoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d,e = sess.run([reluoutput, creluoutput, eluoutput, sigmoidoutput, tanhoutput], feed_dict={mydatainput:inputdata})\n",
    "print(a)\n",
    "for x in [a,b,c,d,e]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(a)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(c)\n",
    "\n",
    "plt.plot(d)\n",
    "\n",
    "plt.plot(e)\n",
    "plt.legend(['relu', 'elu', 'sigmoid', 'tanh'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think: what do I have to do with all this information, how do I use it, what activation function is best? The take-home message here is that you should know what they exist, how each one of them looks in the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Softmax\n",
    "In classificiation it's a good idea to use softmax. \n",
    "\n",
    "Right now each output neuron gives us a 'score'. The neuron with the highest score tells us what flower we are probably dealing with. \n",
    "But what about those difficult flowers that look like each other? We would love to have a probability per class. This is why we use the softmax layer. \n",
    "\n",
    "What this layer does is taking all activations and summing them: \n",
    "XXXX\n",
    "Then it divides each activation through the total sum. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
