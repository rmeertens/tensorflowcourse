{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: predicting the ground state energy of molecules\n",
    "\n",
    "In chapter 1 we learned that Tensorflow does all its computations in its graph, that you first have to define. In chapter 2 we made or first neural network, and made a very simple classifier for the iris dataset. \n",
    "\n",
    "This chapter we are going to make our neural network more interesting. We are going to [predict the ground state energies of molecules](https://www.kaggle.com/haimfeld87/prediction-of-ground-state-energies-of-molecules). \n",
    "\n",
    "http://web.stanford.edu/class/cs20si/lectures/notes_09.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: downloading the data, visualising it \n",
    "Welcome to section 3 of our course. In this section we are going to predict the [ground state energies of molecules](https://www.kaggle.com/haimfeld87/prediction-of-ground-state-energies-of-molecules). \n",
    "\n",
    "In a [blogpost and paper the author explains what kind of data we are working with](https://burakhimmetoglu.com/machine-learning-meets-quantum-mechanics/). An interesting quote is: \n",
    "> This peculiar nonlinear dependence is impossible to model with simple linear models, thus learning algorithms such as neural networks and boosted regression trees are a perfect match for such a task.\n",
    "\n",
    "I also wanted to show you this visualisation: \n",
    "![Vis ground energy state](https://burakhimmetoglu.files.wordpress.com/2016/11/rbpca.png)\n",
    "\n",
    "Looks pretty exciting to work with, and a great challenge. \n",
    "\n",
    "I imagine that the author is also quite happy with us, as he states:\n",
    "> I am looking for Kagglers to find the best model and reduce mean squared error as much as possible!\n",
    "\n",
    "Looks like we now know the function we have to minimise. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and loading data\n",
    "The data is here: https://www.kaggle.com/burakhmmtgl/energy-molecule\n",
    "Download it, unzip it, and place it in the datasets folder...\n",
    "\n",
    "Image called downloaddata1.png here...\n",
    "\n",
    "To load the data I'm going to use a [package called Pandas](http://pandas.pydata.org/). This package is great for reading datasets, and getting an initial understanding of what you are working with. Although we are not doing to discuss all features Pandas has, I just want you to use it one time, and know that it's out there. Pandas is already installed in the Docker image I provided, so let's dive right in: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('robobohr.csv/roboBohr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0          0          1          2          3          4  \\\n",
      "0           0  73.516695  17.817765  12.469551  12.458130  12.454607   \n",
      "1           1  73.516695  20.649126  18.527789  17.891535  17.887995   \n",
      "2           2  73.516695  17.830377  12.512263  12.404775  12.394493   \n",
      "3           3  73.516695  17.875810  17.871259  17.862402  17.850920   \n",
      "4           4  73.516695  17.883818  17.868256  17.864221  17.818540   \n",
      "\n",
      "           5          6          7          8    ...      1267  1268  1269  \\\n",
      "0  12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   0.5   \n",
      "1  17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   0.0   \n",
      "2  12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   0.0   \n",
      "3  17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   0.0   \n",
      "4  12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   0.0   \n",
      "\n",
      "   1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
      "0   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
      "1   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
      "2   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
      "3   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
      "4   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
      "\n",
      "[5 rows x 1278 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(df)\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to predict is the variable in the last axis, called Eat. To do this we are going to use all features, except for the ID, and the pubchem_id. Let's use pandas to remove these columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(['Unnamed: 0', 'pubchem_id'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also check if there is any missing data: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Eat'], axis = 1)\n",
    "Y = df['Eat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data\n",
    "Like we learned in our previous chapter, it's a good idea to split our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16242\n",
      "12993\n",
      "3249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(len(X))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "datasetX = X_train.values\n",
    "datasetY = Y_train.values\n",
    "#print(datasetX.shape)\n",
    "#print(datasetY.shape)\n",
    "#print(datasetY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: a first approach - the neural network we made in section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with manually constructing our network again, just like we did in section 1. \n",
    "\n",
    "## Simplifying layer building\n",
    "In chapter 1 we made our network using the following code: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8d6be63c7a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputplaceholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlayer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutputlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_input, n_output]))\n",
    "biases = tf.Variable(tf.zeros([n_output]))\n",
    "layer_1 = tf.matmul(inputplaceholder, weights)\n",
    "layer_2 = tf.add(layer_1, biases)\n",
    "outputlayer = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "## Which we can simplify to: \n",
    "weights = tf.Variable(tf.random_normal([n_input, n_output]))\n",
    "biases = tf.Variable(tf.zeros([n_output]))\n",
    "\n",
    "outputlayer = tf.nn.sigmoid(tf.add(tf.matmul(inputplaceholder, weights), biases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, typing this out for a big neural network is a lot of work, and it's easy to make errors. This is why you can build functions that perform the same thing for you. \n",
    "\n",
    "You can also use the higher level functions already available in Tensorflow. You can make the same thing using: \n",
    "\n",
    "`tf.layers.dense(inputs=inputplaceholder, units=3, activation=tf.nn.sigmoid, name=\"single_layer_neural_network\") `\n",
    "\n",
    "Another option you might want to consider is the Keras python package. Keras is a high-level API definition for neural networks. It uses Tensorflow as backend to build its functions on. A neural network even more complex than in chapter two can be made like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why stick to Tensorflow\n",
    "It's very likely that for your specific application Keras is good enough (95% sure). However, Tensorflow as been adapting the Keras API specifications, and thus Tensorflow can do roughly the same as Keras. The reason to stick to Tensorflow is that it offers a LOT of extra functionality, which will come in handy for the \"strange\" applications you want to build. \n",
    "\n",
    "Let's create a simple single-layer neural network for our new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1275], name=\"inputplaceholder\")\n",
    "output_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"userdefinedoutput\")\n",
    "\n",
    "dense = tf.layers.dense(inputs=input_pl, units=300, activation=tf.nn.relu, name=\"first_dense_layer\")\n",
    "network_prediction = tf.layers.dense(inputs=dense, units=1, activation=None, name=\"prediction_dense_layer\")\n",
    "\n",
    "print(dense)\n",
    "print(network_prediction)\n",
    "loss = tf.losses.mean_squared_error(output_pl,network_prediction)\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "print(loss)\n",
    "print(\"Optimizer: --------\")\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "zipped = list(zip(datasetX, datasetY))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "for _ in range(10):\n",
    "    datax = list()\n",
    "    datay = list()\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        samp = random.choice(zipped)\n",
    "        datax.append(samp[0])\n",
    "        datay.append([samp[1]])\n",
    "    _, l = sess.run([optimizer,loss], feed_dict={input_pl: datax, output_pl: datay})\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the learning rate\n",
    "As you can see above our loss becomes HIGHER instead of lower. About once a week I see somebody on Stackoverflow ask why this is happening, and why their network is giving nan as output. The answer is: they have to adjust their learning rate.\n",
    "\n",
    "![LR](https://qph.ec.quoracdn.net/main-qimg-3fdd9cba9f37125af1513cce7359d5cf)\n",
    "\n",
    "A nice analogy I like to use is one in which we are walking through misty mountains, and want to reach the deepest valley. Every 5 minutes we determine based on our small observation what path leads down, and continue that way. If we did this every second we could never leave small valleys. If we only did this every day we might walk right out of the valley again...\n",
    "\n",
    "Let's adjust our learning rate tenfold every time till our network is able to learn something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.0005).minimize(loss)\n",
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)\n",
    "\n",
    "zipped = list(zip(datasetX, datasetY))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "for _ in range(1000):\n",
    "    datax = list()\n",
    "    datay = list()\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        samp = random.choice(zipped)\n",
    "        datax.append(samp[0])\n",
    "        datay.append([samp[1]])\n",
    "    _, l = sess.run([optimizer,loss], feed_dict={input_pl: datax, output_pl: datay})\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising your graph\n",
    "When working with Tensorflow it's easy to make a mistake. For example, when adding an extra layer to your network it's easy to forget to connect it to other layers. A cool feature of our Jupyter notebook is that you can visualise the graph you made. The source of this code can be found in [this Stackoverflow answer](https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter/38192374#38192374). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.layers.dense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1275], name=\"inputplaceholder\")\n",
    "output_pl = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"userdefinedoutput\")\n",
    "\n",
    "dense = tf.layers.dense(inputs=input_pl, units=300, activation=tf.nn.relu, name=\"first_dense_layer\")\n",
    "dense2 = tf.layers.dense(inputs=dense, units=150, activation=tf.nn.relu, name=\"second_dense_layer\")\n",
    "dense3 = tf.layers.dense(inputs=dense2, units=1, activation=None, name=\"prediction_dense_layer\")\n",
    "\n",
    "print(dense)\n",
    "print(dense2)\n",
    "loss = tf.losses.mean_squared_error(output_pl,dense3)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 3: Introduction to overfitting: splitting your data in train, test, and validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is a HUGE problem for neural networks. \n",
    "\n",
    "It even turns out that if you take images and give them random labels, neural networks are able to learn these random labels. \n",
    "\n",
    "Best is to split your data in a train, test, and validation set. \n",
    "\n",
    "Use your train set to train on, your test set regularly to see if your approach indeed did what you wanted to do... And use your validation set as sparse as possible, otherwise information leeks from all sets into your train data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(len(X))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 4: normalizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 5: improving the network by understanding the activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 6: the importance of hyper parameters, grid-search optimization -> wide vs deep? -  Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"datasets/celebrities/list_attr_celeba.txt\"])\n",
    "textlinereader = tf.TextLineReader(skip_header_lines=2) #https://www.tensorflow.org/api_docs/python/tf/TextLineReader\n",
    "key, val = textlinereader.read(filename_queue)\n",
    "sess = tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "#record_defaults = [[\"000001.jpg\"] + [0]*40]\n",
    "record_defaults = [[0] for _ in range(40)]\n",
    "content = tf.decode_csv(val, record_defaults=record_defaults)\n",
    "\n",
    "\n",
    "# Evaluate the tensor `c`.\n",
    "print(sess.run([key,val]))\n",
    "print(sess.run([key,val]))\n",
    "print(sess.run([key,val]))\n",
    "print(sess.run([key,val]))\n",
    "#print(sess.run([content]))\n",
    "\n",
    "# https://www.tensorflow.org/programmers_guide/reading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.decode_csv?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(zipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col1, col2, col3, col4, col5 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([col1, col2, col3, col4])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Start populating the filename queue.\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "  for i in range(1200):\n",
    "    # Retrieve a single instance:\n",
    "    example, label = sess.run([features, col5])\n",
    "\n",
    "  coord.request_stop()\n",
    "  coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = tf.estimatorDNNRegressor(\n",
    "    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n",
    "    hidden_units=[1024, 512, 256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.DNNRegressor(feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n",
    "    hidden_units=[1024, 512, 256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cool Keras thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=1275, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=4, batch_size=5, verbose=1)\n",
    "seed=10\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, datasetX, datasetY, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "estimator.fit(datasetX, datasetY)\n",
    "prediction = estimator.predict(datasetXtest)\n",
    "plt.scatter(datasetYtest,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# To getter a better understanding of interaction of the dimensions\n",
    "# plot the first three PCA dimensions\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,\n",
    "           cmap=plt.cm.Paired)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some of the clouds overlap you can see that we are probably able to classify any new species based on these three properties of the flowers. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you have your own dataset with values that look similar to those of the Iris dataset, give the network you just made a go! If the network does not converge yet, there are many tips and tricks you have to know about neural networks that you need to learn, and will learn during this course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(iris['feature_names'])\n",
    "print(len(iris['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cool snail-age tutorial: https://www.tensorflow.org/versions/r0.12/tutorials/estimators/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single layer network\n",
    "This is where we show \n",
    "\n",
    "$Y = \\sum{weight*input} + bias$\n",
    "\n",
    "This means that for one neuron we multiply the input with the weight, do this for each input-weight combination, and then add a bias. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What activation functions are out there?\n",
    "\n",
    "Story about how deep learning advances the last couple of years are party due to new activation functions: RELU and ELU. \n",
    "\n",
    "By only multiplying you get a 'linear' seperation. Why/how? \n",
    "\n",
    "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "\n",
    "That's why it's common to add a 'nonregularity' after each layer. This turns the linear activation into a nonlinear activation. \n",
    "\n",
    "[Available activation functions](https://www.tensorflow.org/api_guides/python/nn#Activation_Functions)\n",
    "Let's take a look at the nonregularities Tensorflow offers out of the box with some sample graphs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputdata = np.arange(-5.0, 5.0, 0.1)\n",
    "SHAPE_INPUT = inputdata.shape\n",
    "print(SHAPE_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydatainput = tf.placeholder(dtype=tf.float32, shape=SHAPE_INPUT)\n",
    "reluoutput = tf.nn.relu(mydatainput) # https://www.tensorflow.org/api_docs/python/tf/nn/relu\n",
    "\n",
    "relu6output = tf.nn.relu6(mydatainput)\n",
    "creluoutput = tf.nn.crelu(mydatainput) # Concatenated relu\n",
    "eluoutput = tf.nn.elu(mydatainput)\n",
    "sigmoidoutput = tf.sigmoid(mydatainput)\n",
    "tanhoutput = tf.tanh(mydatainput)\n",
    "\n",
    "# tf.nn.softplus\n",
    "# tf.nn.softsign\n",
    "# tf.nn.dropout\n",
    "# tf.nn.bias_add\n",
    "\n",
    "print(reluoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "sess = tf.Session() # https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d,e = sess.run([reluoutput, creluoutput, eluoutput, sigmoidoutput, tanhoutput], feed_dict={mydatainput:inputdata})\n",
    "print(a)\n",
    "for x in [a,b,c,d,e]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(a)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(c)\n",
    "\n",
    "plt.plot(d)\n",
    "\n",
    "plt.plot(e)\n",
    "plt.legend(['relu', 'elu', 'sigmoid', 'tanh'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think: what do I have to do with all this information, how do I use it, what activation function is best? The take-home message here is that you should know what they exist, how each one of them looks in the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Softmax\n",
    "In classificiation it's a good idea to use softmax. \n",
    "\n",
    "Right now each output neuron gives us a 'score'. The neuron with the highest score tells us what flower we are probably dealing with. \n",
    "But what about those difficult flowers that look like each other? We would love to have a probability per class. This is why we use the softmax layer. \n",
    "\n",
    "What this layer does is taking all activations and summing them: \n",
    "XXXX\n",
    "Then it divides each activation through the total sum. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
